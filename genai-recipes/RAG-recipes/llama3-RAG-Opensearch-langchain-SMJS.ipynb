{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3eed142-6144-4ba2-a693-2bcfdeeae823",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question (RAG) with Llama3-8B on SageMaker JumpStart and Amazon OpenSearch using LangChain\n",
    "\n",
    "RAG Application use cases with Llama3-8B, BGE Large embedding model on SageMaker and OpenSearch as Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a66db-4d22-4e0f-883c-8e8daf6a4291",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate the use of [Llama3-8B](https://huggingface.co/meta-llama/Llama-2-13b) text generation combined with [BGE Large En v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) embedding model to efficiently construct a Retrieval Augmented Generation (RAG) QnA system on a SageMaker Notebook. This notebook, powered by an `ml.t3.medium instance`, enables the deployment of LLMs on [SageMaker JumpStart](https://aws.amazon.com/sagemaker/jumpstart/). These can be called with an API endpoint created by SageMaker, which we then use to build, experiment with, and tune for comparing Advanced RAG application techniques using [LangChain](https://www.langchain.com/). Additionally, we showcase how we can use [OpenSearch](https://aws.amazon.com/opensearch-service/) Vector Engine Embedding store to archive and retrieve embeddings, integrating it into your RAG workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f4f95-dfab-4c64-956e-6c29274131d0",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d597937-7c3e-42cc-b595-309bd8e5ae28",
   "metadata": {},
   "source": [
    "---\n",
    "This Jupyter Notebook can be run on a t3.medium instance (ml.t3.medium). However, to deploy `Llama3-8B Text Generation` and `BGE Large En v1.5` models, you may need to request a quota increase. \n",
    "\n",
    "To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `ml.g5.12xlarge` for endpoint usage\n",
    "   - `ml.g5.2xlarge` for endpoint usage\n",
    "4. If needed, request a quota increase for these resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad641d-5a50-4493-b308-563f280f2b2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon EC2 service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2277979-97a1-41a6-ac40-0f267326b40a",
   "metadata": {},
   "source": [
    "### Changing instance type\n",
    "---\n",
    "Models are supported on the following instance types:\n",
    "\n",
    " - Llama3-8B Text Generation: `ml.g5.2xlarge`, `ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.g5.12xlarge`, `ml.g5.24xlarge`, `ml.g5.48xlarge`, and `ml.p4d.24xlarge`\n",
    " - BGE Large En v1.5: `ml.g5.2xlarge`, `ml.c6i.xlarge`,`ml.g5.4xlarge`, `ml.g5.8xlarge`, `ml.p3.2xlarge`, and `ml.g4dn.2xlarge`\n",
    "\n",
    "By default, the JumpStartModel class selects a default instance type available in your region. If you would like to use a different instance type, you can do so by specifying instance type in the JumpStartModel class.\n",
    "\n",
    "`my_model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70287fd5-1c6f-4a05-a7cc-085cb10b4508",
   "metadata": {},
   "source": [
    "### Local setup (Optional):\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33af4f4-69fd-4a9f-acb3-f06bbe64fb21",
   "metadata": {},
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e19e16-86b3-4e27-94bf-00e2f832605c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents\n",
    "---\n",
    "\n",
    "1. [Requirements](#Requirements)\n",
    "2. [Model Deployment](#Model-Deployment)\n",
    "3. [Setup LangChain](#Setup-LangChain)\n",
    "4. [Data Preparation](#Data-Preparation)\n",
    "5. [Question Answering with LangChain Vector Store Wrapper](#Question-Answering-with-LangChain-Vector-Store-Wrapper)\n",
    "6. [Conclusion](#Conclusion)\n",
    "7. [Clean Up Resources](#Clean-Up-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bb282-2718-4911-a92b-4ef084441239",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709d071-695d-4102-a8cd-6fba6c4678a3",
   "metadata": {},
   "source": [
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose `ml.t3.medium`.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07d152-2889-4004-8eba-a0d9028708db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22cf2e-971a-4df9-9e27-1cd7a05d8307",
   "metadata": {},
   "source": [
    "To run this notebook you would need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60fde7eb-a354-4934-9126-b793080328c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "pypdf==4.1.0\n",
    "opensearch-py==2.0.0\n",
    "requests-aws4auth==1.1.1\n",
    "boto3==1.34.58\n",
    "sqlalchemy==2.0.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c54cfea5-782f-49c1-8885-c8fc8955e09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69b7e7-1218-4d42-a8d5-1894fc64d721",
   "metadata": {},
   "source": [
    "if you see this error → ERROR: pip's dependency resolver does not currently take into account all the packages after pip install you can ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2e835",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b>\n",
    "\n",
    "Before proceeding, please verify that you have the correct version of the SQLAlchemy library installed. This notebook requires SQLAlchemy >= 2.0.0.\n",
    "\n",
    "To check your installed SQLAlchemy version, you can run the following code:\n",
    "\n",
    "```python\n",
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)\n",
    "```\n",
    "\n",
    "If the version displayed is less than 2.0.0, and you have already installed the correct version using `pip`, you may need to \"<span style=\"color:green;\">restart</span>\" or \"<span style=\"color:green;\">shutdown</span>\" the Jupyter Notebook kernel to load the updated library.\n",
    "\n",
    "To restart the kernel, go to the \"Kernel\" menu and select \"Restart Kernel\". If that doesn't work, try shutting down the notebook completely and relaunching it.\n",
    "\n",
    "Restarting or shutting down the kernel will resolve any dependency issues and ensure that the correct SQLAlchemy version is loaded.\n",
    "\n",
    "If you haven't installed SQLAlchemy >= 2.0.0 yet, you can do so by running the following command in your terminal or command prompt:\n",
    "\n",
    "```\n",
    "pip install sqlalchemy>=2.0.29\n",
    "```\n",
    "\n",
    "Once the installation is complete, restart or shutdown the Jupyter Notebook kernel as described above.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd4370c7-8453-478b-aefd-9c861bf7f472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.29\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "print(sqlalchemy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f89ca85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.14\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c4bf9f6-1127-4444-b684-f0c933c6158a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import sagemaker\n",
    "except ImportError:\n",
    "    !pip install sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8193291-1bf2-478e-afff-d6afd33a358b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Deployment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705564e-143f-411e-8537-a337255f256f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Deploy `Llama 3 8B Instruct` LLM model on Amazon SageMaker JumpStart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40c642f3-7aaf-4c37-9071-36a19188e753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the JumpStartModel class from the SageMaker JumpStart library\n",
    "from sagemaker.jumpstart.model import JumpStartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5be0b3ea-8482-4288-8ed2-e03d7bafcb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# Specify the model ID for the HuggingFace Llama 3 8b Instruct LLM model\n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\"\n",
    "accept_eula = True\n",
    "model = JumpStartModel(model_id=model_id)\n",
    "predictor = model.deploy(accept_eula=accept_eula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fdb95-bd4b-445c-b597-755fbd9a432a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Deploy `BGE Large En` embedding model on Amazon SageMaker JumpStart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13c22418-d962-46b2-8e5e-0bcb74e6ff64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "# Specify the model ID for the HuggingFace BGE Large EN Embedding model\n",
    "model_id = \"huggingface-sentencesimilarity-bge-large-en-v1-5\"\n",
    "text_embedding_model = JumpStartModel(model_id=model_id)\n",
    "embedding_predictor = text_embedding_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8c041-8d3c-4265-bb9b-e4d0dd0bb151",
   "metadata": {},
   "source": [
    "## Setup LangChain\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7f757369-4769-442a-87ba-db9bf75a4dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain_community.embeddings.sagemaker_endpoint import EmbeddingsContentHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0256b-6fd8-4e1a-8206-ff4ca6efda72",
   "metadata": {},
   "source": [
    "Get endpoint names from predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d0e28065-2ccc-4174-98b1-c45fdb83cc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name\n",
    "llm_endpoint_name = predictor.endpoint_name\n",
    "embedding_endpoint_name = embedding_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e593b2-1433-4bef-b217-be724575e4fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Transform input and output data to proccess API calls for`Llama 3 8B Instruct` on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3616478c-d454-4196-b64c-c9445e28839f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class Llama38BContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 1000,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"stop\": [\"<|eot_id|>\"],\n",
    "            },\n",
    "        }\n",
    "        input_str = json.dumps(\n",
    "            payload,\n",
    "        )\n",
    "        #print(input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(response_json)\n",
    "        content = response_json[\"generated_text\"].strip()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5476939-6f13-4ba0-9bc9-b43a92592282",
   "metadata": {},
   "source": [
    "Instantiate the LLM with SageMaker and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c425b6fe-a572-4725-a1ad-ee1c7bf29db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the content handler for Llama3-8B\n",
    "llama_content_handler = Llama38BContentHandler()\n",
    "\n",
    "# Setup for using the Llama3-8B model with SageMaker Endpoint\n",
    "llm = SagemakerEndpoint(\n",
    "     endpoint_name=llm_endpoint_name,\n",
    "     region_name=region, \n",
    "     model_kwargs={\"max_new_tokens\": 1024, \"top_p\": 0.9, \"temperature\": 0.7},\n",
    "     content_handler=llama_content_handler\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58be6d-c4ff-4948-babc-52c41228c427",
   "metadata": {},
   "source": [
    "Transform input and output data to proccess API calls for`BGE Large En` on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "410c2cf2-dda6-4617-b828-255b4aa4dd57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class BGEContentHandlerV15(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, text_inputs: List[str], model_kwargs: dict) -> bytes:\n",
    "        \"\"\"\n",
    "        Transforms the input into bytes that can be consumed by SageMaker endpoint.\n",
    "        Args:\n",
    "            text_inputs (list[str]): A list of input text strings to be processed.\n",
    "            model_kwargs (Dict): Additional keyword arguments to be passed to the endpoint.\n",
    "               Possible keys and their descriptions:\n",
    "               - mode (str): Inference method. Valid modes are 'embedding', 'nn_corpus', and 'nn_train_data'.\n",
    "               - corpus (str): Corpus for Nearest Neighbor. Required when mode is 'nn_corpus'.\n",
    "               - top_k (int): Top K for Nearest Neighbor. Required when mode is 'nn_corpus'.\n",
    "               - queries (list[str]): Queries for Nearest Neighbor. Required when mode is 'nn_corpus' or 'nn_train_data'.\n",
    "        Returns:\n",
    "            The transformed bytes input.\n",
    "        \"\"\"\n",
    "        input_str = json.dumps(\n",
    "            {\n",
    "                \"text_inputs\": text_inputs,\n",
    "                **model_kwargs\n",
    "            }\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Transforms the bytes output from the endpoint into a list of embeddings.\n",
    "        Args:\n",
    "            output: The bytes output from SageMaker endpoint.\n",
    "        Returns:\n",
    "            The transformed output - list of embeddings\n",
    "        Note:\n",
    "            The length of the outer list is the number of input strings.\n",
    "            The length of the inner lists is the embedding dimension.\n",
    "        \"\"\"\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4749e-e437-4212-b69d-6c800aa0e21c",
   "metadata": {},
   "source": [
    "Instantiate the embedding model with SageMaker and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84f6e08e-c969-4a05-8587-ad49f9d9dca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bge_content_handler = BGEContentHandlerV15()\n",
    "sagemaker_embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs={\"mode\": \"embedding\"},\n",
    "    content_handler=bge_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88793e5-c562-48ce-858b-50c918ac5249",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e0dc2-718f-47af-aa60-30fa9a60cae3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's first download some of the files to build our document store.\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8b0cbc6-367a-443a-9e59-c63640a1e4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/c7c14359-36fa-40c3-b3ca-5bf7f3fa0b96.pdf',\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/d2fde7ee-05f7-419d-9ce8-186de4c96e25.pdf',\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/f965e5c3-fded-45d3-bbdb-f750f156dcc9.pdf',\n",
    "    'https://d18rn0p25nwr6d.cloudfront.net/CIK-0001018724/336d8745-ea82-40a5-9acc-1a89df23d0f3.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2024-10-K-Annual-Report.pdf',\n",
    "    'AMZN-2023-10-K-Annual-Report.pdf',\n",
    "    'AMZN-2022-10-K-Annual-Report.pdf',\n",
    "    'AMZN-2021-10-K-Annual-Report.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2024, source=filenames[0]),\n",
    "    dict(year=2023, source=filenames[1]),\n",
    "    dict(year=2022, source=filenames[2]),\n",
    "    dict(year=2021, source=filenames[3])]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859253bf-4bb0-43bf-999a-e1abb1f6983b",
   "metadata": {},
   "source": [
    "If you take a look into the Amazon 10-Ks, the first 4 pages are all the very similar and may skew the responses if you they are kept in the embeddings. This will cause repetition, take longer to generate embeddings, and may skew your results. In the next section you will take the downloaded data, trim the 10-K (first 4 pages) and overwrite them as processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ac21b76-14b4-4c64-8cfe-408d877426c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "import glob\n",
    "\n",
    "local_pdfs = glob.glob(data_root + '*.pdf')\n",
    "\n",
    "# Iterate over each PDF file\n",
    "for idx, local_pdf in enumerate(local_pdfs):\n",
    "    pdf_reader = PdfReader(local_pdf)\n",
    "    pdf_writer = PdfWriter()\n",
    "    \n",
    "    if idx == 0:\n",
    "        # Keep the first 4 pages for the first document\n",
    "        for pagenum in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[pagenum]\n",
    "            pdf_writer.add_page(page)\n",
    "    else:\n",
    "        # Remove the first 4 pages for other documents\n",
    "        for pagenum in range(4, len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[pagenum]\n",
    "            pdf_writer.add_page(page)\n",
    "\n",
    "    # Write the modified content to a new file\n",
    "    with open(local_pdf, 'wb') as new_file:\n",
    "        new_file.seek(0)\n",
    "        pdf_writer.write(new_file)\n",
    "        new_file.truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fa7ac-6605-4842-87f8-7cc844e01c12",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of [DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html) and splitting them into smaller chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80600818-b41c-45b2-b86b-2e4c69271ed6",
   "metadata": {},
   "source": [
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 512 tokens, which roughly translates to ~2000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f13cbcc0-908c-4fa3-adab-f9eae209cf92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Table of Contents\\nItem 7. Management’s Discussion and Analysis of Financial Condition and Results of Operations\\nForward-Looking Statements\\nThis Annual Report on Form 10-K includes forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995. All\\nstatements other than statements of historical fact, including statements regarding guidance, industry prospects, or future results of operations or financial\\nposition, made in this Annual Report on Form 10-K are forward-looking. We use words such as anticipates, believes, expects, future, intends, and similar\\nexpressions to identify forward-looking statements. Forward-looking statements reflect management’s current expectations and are inherently uncertain. Actual\\nresults and outcomes could differ materially for a variety of reasons, including, among others, fluctuations in foreign exchange rates, changes in global' metadata={'year': 2024, 'source': 'AMZN-2024-10-K-Annual-Report.pdf'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "\n",
    "for idx, file in enumerate(filenames):\n",
    "    loader = PyPDFLoader(data_root + file)\n",
    "    document = loader.load()\n",
    "    for document_fragment in document:\n",
    "        document_fragment.metadata = metadata[idx]\n",
    "\n",
    "    documents += document\n",
    "\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(docs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27bc3e-9d2b-47cc-9764-9c8b16200b06",
   "metadata": {},
   "source": [
    "Before we are proceeding we are looking into some interesting statistics regarding the document preprocessing we just performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a1d6183e-9ceb-429c-8042-935d56acf4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 437 documents loaded is 3348 characters.\n",
      "After the split we have 1801 documents as opposed to the original 437.\n",
      "Average length among 1801 documents (after split) is 823 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_doc_length(documents)} characters.')\n",
    "print(f'After the split we have {len(docs)} documents as opposed to the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_doc_length(docs)} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55e72f-6162-4aa5-9aa9-0bbb29b026ea",
   "metadata": {},
   "source": [
    "We had 4 PDF documents which have been split into smaller ~500 chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b550cd-1f0f-445b-9c3b-dbd7abf5294f",
   "metadata": {},
   "source": [
    "Now we can see how a sample embedding would look like for one of those chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f67cd64a-ba7a-419e-953a-307704772f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding of a document chunk:  [-0.01814743  0.01363395  0.02249585 ... -0.01061721  0.02277609\n",
      "  0.0064105 ]\n",
      "Size of the embedding:  (1024,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(sagemaker_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967efb1a-8586-4a74-bbd4-b52d1730693b",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can use [OpenSearch](https://aws.amazon.com/opensearch-service/) implementation with [LangChain](https://python.langchain.com/v0.2/api_reference/community/vectorstores/langchain_community.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html) to ingest the documents to OpenSearch service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "375f1ae6-e5c8-4283-a93c-96a57432cbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-2'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Provide the Opensearch url here \n",
    "import os\n",
    "os.environ['OPENSEARCH_URL'] = \"https://search-opensearchservi-6ecgcoz36wdo-4ji2ssdzpwu55ojmnxbxpsxgwi.us-east-2.es.amazonaws.com\"\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d27ba0e2-a01c-404c-8d38-15c4dec063a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIAQUKFTCLE4YXPQVF4\n"
     ]
    },
    {
     "ename": "AuthorizationException",
     "evalue": "AuthorizationException(403, 'security_exception', 'no permissions for [indices:admin/get] and User [name=arn:aws:iam::043632497353:role/service-role/AmazonSageMaker-ExecutionRole-20240923T140787, backend_roles=[arn:aws:iam::043632497353:role/service-role/AmazonSageMaker-ExecutionRole-20240923T140787], requestedTenant=null]')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthorizationException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndexWrapper\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Initialize OpenSearchVectorSearch\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m vectorstore_opensearch \u001b[38;5;241m=\u001b[39m \u001b[43mOpenSearchVectorSearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43msagemaker_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mawsauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Auth will use the IAM role\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_ssl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnection_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRequestsHttpConnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbulk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase this to accommodate the number of documents you have\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Wrap the OpenSearch vector store with the VectorStoreIndexWrapper\u001b[39;00m\n\u001b[1;32m     35\u001b[0m wrapper_store_opensearch \u001b[38;5;241m=\u001b[39m VectorStoreIndexWrapper(vectorstore\u001b[38;5;241m=\u001b[39mvectorstore_opensearch)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/vectorstores.py:550\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    549\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_community/vectorstores/opensearch_vector_search.py:1144\u001b[0m, in \u001b[0;36mOpenSearchVectorSearch.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, bulk_size, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct OpenSearchVectorSearch wrapper from raw texts.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \n\u001b[1;32m   1101\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \n\u001b[1;32m   1142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m-> 1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbulk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_community/vectorstores/opensearch_vector_search.py:1333\u001b[0m, in \u001b[0;36mOpenSearchVectorSearch.from_embeddings\u001b[0;34m(cls, embeddings, texts, embedding, metadatas, bulk_size, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1331\u001b[0m [kwargs\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys_list]\n\u001b[1;32m   1332\u001b[0m client \u001b[38;5;241m=\u001b[39m _get_opensearch_client(opensearch_url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1333\u001b[0m \u001b[43m_bulk_ingest_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_chunk_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_chunk_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_aoss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_aoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(opensearch_url, index_name, embedding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_community/vectorstores/opensearch_vector_search.py:153\u001b[0m, in \u001b[0;36m_bulk_ingest_embeddings\u001b[0;34m(client, index_name, embeddings, texts, metadatas, ids, vector_field, text_field, mapping, max_chunk_bytes, is_aoss)\u001b[0m\n\u001b[1;32m    150\u001b[0m mapping \u001b[38;5;241m=\u001b[39m mapping\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m not_found_error:\n\u001b[1;32m    155\u001b[0m     client\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mcreate(index\u001b[38;5;241m=\u001b[39mindex_name, body\u001b[38;5;241m=\u001b[39mmapping)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/client/utils.py:177\u001b[0m, in \u001b[0;36mquery_params.<locals>._wrapper.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    176\u001b[0m         params[p] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(p)\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/client/indices.py:188\u001b[0m, in \u001b[0;36mIndicesClient.get\u001b[0;34m(self, index, params, headers)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m SKIP_IN_PATH:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty value passed for a required argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_make_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/transport.py:407\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# connection didn't fail, confirm it's live status\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_pool\u001b[38;5;241m.\u001b[39mmark_live(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/transport.py:368\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    365\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_connection()\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     status, headers_response, data \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# Lowercase all the header names for consistency in accessing them.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     headers_response \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    380\u001b[0m         header\u001b[38;5;241m.\u001b[39mlower(): value \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers_response\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    381\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/connection/http_requests.py:203\u001b[0m, in \u001b[0;36mRequestsHttpConnection.perform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore\n\u001b[1;32m    193\u001b[0m ):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_request_fail(\n\u001b[1;32m    195\u001b[0m         method,\n\u001b[1;32m    196\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m         raw_data,\n\u001b[1;32m    202\u001b[0m     )\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_request_success(\n\u001b[1;32m    210\u001b[0m     method,\n\u001b[1;32m    211\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m     duration,\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mheaders, raw_data\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/opensearchpy/connection/base.py:300\u001b[0m, in \u001b[0;36mConnection._raise_error\u001b[0;34m(self, status_code, raw_data, content_type)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    298\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndecodable raw error response from server: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, err)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[38;5;241m.\u001b[39mget(status_code, TransportError)(\n\u001b[1;32m    301\u001b[0m     status_code, error_message, additional_info\n\u001b[1;32m    302\u001b[0m )\n",
      "\u001b[0;31mAuthorizationException\u001b[0m: AuthorizationException(403, 'security_exception', 'no permissions for [indices:admin/get] and User [name=arn:aws:iam::043632497353:role/service-role/AmazonSageMaker-ExecutionRole-20240923T140787, backend_roles=[arn:aws:iam::043632497353:role/service-role/AmazonSageMaker-ExecutionRole-20240923T140787], requestedTenant=null]')"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import boto3\n",
    "\n",
    "##Make sure the execution role has permissions for Opensearch\n",
    "service = \"es\"\n",
    "region_4Auth = region\n",
    "credentials = boto3.Session().get_credentials()\n",
    "print(credentials.access_key)\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region_4Auth,\n",
    "    service,\n",
    "    session_token=credentials.token\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "# Initialize OpenSearchVectorSearch\n",
    "vectorstore_opensearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    sagemaker_embeddings,\n",
    "    http_auth=awsauth,  # Auth will use the IAM role\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    bulk_size=2000  # Increase this to accommodate the number of documents you have\n",
    ")\n",
    "\n",
    "# Wrap the OpenSearch vector store with the VectorStoreIndexWrapper\n",
    "wrapper_store_opensearch = VectorStoreIndexWrapper(vectorstore=vectorstore_opensearch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95675c9-7116-4bd3-ba63-30963750e36f",
   "metadata": {},
   "source": [
    "## Question Answering with LangChain Vector Store Wrapper\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d335bb-63bf-4870-8e6e-019a1b7c005d",
   "metadata": {
    "tags": []
   },
   "source": [
    "We use the wrapper provided by LangChain which wraps around the Vector Store and takes input the LLM. This wrapper performs the following steps behind the scences:\n",
    "\n",
    "- Takes input the question\n",
    "- Create question embedding\n",
    "- Fetch relevant documents\n",
    "- Stuff the documents and the question into a prompt\n",
    "- Invoke the model with the prompt and generate the answer in a human readable manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2ff92-84dd-4ad7-856c-643e342cf5cd",
   "metadata": {},
   "source": [
    "*Note: In this example we are using `Llama 3 8B Instruct` as the LLM under Amazon SageMaker, this particular model performs best if the inputs are provided under `<|begin_of_text|><|start_header_id|>system<|end_header_id|>`, `{{system_message}}`, `<|eot_id|><|start_header_id|>user<|end_header_id|>`, `{{user_message}}`, and the model is requested to generate an output after `<|eot_id|><|start_header_id|>assistant<|end_header_id|>`. In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d5188-4caa-414c-a76e-33ae8cec19ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{query}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"query\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa490dbc-9abe-4bb6-a26c-26f6d10591ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How did AWS perform in 2021?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37091fb6-02e7-4601-8210-83129076a87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = wrapper_store_opensearch.query(question=PROMPT.format(query=query), llm=llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964bc809-6c6b-4989-8835-1cc7fcc731f7",
   "metadata": {},
   "source": [
    "We can ask another question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94688dd-2ef4-462e-9186-1a828b02e558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_2 = \"How much square footage did Amazon have in North America in 2023?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac316dbb-94c8-4bb6-872a-6c027df8f9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = wrapper_store_opensearch.query(question=PROMPT.format(query=query_2), llm=llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187b411-b7ce-48e7-bda4-d8d2abcefc53",
   "metadata": {},
   "source": [
    "### Regular Retriever Chain\n",
    "---\n",
    "In the above scenario you explored the quick and easy way to get a context-aware answer to your question. Now let's have a look at a more customizable option with the help of [RetrievalQA](https://docs.smith.langchain.com/cookbook/hub-examples/retrieval-qa-chain) where you can customize how the documents fetched should be added to prompt using `chain_type` parameter. Also, if you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using `return_source_documents` which returns the documents that are added to the context of the LLM prompt. `RetrievalQA` also allows you to provide a custom [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/) which can be specific to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b9a12-3407-47ce-8457-437059d84788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "This is a conversation between an AI assistant and a Human.\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "#### Context ####\n",
    "{context}\n",
    "#### End of Context ####\n",
    "\n",
    "Question: {question}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_opensearch.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff4d62-82a5-4f07-9ed4-828b468b6356",
   "metadata": {},
   "source": [
    "Let's start asking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304e6ab-a8ed-4d85-be9b-35ed60721a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How did AWS perform in 2023?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba165d2c-8ee5-40e7-91b5-94be5bdc9fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are some of the risk factors associated to Amazon?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4a7f8-6bc5-4ede-bacb-ffcd656e9e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Was Amazon involved in any lawsuits in 2022? What were they?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff4635-7987-42a2-aea6-b3d90110c7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What was Amazon's revenue in 2021?\"\n",
    "\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "print(result['result'])\n",
    "\n",
    "print(f\"\\n{result['source_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e4c55-0f80-47eb-83f7-6ae28fedf57f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32616f-9b97-4960-9ccc-47f20a95dcc6",
   "metadata": {},
   "source": [
    "Congratulations on completing the Retrieval Augmented Generation(RAG) notebook with `Llama3 8b`! Through this notebook, you were able to learn how to leverage the power of `Llama3 8b` with  `LangChain` and `OpenSearch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d260d-8771-41ae-b173-0f81228b28dc",
   "metadata": {},
   "source": [
    "In the above implementation of Advanced RAG based Question Answering we have explored the following concepts and how to implement them using Amazon SageMaker JumpStart and it's LangChain integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d08cb-6828-4089-a8fc-55c12637a2f1",
   "metadata": {},
   "source": [
    "- Deploying models on Amazon SageMaker JumpStart\n",
    "- Setting up `Llama3-8b` and `BGE Large En v1.5` with LangChain\n",
    "- Loading documents of different kind and generating embeddings to create a vector store\n",
    "- Retrieving documents to the question using the following approaches from LangChain\n",
    "    - Regular Retrieval Chain\n",
    "- Preparing a prompt which goes as input to the LLM\n",
    "- Present an answer in a human friendly manner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0319f2-5309-45ba-a15c-2ff7b3720133",
   "metadata": {},
   "source": [
    "### Take-aways\n",
    "---\n",
    "- Experiment with different retrieval techniques\n",
    "- Leverage `Llama3-8b` and `BGE Large En v1.5` models available under Amazon SageMaker JumpStart\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Integration with enterprise data stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e4cb2-5ad3-439c-90ea-f538bc9872e8",
   "metadata": {},
   "source": [
    "## Clean Up Resources\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d7aae-56a6-4c63-8cea-9ac73b930eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete resources\n",
    "llm_predictor.delete_model()\n",
    "llm_predictor.delete_endpoint()\n",
    "embedding_predictor.delete_model()\n",
    "embedding_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7dc0f-cf00-4bd7-a20c-8952ef75fa86",
   "metadata": {},
   "source": [
    "# Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
