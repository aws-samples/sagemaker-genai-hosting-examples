{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4cae57",
   "metadata": {},
   "source": [
    "\n",
    "# Deploying Llama-4 Scout on SageMaker with \n",
    "\n",
    "This notebook demonstrates deploying and running inference with the Llama-4 Scout model. We will cover \n",
    "\n",
    "1. Installing SageMaker python SDK, Setting up SageMaker resources and permissions\n",
    "2. Deploying the model using SageMaker LMI (Large Model Inference Container powered by Vllm 0.8.4)\n",
    "3. Invoking the model using streaming responses\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, we'll install the SageMaker SDK to ensure compatibility with the latest features, particularly those needed for large language model deployment and streaming inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e49ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca4505-aa3b-477b-9b90-aa3ef8f26576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb6e81",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Llama-4, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container with vLLM V1-0.8.4** : A container optimized for large language model inference\n",
    "- **P5 Instance**: AWS's latest GPU instance type optimized for large model inference\n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.p5.48xlarge` instances which offer:\n",
    "  - 8 NVIDIA H100 GPUs\n",
    "  - 640 GB of memory\n",
    "  - High network bandwidth for optimal inference performance\n",
    "\n",
    "> **Note**: The region in the container URI should match your AWS region. Replace `us-east-2` with your region if different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a91b2-29c3-4056-944f-130759ec5f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define region where you have capacity\n",
    "REGION = 'us-east-1'  \n",
    "\n",
    "#Select the latest container. Check the link for the latest available version https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers \n",
    "CONTAINER_VERSION = '0.33.0-lmi15.0.0-cu128'\n",
    "\n",
    "# Construct container URI\n",
    "container_uri = f'763104351884.dkr.ecr.{REGION}.amazonaws.com/djl-inference:{CONTAINER_VERSION}'\n",
    "\n",
    "# Select instance type\n",
    "instance_type = \"ml.p5.48xlarge\"  # Alternative: \"ml.p5e.48xlarge\"\n",
    "\n",
    "# Validate region and print configuration\n",
    "if REGION != sess.boto_region_name:\n",
    "    print(f\"âš ï¸ Warning: Container region ({REGION}) differs from session region ({sess.boto_region_name})\")\n",
    "else:\n",
    "    print(f\"âœ… Region validation passed: {REGION}\")\n",
    "    \n",
    "print(f\"ðŸ“¦ Container URI: {container_uri}\")\n",
    "print(f\"ðŸ–¥ï¸ Instance Type: {instance_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfeb11-df3b-4b21-bb96-21c2390fad60",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "**Important**: Before you proceed, request access to the model In HuggingFace, the request should be approved in a few minutes,once request is approved, generate a HuggingFace token key and update it in the properties below\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- vllm env variables\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Env Variables** (`env`): Our variables for the model server\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346ff77-ad3a-4ee5-8ec9-f11a6114c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_config = {\n",
    "    \"HF_MODEL_ID\": \"meta-llama/Llama-4-Scout-17B-16E\",\n",
    "    \"HF_TOKEN\": \"\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"250000\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"8\",\n",
    "    \"OPTION_MODEL_LOADING_TIMEOUT\": \"1500\",\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecd197-91b2-4f36-bcec-2afb8c7c370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(image_uri=container_uri,\n",
    "              role=role,\n",
    "              env=vllm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba06783-5e04-4a7e-9dc9-0346535e85bc",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (P5 instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.p5.48xlarge` for high-performance inference\n",
    "- **Health Check Timeout**: 1800 seconds \n",
    "  - Extended timeout needed for large model loading\n",
    "  - Includes time for container setup and model initialization\n",
    "\n",
    "> âš ï¸ **Important**: \n",
    "> - Deployment can take upto 15 minutes\n",
    "> - Monitor the endpoint status in SageMaker Console and CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2741a6-b7e1-4b75-8cb2-95555fd27968",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = sagemaker.utils.name_from_base(\"Llama-4\")\n",
    "\n",
    "print(endpoint_name)\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout = 1800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e6565-53d5-4595-a0e1-4ed9ac799a71",
   "metadata": {},
   "source": [
    "## Running Inference requests to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a02b4d-1e4e-4fe6-b662-9e25e1c0f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "# Create SageMaker Runtime client\n",
    "smr_client = boto3.client('sagemaker-runtime')\n",
    "##Add your endpoint here \n",
    "endpoint_name = ''\n",
    "\n",
    "# Invoke with messages format\n",
    "body = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Name popular places to visit in London?\"}\n",
    "    ],\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 256,\n",
    "    \"stream\": True,\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "first_token_received = False\n",
    "ttft = None\n",
    "token_count = 0\n",
    "full_response = \"\"\n",
    "\n",
    "print(f\"Prompt: {body['messages'][0]['content']}\\n\")\n",
    "print(\"Response:\", end=' ', flush=True)\n",
    "\n",
    "# Invoke endpoint with streaming\n",
    "resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(body),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "# Process streaming response\n",
    "for event in resp['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        payload = event['PayloadPart']['Bytes'].decode()\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            if payload.startswith('data: '):\n",
    "                data = json.loads(payload[6:])  # Skip \"data: \" prefix\n",
    "            else:\n",
    "                data = json.loads(payload)\n",
    "            \n",
    "            token_count += 1\n",
    "            if not first_token_received:\n",
    "                ttft = time.time() - start_time\n",
    "                first_token_received = True\n",
    "            \n",
    "            # Handle different streaming response formats\n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                # Messages-compatible format\n",
    "                if 'delta' in data['choices'][0] and 'content' in data['choices'][0]['delta']:\n",
    "                    token_text = data['choices'][0]['delta']['content']\n",
    "                    full_response += token_text\n",
    "                    print(token_text, end='', flush=True)\n",
    "            elif 'token' in data and 'text' in data['token']:\n",
    "                # TGI format\n",
    "                token_text = data['token']['text']\n",
    "                full_response += token_text\n",
    "                print(token_text, end='', flush=True)\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            # Skip invalid JSON\n",
    "            continue\n",
    "\n",
    "end_time = time.time()\n",
    "total_latency = end_time - start_time\n",
    "\n",
    "print(\"\\n\\nMetrics:\")\n",
    "print(f\"Time to First Token (TTFT): {ttft:.2f} seconds if tokens received else 'No tokens received'\")\n",
    "print(f\"Total Tokens Generated: {token_count}\")\n",
    "print(f\"Total Latency: {total_latency:.2f} seconds\")\n",
    "#print(f\"\\nFull Response:\\n{full_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189c796-e112-4413-bcfb-8dc843b05892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "# Function to convert image to base64 data URI\n",
    "def image_to_base64_data_uri(file_path):\n",
    "    with open(file_path, \"rb\") as img_file:\n",
    "        base64_data = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "        return base64_data\n",
    "\n",
    "# Path to your PNG image\n",
    "image_path = \"./img/trip.png\"\n",
    "base64_image = image_to_base64_data_uri(image_path)\n",
    "\n",
    "# Create SageMaker Runtime client for invocation\n",
    "smr_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Update Your endpoint name\n",
    "endpoint_name = ''\n",
    "\n",
    "# Prepare request payload with image in OpenAI format\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this image in detail please.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_tokens\": 512\n",
    "}\n",
    "\n",
    "# Option 1: Non-streaming invocation\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a87940-7aac-471a-b1e1-c988b18eb5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Streaming response invocation\n",
    "streaming_payload = payload.copy()\n",
    "streaming_payload[\"stream\"] = True\n",
    "\n",
    "response_stream = smr_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(streaming_payload)\n",
    ")\n",
    "\n",
    "print(\"Response:\", end=' ', flush=True)\n",
    "full_response = \"\"\n",
    "\n",
    "for event in response_stream['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        \n",
    "        try:\n",
    "            # Handle SSE format (data: prefix)\n",
    "            if chunk.startswith('data: '):\n",
    "                data = json.loads(chunk[6:])  # Skip \"data: \" prefix\n",
    "            else:\n",
    "                data = json.loads(chunk)\n",
    "            \n",
    "            # Extract token based on OpenAI format\n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                if 'delta' in data['choices'][0] and 'content' in data['choices'][0]['delta']:\n",
    "                    token_text = data['choices'][0]['delta']['content']\n",
    "                    full_response += token_text\n",
    "                    print(token_text, end='', flush=True)\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c37be7-fba1-4af0-a317-31436331a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete endpoint\n",
    "\n",
    "#import boto3\n",
    "#import sagemaker\n",
    "\n",
    "# Initialize session\n",
    "#sess = sagemaker.Session()\n",
    "\n",
    "\n",
    "print(f\"Deleting SageMaker resources for endpoint: {endpoint_name}\")\n",
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9c846-fdc4-4167-af9d-67d7193d5115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
