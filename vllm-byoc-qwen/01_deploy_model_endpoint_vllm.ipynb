{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60500b71-7985-4149-941b-94449fcf3d87",
   "metadata": {},
   "source": [
    "# Deploying a model with vLLM to Amazon SageMaker AI for Inference\n",
    "\n",
    "This notebook guides you through the process of deploying any model supported by vLLM on SageMaker AI. The deployment process includes several key steps:\n",
    "\n",
    "1. **Environment Setup**: Installing necessary dependencies and configuring the AWS environment\n",
    "2. **Container Infrastructure**: Building and pushing a custom container to Amazon ECR\n",
    "3. **SageMaker Deployment**: Creating and deploying a SageMaker endpoint\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- AWS credentials configured with appropriate permissions\n",
    "- AWS CLI installed\n",
    "- vLLM model support\n",
    "\n",
    "**Important Notes**\n",
    "\n",
    "- The deployment uses an ml.g5.2xlarge instance which provides GPU acceleration necessary for efficient inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a26393-f759-42fb-987a-5cf84019c613",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, we'll install `jq`, a lightweight command-line JSON processor. This will be used to parse AWS metadata and credentials later in our deployment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1c3dd-bc7a-4915-8f42-4300cba86dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -qq -y jq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248133b-287a-499e-8d87-82c423bc9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 sagemaker pandas huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99831e-0ee6-43fa-a448-da9850ff4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68975c5a-8e26-4226-99d3-020231c2b174",
   "metadata": {},
   "source": [
    "Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae9a63-9628-4a58-ab72-2a6b2a95b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sagemaker.model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18a783-b392-420d-a04e-8bda4618bd51",
   "metadata": {},
   "source": [
    "## Initialize AWS Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44147bba-d2e6-4677-a1f6-8b9a2ffeb71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session(boto_session=boto3.Session(region_name=region))\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c2184-cfdc-4294-a5ad-5db89b61e0ba",
   "metadata": {},
   "source": [
    "## Build and Push Docker Inference Container\n",
    "\n",
    "Amazon SageMaker AI offers [three primary methods](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html) for deploying ML models to an SageMaker AI Inference Endpoint:\n",
    "1. Using pre-built SageMaker containers for standard frameworks like PyTorch or TensorFlow\n",
    "2. Modifying existing Docker containers with your own dependencies through a requirements.txt file\n",
    "3. Or creating completely custom containers that implements a web server listening for requests (/invocations) for maximum flexibility and control over dependencies and requirements.\n",
    "\n",
    "To run the fine-tuned models you will build our custom container and push it to Amazon Elastic Container Registry (ECR). The container will be built from our Dockerfile and pushed to ECR, making it available for SageMaker to use when deploying our endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ea352-6b31-41dc-bb37-52f647346e21",
   "metadata": {},
   "source": [
    "### Docker Installation\n",
    "\n",
    "To create our custom container for model serving, we first need Docker installed in our environment. This script handles the installation of Docker and its dependencies, including necessary security keys and repository configurations.\n",
    "\n",
    "**Install docker-cli**\n",
    "\n",
    "At the end of this install you should see,\n",
    "\n",
    "```bash\n",
    "Client: Docker Engine - Community\n",
    " Version:           20.10.24\n",
    " API version:       1.41\n",
    " Go version:        go1.19.7\n",
    " Git commit:        297e128\n",
    " Built:             Tue Apr  4 18:21:03 2023\n",
    " OS/Arch:           linux/amd64\n",
    " Context:           default\n",
    " Experimental:      true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d7d4f-9cbd-46a3-9cc7-05634c4a0efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash docker-artifacts/01_docker_install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca114b-fb6a-4b83-9474-2f9d4bd1530e",
   "metadata": {},
   "source": [
    "### Build and push a custom image\n",
    "\n",
    "SageMaker Inference supports simplified deployment of Qwen2 using Large Model Inference (LMI) container images as indicated [here](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) and available images listed [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).\n",
    "\n",
    "However, due to the fast release cycle of new models, it happens that the available LMI container doesn't support the new model yet. In these scenarios, we can use a more recent vLLM version which supports the new model and build a custom image inside SageMaker Studio using `docker-cli`. If needed, adjust the vLLM version (docker-artifacts/dockerfile) to fit your and model deployment. \n",
    "\n",
    "**Build our custom Docker image containing custom inference handler and push it to Amazon ECR (Elastic Container Registry).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538728f4-4c44-497a-a6aa-40e04857ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = \"vllm-sagemaker\"\n",
    "os.environ['REPO_NAME'] = REPO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee339f-3284-4f85-8862-c06759e10551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash -s {region} {account_id}\n",
    "\n",
    "REGION=$1\n",
    "\n",
    "VERSION_TAG=\"latest\"\n",
    "CURRENT_ACCOUNT_NUMBER=$2\n",
    "\n",
    "echo \"bash 02_build_and_push.sh $REPO_NAME $VERSION_TAG $REGION $CURRENT_ACCOUNT_NUMBER\"\n",
    "cd docker-artifacts && bash 02_build_and_push.sh $REPO_NAME $VERSION_TAG $REGION $CURRENT_ACCOUNT_NUMBER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da3271-aa4b-48b7-8d83-1129048357f3",
   "metadata": {},
   "source": [
    "### Getting Container Image URI\n",
    "\n",
    "Retrieve the full URI of our Docker image from ECR. This URI is essential for SageMaker deployment as it tells SageMaker exactly where to find our custom container image. The URI follows the format:\n",
    "`{account_id}.dkr.ecr.{region}.amazonaws.com/{repository_name}:{tag}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ce6f6-ac3c-4b12-87b6-647f800edf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{REPO_NAME}:latest\"\n",
    "print(f\"Base image to deploy a SageMaker endpoint: {image_uri}\")\n",
    "\n",
    "os.environ['CUSTOM_IMAGE'] = image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fe5ca-226a-4737-a451-399fbc709337",
   "metadata": {},
   "source": [
    "## Understanding the Model Serving Architecture\n",
    "\n",
    "When we deploy our model to a SageMaker endpoint, here's how the components work together:\n",
    "\n",
    "1. **Docker Container Structure**:\n",
    "   - The container runs on the SageMaker instance (ml.g5.2xlarge)\n",
    "\n",
    "2. **Request Flow**:\n",
    "   - External requests → SageMaker endpoint → Container's port 8080\n",
    "   - The `sed` commands we used modified the API paths to match SageMaker's expected structure:\n",
    "     - `/ping` for health checks\n",
    "     - `/invocations` for model inference\n",
    "     - `/invocations/completions` for completion requests\n",
    "\n",
    "3. **SageMaker Integration**:\n",
    "   - Routes HTTPS requests to our container\n",
    "   - Manages container lifecycle\n",
    "   - Handles authentication and scaling\n",
    "   - Monitors container health via the `/ping` endpoint\n",
    "\n",
    "This setup allows us to serve our fine-tuned model with production-grade reliability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1f1e5-1789-4218-83ce-af384449cff3",
   "metadata": {},
   "source": [
    "**[Optional] We can run our container interactively in Terminal by using the command below. Make sure you are using a GPU instance for your Jupyterlab space since model inference requires a GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776f0a3-76c3-487f-9fda-d28c4ad36140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_env_file(dict_data, output_file='output.env'):\n",
    "    \"\"\"\n",
    "    Convert dictionary to environment file format (VARIABLE=VALUE)\n",
    "    \n",
    "    Args:\n",
    "        dict_data (dict): Dictionary containing environment variables\n",
    "        output_file (str): Name of the output file (default: output.env)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            for key, value in dict_data.items():\n",
    "                # Convert value to string and escape special characters if needed\n",
    "                value_str = str(value).replace('\"', '\\\\\"')\n",
    "                # Write each variable in KEY=VALUE format\n",
    "                f.write(f'{key}={value_str}\\n')\n",
    "        print(f\"Successfully wrote environment variables to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to env file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416883b-2603-4ffb-996a-f254a925cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment variables for the model\n",
    "environment = {\n",
    "    \"HF_TOKEN\":\"your_token_here\"\n",
    "    # \"USE_HF_TRANSFER\": \"true\",  # Enable faster downloads\n",
    "    # \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n",
    "    \"SM_VLLM_MODEL\": \"Qwen/Qwen2.5-VL-3B-Instruct\", # you can name your model whatever you want    \n",
    "    \"SM_VLLM_LIMIT_MM_PER_PROMPT.IMAGE\": 2, # max number of images allowed in prompt. Increase for multi-page documents. Requires more memory.\n",
    "    \"SM_VLLM_LIMIT_MM_PER_PROMPT.VIDEO\": 0\n",
    "    \"SM_VLLM_MAX_NUM_SEQS\":\"8\", # decrease if less GPU memory available\n",
    "    \"SM_VLLM_MAX_MODEL_LEN\":\"38608\", # max context length, decrease if less GPU memory available\n",
    "    # \"SM_VLLM_MAX_MODEL_LEN\":\"10608\", # max context length, decrease if less GPU memory available\n",
    "    # \"SM_VLLM_DTYPE\": \"bfloat16\"\n",
    "}\n",
    "\n",
    "dict_to_env_file(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2f6cf4-ae0d-4b7a-a302-ca733f64c3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile run_container.sh\n",
    "REPO_NAME=$1\n",
    "\n",
    "# # Get credentials from instance metadata\n",
    "TOKEN=$(curl -s -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\")\n",
    "ROLE_NAME=$(curl -s -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/iam/security-credentials/)\n",
    "export $(curl -s -H \"X-aws-ec2-metadata-token: $TOKEN\" \\\n",
    "  http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE_NAME \\\n",
    "  | jq -r '\"AWS_ACCESS_KEY_ID=\"+.AccessKeyId, \"AWS_SECRET_ACCESS_KEY=\"+.SecretAccessKey, \"AWS_SESSION_TOKEN=\"+.Token')\n",
    "\n",
    "# # Now run your docker container with these environment variables\n",
    "# # Add --entrypoint /bin/bash in case you want to manually look into the container\n",
    "set -x\n",
    "docker run -d --gpus all --network host -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN --env-file output.env ${REPO_NAME}\n",
    "\n",
    "echo \"Waiting for container to be ready...\"\n",
    "max_attempts=60  # Maximum number of attempts (10 minutes with 10-second intervals)\n",
    "attempt=1\n",
    "\n",
    "while [ $attempt -le $max_attempts ]; do\n",
    "    echo \"Attempt $attempt of $max_attempts: Checking if container is ready...\"\n",
    "    \n",
    "    if curl -s -f http://localhost:8080/ping > /dev/null; then\n",
    "        echo \"Container is ready!\"\n",
    "        break\n",
    "    fi\n",
    "    \n",
    "    if [ $attempt -eq $max_attempts ]; then\n",
    "        echo \"Container failed to become ready within the timeout period\"\n",
    "        docker logs inference_container\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    attempt=$((attempt + 1))\n",
    "    sleep 10\n",
    "done\n",
    "\n",
    "# Test chat completions endpoint\n",
    "echo -e \"\\nTesting chat completions endpoint...\"\n",
    "curl -X POST http://localhost:8080/invocations \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "        ],\n",
    "        \"model\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 100\n",
    "    }'\n",
    "\n",
    "# Test completions endpoint\n",
    "echo -e \"\\nTesting completions endpoint...\"\n",
    "curl -X POST http://localhost:8080/invocations/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"prompt\": \"Hello, how are you?\",\n",
    "        \"model\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 100\n",
    "    }'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4a30e-6fa5-48a5-8c3a-be58f8821527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash run_container.sh \"{REPO_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d439b7e-321b-428f-a6c0-53b7fa821acc",
   "metadata": {},
   "source": [
    "## Creating a SageMaker Model and deploy a SageMaker endpoint\n",
    "\n",
    "Finally, we'll create a SageMaker model and deploy it to an inference endpoint. This will give us an HTTPS endpoint that we can use for inference.\n",
    "\n",
    "Note: We're using an ml.g5.2xlarge instance which provides GPU acceleration necessary for efficient inference with a small multimodal model.\n",
    "\n",
    "For more throughphut, lower latency, or when deploying a bigger model you might want to use a bigger instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7045db-ada0-4152-84b6-701b3e5495b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "sm_model_name = \"qwen25vl3b\"\n",
    "sm_endpoint_name = \"vllm-sagemaker-qwen\"\n",
    "\n",
    "print(f\"Model name: {sm_model_name}\")\n",
    "print(f\"Endpoint name: {sm_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95428db3-ce57-4068-9c29-6c3d871ce3c9",
   "metadata": {},
   "source": [
    "Deploy our model to a SageMaker endpoint using an ml.g5.2xlarge instance. This GPU-enabled instance type provides the computational power needed for efficient inference with a Qwen2-VL model. The deployment:\n",
    "- Creates a SageMaker model using our custom container\n",
    "- Configures the endpoint with specified resources\n",
    "- Initiates asynchronous deployment (wait=False)\n",
    "- Sets up HTTPS endpoint for inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074d5a3-41ba-4431-a01b-6c901f102fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main deployment logic\n",
    "from utils.helpers import check_model_exists, check_endpoint_config_exists, check_endpoint_exists, delete_all_resources\n",
    "\n",
    "endpoint_exists = check_endpoint_exists(endpoint_name=sm_endpoint_name, sm_client=sm_client)\n",
    "model_exists = check_model_exists(sm_model_name, sm_client=sm_client)\n",
    "config_exists = check_endpoint_config_exists(sm_endpoint_name, sm_client=sm_client)\n",
    "\n",
    "if endpoint_exists or model_exists or config_exists:\n",
    "    print(f\"\\nFound existing resources:\")\n",
    "    if endpoint_exists:\n",
    "        print(f\"- Endpoint: {sm_endpoint_name}\")\n",
    "    if model_exists:\n",
    "        print(f\"- Model: {sm_model_name}\")\n",
    "    if config_exists:\n",
    "        print(f\"- Endpoint config: {sm_endpoint_name}\")\n",
    "    \n",
    "    delete_all_resources(sm_model_name, sm_endpoint_name, sm_client=sm_client)\n",
    "\n",
    "# # Define environment variables for the model\n",
    "# environment = {\n",
    "#     # \"USE_HF_TRANSFER\": \"true\",  # Enable faster downloads\n",
    "#     # \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n",
    "#     \"SM_VLLM_MODEL\": hf_model, # you can name your model whatever you want\n",
    "#     # \"SM_VLLM_LIMIT_MM_PER_PROMPT\": \"image=2, video=0\", # max number of images allowed in prompt. Increase for multi-page documents. Requires more memory.\n",
    "#     # \"SM_VLLM_MAX_NUM_SEQS\":\"8\", # decrease if less GPU memory available\n",
    "#     # \"SM_VLLM_MAX_MODEL_LEN\":\"38608\", # max context length, decrease if less GPU memory available\n",
    "#     # \"SM_VLLM_DTYPE\": \"bfloat16\"\n",
    "# }\n",
    "\n",
    "# If we get here, either nothing existed or we've cleaned up\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    name=sm_model_name,\n",
    "    env=environment,\n",
    ")\n",
    "\n",
    "print(f\"\\nEndpoint is now being deployed.... This may take several minutes.\")\n",
    "\n",
    "# Deploy a new endpoint\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    # instance_type=\"local\",\n",
    "    endpoint_name=sm_endpoint_name,\n",
    "    wait=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff834f58-f5ea-4c9c-8d43-b3412b863b24",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "After deploying the model as a SageMaker endpoint, we can call the model endpoint to run inference with the sample code in the next notebook [02_consume_model.ipynb](./02_consume_model.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
