{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "# How to deploy OpenAI's whisper ASR ( automatic speech recognition ) model for inference on Amazon SageMakerAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e793b-0512-474e-8e82-6391d93fa0ec",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how to deploy **OpenAI's whisper-large-v2** model.(HuggingFace model ID [openai/whisper-large-v2](https://huggingface.co/openai/whisper-large-v2): using Amazon SageMaker AI. The inference image will be the SageMaker-managed [LMI (Large Model Inference) v15](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "Whisper Large v2 is a version of OpenAI's robust, multilingual speech-to-text model.It performs well on benchmarks like Common Voice and Fleurs, supports transcription and translation, and is suitable for applications like podcasting, video subtitles, and lecture note-taking. For more details please read [post](https://openai.com/index/whisper/).\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- Multilingual and Multitask:\n",
    "Whisper is trained on a massive dataset of multilingual audio, allowing it to transcribe in many languages and even translate speech into English. \n",
    "- Robustness:\n",
    "It's designed to be resilient to various acoustic challenges, including different accents and noisy environments. \n",
    "- Improved Performance:\n",
    "Large v2 achieved better performance over large v1, with relative error reductions in English (around 5%) and other languages (around 10%). \n",
    "- End-to-End Transformer:\n",
    "The model uses an encoder-decoder transformer architecture, processing audio by converting it into a log-Mel spectrogram before feeding it to the encoder. \n",
    "\n",
    "### Usage\n",
    "\n",
    "We provide a reference implementation of whisper-large-v2 , as well as sampling code, in a dedicated github repository. Developers and creatives looking to build on top of whisper-large-v2 are encouraged to use this as a starting point.\n",
    "\n",
    "### Out-of-Scope Use \n",
    "The model and its derivatives may not be used\n",
    "\n",
    "- In any way that violates any applicable national, federal, state, local or international law or regulation.\n",
    "- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\n",
    "- To generate or disseminate verifiably false information and/or content with the purpose of harming others.\n",
    "- To generate or disseminate personal identifiable information that can be used to harm an individual.\n",
    "- To harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\n",
    "- To create non-consensual nudity or illegal pornographic content.\n",
    "- For fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.\n",
    "- Generating or facilitating large-scale disinformation campaigns.\n",
    "\n",
    "\n",
    "### License agreement\n",
    "* This model is open source on HuggingFace, please refer to the original [model card](https://huggingface.co/openai/whisper-large-v2)\n",
    "* This notebook is a sample notebook and not intended for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c01a4b2-b0d3-420c-8335-4efd984bb5dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:12:55.947397Z",
     "iopub.status.busy": "2025-08-30T02:12:55.947127Z",
     "iopub.status.idle": "2025-08-30T02:12:56.821558Z",
     "shell.execute_reply": "2025-08-30T02:12:56.820864Z",
     "shell.execute_reply.started": "2025-08-30T02:12:55.947376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:13:01.299068Z",
     "iopub.status.busy": "2025-08-30T02:13:01.298789Z",
     "iopub.status.idle": "2025-08-30T02:13:01.608128Z",
     "shell.execute_reply": "2025-08-30T02:13:01.607611Z",
     "shell.execute_reply.started": "2025-08-30T02:13:01.299044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::992382553328:role/amazon-sagemaker-base-executionrole\n",
      "sagemaker bucket: sagemaker-us-west-2-992382553328\n",
      "sagemaker session region: us-west-2\n",
      "sagemaker version: 2.251.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063eecf-fc9f-4b64-8dfe-a394e178ec75",
   "metadata": {},
   "source": [
    "## HF container with default handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc183a7c-bb78-43c0-b477-7659ba3240df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:13:09.319809Z",
     "iopub.status.busy": "2025-08-30T02:13:09.319545Z",
     "iopub.status.idle": "2025-08-30T02:19:12.503189Z",
     "shell.execute_reply": "2025-08-30T02:19:12.502675Z",
     "shell.execute_reply.started": "2025-08-30T02:13:09.319788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "model_name = sagemaker.utils.name_from_base(\"model\")\n",
    "endpoint_name = model_name\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t\"HF_MODEL_ID\": \"openai/whisper-large-v2\",\n",
    "\t\"HF_TASK\": \"automatic-speech-recognition\"\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\tname=model_name,\n",
    "    transformers_version='4.49.0',\n",
    "\tpytorch_version='2.6.0',\n",
    "\tpy_version='py312',\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.g5.12xlarge', # ec2 instance type\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9e902-b3ac-4785-aae6-1742eccf14dd",
   "metadata": {},
   "source": [
    "### Download audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4325ccec-5a67-4018-bdb2-bcb40c970812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:19:20.416219Z",
     "iopub.status.busy": "2025-08-30T02:19:20.415904Z",
     "iopub.status.idle": "2025-08-30T02:19:20.639919Z",
     "shell.execute_reply": "2025-08-30T02:19:20.639396Z",
     "shell.execute_reply.started": "2025-08-30T02:19:20.416196Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart import utils\n",
    "\n",
    "# The wav files must be sampled at 16kHz (this is required by the automatic speech recognition models), so make sure to resample them if required. The input audio file must be less than 30 seconds.\n",
    "s3_bucket = utils.get_jumpstart_content_bucket()\n",
    "key_prefix = \"training-datasets/asr_notebook_data\"\n",
    "input_audio_file_name = \"sample1.wav\"\n",
    "\n",
    "s3_client.download_file(s3_bucket, f\"{key_prefix}/{input_audio_file_name }\", input_audio_file_name)\n",
    "\n",
    "input_audio_file_name = \"sample_french1.wav\"\n",
    "\n",
    "s3_client.download_file(s3_bucket, f\"{key_prefix}/{input_audio_file_name }\", input_audio_file_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "469cdedd-6b88-4335-bd70-d99e0b6e9b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:19:32.635916Z",
     "iopub.status.busy": "2025-08-30T02:19:32.635645Z",
     "iopub.status.idle": "2025-08-30T02:19:35.137925Z",
     "shell.execute_reply": "2025-08-30T02:19:35.137421Z",
     "shell.execute_reply.started": "2025-08-30T02:19:32.635896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" Bienvenue chez JPB Systèmes, ici. C'est plus de 150 collaborateurs, c'est plus de 90% de chiffre d'affaires à l'export et d'un produit, c'est une quinzaine de preuves que nous avons développées.\"}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.serializers import DataSerializer\n",
    "\t\n",
    "predictor.serializer = DataSerializer(content_type='audio/x-audio')\n",
    "predictor.content_type = \"audio/x-audio\"\n",
    "\n",
    "# Make sure the input file \"sample1.flac\" exists\n",
    "with open(input_audio_file_name, \"rb\") as f:\n",
    "\tdata = f.read()\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8ceb5da-3499-41cd-b532-0e43ed485d32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:19:38.955431Z",
     "iopub.status.busy": "2025-08-30T02:19:38.955174Z",
     "iopub.status.idle": "2025-08-30T02:19:39.625776Z",
     "shell.execute_reply": "2025-08-30T02:19:39.625188Z",
     "shell.execute_reply.started": "2025-08-30T02:19:38.955411Z"
    }
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634e71f-a190-408f-bcb9-a314cd057e30",
   "metadata": {},
   "source": [
    "## HF container with custom handler\n",
    "\n",
    "Model is deployed from HF hub. Custom handler is placed on S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979156c8-ef65-4cb7-9fec-fa737f1163be",
   "metadata": {},
   "source": [
    "## Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "If you are deploying a model hosted on the HuggingFace Hub, you must specify the `option.model_id=<hf_hub_model_id>` configuration. When using a model directly from the hub, we recommend you also specify the model revision (commit hash or branch) via `option.revision=<commit hash/branch>`. *Here we are using the env variable during deployment instead of serving.properties file*\n",
    "\n",
    "Since model artifacts are downloaded at runtime from the Hub, using a specific revision ensures you are using a model compatible with package versions in the runtime environment. Open Source model artifacts on the hub are subject to change at any time. These changes may cause issues when instantiating the model (updated model artifacts may require a newer version of a dependency than what is bundled in the container). If a model provides custom model (modeling.py) and/or custom tokenizer (tokenizer.py) files, you need to specify option.trust_remote_code=true to load and use the model.\n",
    "\n",
    "In this example, we will demonstrate how to download your copy of the model from huggingface and upload it to an s3 location in your AWS account, then deploy the model with the downloaded model artifacts to an endpoint.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646cf08-e162-4b0e-9f2c-f3350987c85b",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    "> **Store Models in Your Own S3 Bucket**\n",
    "For production use-cases, always download and store model files in your own S3 bucket to ensure validated artifacts. This provides verified provenance, improved access control, consistent availability, protection against upstream changes, and compliance with organizational security protocols.\n",
    ">\n",
    ">> ⚠️ **Important**: \n",
    "> - Downloading filescan take time. Please ensure this step completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08f4849c-6e8b-45fd-9771-169720b103c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:22:49.160394Z",
     "iopub.status.busy": "2025-08-30T02:22:49.160235Z",
     "iopub.status.idle": "2025-08-30T02:22:49.316112Z",
     "shell.execute_reply": "2025-08-30T02:22:49.315565Z",
     "shell.execute_reply.started": "2025-08-30T02:22:49.160376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1dbc840fe6449990820005f9dcad8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/sagemaker-genai-hosting-examples/workshop/inference/lab4/Whishper/model-files'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "model_dir = Path('model-files')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "HF_MODEL_ID = \"openai/whisper-large-v2\"\n",
    "snapshot_download(HF_MODEL_ID, local_dir=model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d236c6ff-b4aa-4a85-b866-a7f6df5e9a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:23:12.571424Z",
     "iopub.status.busy": "2025-08-30T02:23:12.571163Z",
     "iopub.status.idle": "2025-08-30T02:23:12.575277Z",
     "shell.execute_reply": "2025-08-30T02:23:12.574768Z",
     "shell.execute_reply.started": "2025-08-30T02:23:12.571404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whisper-large-v2'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_name = HF_MODEL_ID.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = HF_MODEL_ID.split(\"/\")[0]\n",
    "base_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054da0dd-da73-4b56-b9d3-8c258f744a14",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    "> **Note**: When your model and configuration files are in different S3 locations, set `option.model_id=<s3_model_uri>` in your serving.properties file, where `s3_model_uri` is the S3 object prefix containing your model artifacts. SageMaker AI will automatically download the model files by looking at the S3URI in model_id\n",
    ">*Here we are using the env variable during deployment instead of serving.properties file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a24cf309-00e3-4499-a1f1-8129a5689dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:23:17.495298Z",
     "iopub.status.busy": "2025-08-30T02:23:17.495039Z",
     "iopub.status.idle": "2025-08-30T02:23:17.498966Z",
     "shell.execute_reply": "2025-08-30T02:23:17.498478Z",
     "shell.execute_reply.started": "2025-08-30T02:23:17.495278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./model-files/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-files/requirements.txt\n",
    "openai-whisper\n",
    "ffmpeg\n",
    "torchaudio\n",
    "nvgpu\n",
    "transformers>=4.46.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10d2154b-5a19-4a8a-b153-ffcdbbb37d64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:23:21.667247Z",
     "iopub.status.busy": "2025-08-30T02:23:21.666982Z",
     "iopub.status.idle": "2025-08-30T02:23:21.675664Z",
     "shell.execute_reply": "2025-08-30T02:23:21.675201Z",
     "shell.execute_reply.started": "2025-08-30T02:23:21.667226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./model-files/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-files/inference.py\n",
    "import ast\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from sagemaker_inference import encoder\n",
    "from scipy.io.wavfile import read\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import WhisperProcessor\n",
    "from transformers.pipelines.audio_utils import ffmpeg_read\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "AUDIO_WAV = \"audio/wav\"\n",
    "APPLICATION_JSON = \"application/json\"\n",
    "STR_DECODE_CODE = \"utf-8\"\n",
    "\n",
    "VERBOSE_EXTENSION = \";verbose\"\n",
    "\n",
    "AUTOMATIC_SPEECH_RECOGNITION = \"automatic-speech-recognition\"\n",
    "TEXT = \"text\"\n",
    "\n",
    "SUPPORTED_LANGUAGES = [\n",
    "    \"english\",\n",
    "    \"chinese\",\n",
    "    \"german\",\n",
    "    \"spanish\",\n",
    "    \"russian\",\n",
    "    \"korean\",\n",
    "    \"french\",\n",
    "    \"japanese\",\n",
    "    \"portuguese\",\n",
    "    \"turkish\",\n",
    "    \"polish\",\n",
    "    \"catalan\",\n",
    "    \"dutch\",\n",
    "    \"arabic\",\n",
    "    \"swedish\",\n",
    "    \"italian\",\n",
    "    \"indonesian\",\n",
    "    \"hindi\",\n",
    "    \"finnish\",\n",
    "    \"vietnamese\",\n",
    "    \"hebrew\",\n",
    "    \"ukrainian\",\n",
    "    \"greek\",\n",
    "    \"malay\",\n",
    "    \"czech\",\n",
    "    \"romanian\",\n",
    "    \"danish\",\n",
    "    \"hungarian\",\n",
    "    \"tamil\",\n",
    "    \"norwegian\",\n",
    "    \"thai\",\n",
    "    \"urdu\",\n",
    "    \"croatian\",\n",
    "    \"bulgarian\",\n",
    "    \"lithuanian\",\n",
    "    \"latin\",\n",
    "    \"maori\",\n",
    "    \"malayalam\",\n",
    "    \"welsh\",\n",
    "    \"slovak\",\n",
    "    \"telugu\",\n",
    "    \"persian\",\n",
    "    \"latvian\",\n",
    "    \"bengali\",\n",
    "    \"serbian\",\n",
    "    \"azerbaijani\",\n",
    "    \"slovenian\",\n",
    "    \"kannada\",\n",
    "    \"estonian\",\n",
    "    \"macedonian\",\n",
    "    \"breton\",\n",
    "    \"basque\",\n",
    "    \"icelandic\",\n",
    "    \"armenian\",\n",
    "    \"nepali\",\n",
    "    \"mongolian\",\n",
    "    \"bosnian\",\n",
    "    \"kazakh\",\n",
    "    \"albanian\",\n",
    "    \"swahili\",\n",
    "    \"galician\",\n",
    "    \"marathi\",\n",
    "    \"punjabi\",\n",
    "    \"sinhala\",\n",
    "    \"khmer\",\n",
    "    \"shona\",\n",
    "    \"yoruba\",\n",
    "    \"somali\",\n",
    "    \"afrikaans\",\n",
    "    \"occitan\",\n",
    "    \"georgian\",\n",
    "    \"belarusian\",\n",
    "    \"tajik\",\n",
    "    \"sindhi\",\n",
    "    \"gujarati\",\n",
    "    \"amharic\",\n",
    "    \"yiddish\",\n",
    "    \"lao\",\n",
    "    \"uzbek\",\n",
    "    \"faroese\",\n",
    "    \"haitian creole\",\n",
    "    \"pashto\",\n",
    "    \"turkmen\",\n",
    "    \"nynorsk\",\n",
    "    \"maltese\",\n",
    "    \"sanskrit\",\n",
    "    \"luxembourgish\",\n",
    "    \"myanmar\",\n",
    "    \"tibetan\",\n",
    "    \"tagalog\",\n",
    "    \"malagasy\",\n",
    "    \"assamese\",\n",
    "    \"tatar\",\n",
    "    \"hawaiian\",\n",
    "    \"lingala\",\n",
    "    \"hausa\",\n",
    "    \"bashkir\",\n",
    "    \"javanese\",\n",
    "    \"sundanese\",\n",
    "    \"burmese\",\n",
    "    \"valencian\",\n",
    "    \"flemish\",\n",
    "    \"haitian\",\n",
    "    \"letzeburgesch\",\n",
    "    \"pushto\",\n",
    "    \"panjabi\",\n",
    "    \"moldavian\",\n",
    "    \"moldovan\",\n",
    "    \"sinhalese\",\n",
    "    \"castilian\",\n",
    "]\n",
    "SUPPORTED_TASKS = [\"translate\", \"transcribe\"]\n",
    "\n",
    "# Audio Parameters\n",
    "AUDIO_INPUT = \"audio_input\"\n",
    "LANGUAGE = \"language\"\n",
    "TASK = \"task\"\n",
    "FORCED_DECODER_IDS = \"forced_decoder_ids\"\n",
    "\n",
    "# Text Generation parameters\n",
    "MAX_LENGTH = \"max_length\"\n",
    "NUM_RETURN_SEQUENCES = \"num_return_sequences\"\n",
    "NUM_BEAMS = \"num_beams\"\n",
    "TOP_P = \"top_p\"\n",
    "EARLY_STOPPING = \"early_stopping\"\n",
    "DO_SAMPLE = \"do_sample\"\n",
    "NO_REPEAT_NGRAM_SIZE = \"no_repeat_ngram_size\"\n",
    "TOP_K = \"top_k\"\n",
    "TEMPERATURE = \"temperature\"\n",
    "MIN_LENGTH = \"min_length\"\n",
    "MIN_NEW_TOKENS = \"min_new_tokens\"\n",
    "MAX_NEW_TOKENS = \"max_new_tokens\"\n",
    "LENGTH_PENALTY = \"length_penalty\"\n",
    "MAX_TIME = \"max_time\"\n",
    "\n",
    "\n",
    "ALL_PARAM_NAMES = [\n",
    "    AUDIO_INPUT,\n",
    "    LANGUAGE,\n",
    "    TASK,\n",
    "    FORCED_DECODER_IDS,\n",
    "    MAX_LENGTH,\n",
    "    NUM_RETURN_SEQUENCES,\n",
    "    NUM_BEAMS,\n",
    "    TOP_P,\n",
    "    EARLY_STOPPING,\n",
    "    DO_SAMPLE,\n",
    "    NO_REPEAT_NGRAM_SIZE,\n",
    "    TOP_K,\n",
    "    TEMPERATURE,\n",
    "    MIN_LENGTH,\n",
    "    MAX_NEW_TOKENS,\n",
    "    MIN_NEW_TOKENS,\n",
    "    LENGTH_PENALTY,\n",
    "    MAX_TIME,\n",
    "]\n",
    "\n",
    "# Model parameter ranges\n",
    "LENGTH_MIN = 1\n",
    "NUM_RETURN_SEQUENCE_MIN = 1\n",
    "NUM_BEAMS_MIN = 1\n",
    "TOP_P_MIN = 0\n",
    "TOP_P_MAX = 1\n",
    "NO_REPEAT_NGRAM_SIZE_MIN = 1\n",
    "TOP_K_MIN = 0\n",
    "TEMPERATURE_MIN = 0\n",
    "NEW_TOKENS_MIN = 0\n",
    "\n",
    "\n",
    "def is_list_of_strings(parameter: Any) -> bool:\n",
    "    \"\"\"Return True if the parameter is a list of strings.\"\"\"\n",
    "    if parameter and isinstance(parameter, list):\n",
    "        return all(isinstance(elem, str) for elem in parameter)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _validate_payload(payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Validate the parameters in the input loads.\n",
    "\n",
    "    Checks if max_length, num_return_sequences, num_beams, top_p and temprature are in bounds.\n",
    "    Checks if do_sample is boolean.\n",
    "    Checks max_length, num_return_sequences and num_beams integers.\n",
    "\n",
    "    Args:\n",
    "        payload: a decoded input payload (dictionary of input parameter and values)\n",
    "\n",
    "    Raises: ValueError is any of the check fails.\n",
    "    \"\"\"\n",
    "    # For all parameters used in generation task, please see\n",
    "    # https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "    for param_name in payload:\n",
    "        if param_name not in ALL_PARAM_NAMES:\n",
    "            raise ValueError(f\"Input payload contains an invalid key '{param_name}'. Valid keys are {ALL_PARAM_NAMES}.\")\n",
    "\n",
    "    if AUDIO_INPUT not in payload:\n",
    "        raise ValueError(f\"Input payload must contain {AUDIO_INPUT} key.\")\n",
    "\n",
    "    if LANGUAGE in payload:\n",
    "        value = payload[LANGUAGE]\n",
    "        if type(value) != str:\n",
    "            raise ValueError(f\"{LANGUAGE} must be a string, got {value}.\")\n",
    "        value = value.lower()\n",
    "        payload[LANGUAGE] = value\n",
    "        if value not in SUPPORTED_LANGUAGES:\n",
    "            raise ValueError(\n",
    "                f\"Input payload contains an invalid language {value}. \"\n",
    "                f\"Valid languages are {SUPPORTED_LANGUAGES}.\"\n",
    "            )\n",
    "        if TASK not in payload:\n",
    "            raise ValueError(\"Input payload should contain both language and task\")\n",
    "\n",
    "    if TASK in payload:\n",
    "        value = payload[TASK]\n",
    "        if type(value) != str:\n",
    "            raise ValueError(f\"{TASK} must be a string, got {value}.\")\n",
    "        value = value.lower()\n",
    "        payload[TASK] = value\n",
    "        if value not in SUPPORTED_TASKS:\n",
    "            raise ValueError(\n",
    "                f\"Input payload contains an invalid task {value}. Valid tasks are {SUPPORTED_TASKS}.\"\n",
    "            )\n",
    "        if LANGUAGE not in payload:\n",
    "            raise ValueError(\"Input payload should contain both language and task\")\n",
    "\n",
    "    for param_name in [MAX_LENGTH, NUM_RETURN_SEQUENCES, NUM_BEAMS]:\n",
    "        if param_name in payload:\n",
    "            if type(payload[param_name]) != int:\n",
    "                raise ValueError(f\"{param_name} must be an integer, got {payload[param_name]}.\")\n",
    "\n",
    "    if MAX_LENGTH in payload:\n",
    "        if payload[MAX_LENGTH] < LENGTH_MIN:\n",
    "            raise ValueError(f\"{MAX_LENGTH} must be at least {LENGTH_MIN}, got {payload[MAX_LENGTH]}.\")\n",
    "\n",
    "    if MIN_LENGTH in payload:\n",
    "        if payload[MIN_LENGTH] < LENGTH_MIN:\n",
    "            raise ValueError(f\"{MIN_LENGTH} must be at least {LENGTH_MIN}, got {payload[MIN_LENGTH]}.\")\n",
    "\n",
    "    if MAX_NEW_TOKENS in payload:\n",
    "        if payload[MAX_NEW_TOKENS] < NEW_TOKENS_MIN:\n",
    "            raise ValueError(f\"{MAX_NEW_TOKENS} must be at least {NEW_TOKENS_MIN}, got {payload[MAX_NEW_TOKENS]}.\")\n",
    "\n",
    "    if MIN_NEW_TOKENS in payload:\n",
    "        if payload[MIN_NEW_TOKENS] < NEW_TOKENS_MIN:\n",
    "            raise ValueError(f\"{MIN_NEW_TOKENS} must be at least {NEW_TOKENS_MIN}, got {payload[MIN_NEW_TOKENS]}.\")\n",
    "\n",
    "    if NUM_RETURN_SEQUENCES in payload:\n",
    "        if payload[NUM_RETURN_SEQUENCES] < NUM_RETURN_SEQUENCE_MIN:\n",
    "            raise ValueError(\n",
    "                f\"{NUM_RETURN_SEQUENCES} must be at least {NUM_RETURN_SEQUENCE_MIN}, \"\n",
    "                f\"got {payload[NUM_RETURN_SEQUENCES]}.\"\n",
    "            )\n",
    "\n",
    "    if NUM_BEAMS in payload:\n",
    "        if payload[NUM_BEAMS] < NUM_BEAMS_MIN:\n",
    "            raise ValueError(f\"{NUM_BEAMS} must be at least {NUM_BEAMS_MIN}, got {payload[NUM_BEAMS]}.\")\n",
    "\n",
    "    if NUM_RETURN_SEQUENCES in payload and NUM_BEAMS in payload:\n",
    "        if payload[NUM_RETURN_SEQUENCES] > payload[NUM_BEAMS]:\n",
    "            raise ValueError(\n",
    "                f\"{NUM_BEAMS} must be at least {NUM_RETURN_SEQUENCES}. Instead got \"\n",
    "                f\"{NUM_BEAMS}={payload[NUM_BEAMS]} and {NUM_RETURN_SEQUENCES}=\"\n",
    "                f\"{payload[NUM_RETURN_SEQUENCES]}.\"\n",
    "            )\n",
    "\n",
    "    if TOP_P in payload:\n",
    "        if payload[TOP_P] < TOP_P_MIN or payload[TOP_P] > TOP_P_MAX:\n",
    "            raise ValueError(f\"{TOP_K} must be in range [{TOP_P_MIN},{TOP_P_MAX}], got \" f\"{payload[TOP_P]}\")\n",
    "\n",
    "    if TEMPERATURE in payload:\n",
    "        if payload[TEMPERATURE] < TEMPERATURE_MIN:\n",
    "            raise ValueError(\n",
    "                f\"{TEMPERATURE} must be a float with value at least {TEMPERATURE_MIN}, got \" f\"{payload[TEMPERATURE]}.\"\n",
    "            )\n",
    "\n",
    "    if DO_SAMPLE in payload:\n",
    "        if type(payload[DO_SAMPLE]) != bool:\n",
    "            raise ValueError(f\"{DO_SAMPLE} must be a boolean, got {payload[DO_SAMPLE]}.\")\n",
    "\n",
    "    return payload\n",
    "\n",
    "\n",
    "def _update_num_beams(payload: Dict[str, Union[str, float, int]]) -> Dict[str, Union[str, float, int]]:\n",
    "    \"\"\"Add num_beans to the payload if missing and num_return_sequences is present.\n",
    "\n",
    "    Args:\n",
    "        payload (Dict): dictionary of input text and parameters\n",
    "    Returns:\n",
    "        payload (Dict): payload with number of beams updated\n",
    "    \"\"\"\n",
    "\n",
    "    if NUM_RETURN_SEQUENCES in payload and NUM_BEAMS not in payload:\n",
    "        payload[NUM_BEAMS] = payload[NUM_RETURN_SEQUENCES]\n",
    "    return payload\n",
    "\n",
    "\n",
    "\n",
    "class ModelAndProcessor:\n",
    "    \"\"\"An ASR model with explicit model and tokenizer objects.\"\"\"\n",
    "\n",
    "    def __init__(self, model_dir: str) -> None:\n",
    "        \"\"\"Initialize model with provided model kwargs and processor objects.\"\"\"\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_dir)\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.config.forced_decoder_ids = None\n",
    "        self.model.eval()\n",
    "        logging.info(\"Loaded model\")\n",
    "\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_dir)\n",
    "        logging.info(\"Loaded processor\")\n",
    "\n",
    "    def __call__(self, audio_input: Dict, **kwargs: Any) -> List:\n",
    "        \"\"\"Perform inference via calls to processor and model's generate method.\n",
    "\n",
    "        If the model is loaded on the GPU, input_ids are placed on the GPU device context.\n",
    "        \"\"\"\n",
    "        input_features = self.processor(\n",
    "            audio_input[\"raw\"], sampling_rate=audio_input[\"sampling_rate\"], return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "\n",
    "        if next(self.model.parameters()).is_cuda:\n",
    "            input_ids_device = input_features.cuda()\n",
    "        else:\n",
    "            input_ids_device = input_features\n",
    "\n",
    "        if kwargs:\n",
    "            if LANGUAGE in kwargs:\n",
    "                language = kwargs.pop(LANGUAGE)\n",
    "                task = kwargs.pop(TASK)\n",
    "                kwargs[FORCED_DECODER_IDS] = self.processor.get_decoder_prompt_ids(\n",
    "                    language=language, task=task\n",
    "                )\n",
    "\n",
    "        predicted_ids = self.model.generate(input_ids_device, **kwargs)\n",
    "\n",
    "        outputs = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "        return {TEXT: outputs}\n",
    "\n",
    "\n",
    "def model_fn(model_dir: str) -> Tuple[WhisperForConditionalGeneration, WhisperProcessor]:\n",
    "    \"\"\"Create our inference task as a delegate to the model.\n",
    "\n",
    "    This runs only once per one worker.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): directory where the model files are stored.\n",
    "    Returns:\n",
    "        WhisperForConditionalGeneration: a huggingface model for Automatic Speech Recognition.\n",
    "        WhisperProcessor: a huggingface processor for pre-process the audio inputs and post-process the model outputs.\n",
    "\n",
    "    Raises:\n",
    "        ValueError if the model file cannot be found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ModelAndProcessor(model_dir)\n",
    "    except Exception:\n",
    "        logging.exception(f\"Failed to load model from: {model_dir}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def transform_fn(\n",
    "    audio_generator_processor: ModelAndProcessor,\n",
    "    input_data: bytes,\n",
    "    content_type: str,\n",
    "    accept: str,\n",
    ") -> bytes:\n",
    "    \"\"\"Make predictions against the model and return a serialized response.\n",
    "\n",
    "    The function signature conforms to the SM contract.\n",
    "\n",
    "    Args:\n",
    "        audio_generator_processor: a huggingface pipeline\n",
    "        input_data (obj): the request data.\n",
    "        content_type (str): the request content type.\n",
    "        accept (str): accept header expected by the client.\n",
    "    Returns:\n",
    "        obj: a byte string of the prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    if content_type == AUDIO_WAV:\n",
    "        try:\n",
    "            data = ffmpeg_read(input_data, SAMPLE_RATE)\n",
    "            audio_input = {\"sampling_rate\": SAMPLE_RATE, \"raw\": data}\n",
    "        except Exception:\n",
    "            logging.exception(\n",
    "                f\"Failed to parse input payload. For content_type= {AUDIO_WAV}, input \"\n",
    "                f\"payload must be a bytearray\"\n",
    "            )\n",
    "            raise\n",
    "        try:\n",
    "            output = audio_generator_processor(deepcopy(audio_input))\n",
    "        except Exception:\n",
    "            logging.exception(\"Failed to do inference\")\n",
    "            raise\n",
    "\n",
    "    elif content_type == APPLICATION_JSON:\n",
    "        try:\n",
    "            payload = json.loads(input_data)\n",
    "        except Exception:\n",
    "            logging.exception(\n",
    "                f\"Failed to parse input payload. For content_type={APPLICATION_JSON}, input \"\n",
    "                f\"payload must be a json encoded dictionary with keys {ALL_PARAM_NAMES}.\"\n",
    "            )\n",
    "            raise\n",
    "        payload = _validate_payload(payload)\n",
    "        payload = _update_num_beams(payload)\n",
    "        audio_input = payload.pop(AUDIO_INPUT)\n",
    "        audio_input = ffmpeg_read(bytes.fromhex(audio_input), SAMPLE_RATE)\n",
    "\n",
    "        audio_input = {\"sampling_rate\": SAMPLE_RATE, \"raw\": audio_input}\n",
    "\n",
    "        try:\n",
    "            output = audio_generator_processor(deepcopy(audio_input), **payload)\n",
    "        except Exception:\n",
    "            logging.exception(\"Failed to do inference\")\n",
    "            raise\n",
    "    else:\n",
    "        raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(content_type or \"unknown\"))\n",
    "    if accept.endswith(VERBOSE_EXTENSION):\n",
    "        accept = accept.rstrip(VERBOSE_EXTENSION)  # Verbose and non-verbose response are identical\n",
    "    return encoder.encode(output, accept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb4f3e-fcde-45eb-a958-6346a930418b",
   "metadata": {},
   "source": [
    "### Upload model files to S3 in uncompress format for SageMaker AI\n",
    "SageMaker AI allows us to provide [uncompressed](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html) files. Thus, we directly upload the folder that contains model files to s3\n",
    "> **Note**: The default SageMaker bucket follows the naming pattern: `sagemaker-{region}-{account-id}`\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Uploading to s3 can take approximately 5 minutes. Please ensure this step completes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e268d418-cbca-4f9c-9fef-71debfe6be39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:23:37.896491Z",
     "iopub.status.busy": "2025-08-30T02:23:37.896222Z",
     "iopub.status.idle": "2025-08-30T02:25:52.058444Z",
     "shell.execute_reply": "2025-08-30T02:25:52.057915Z",
     "shell.execute_reply.started": "2025-08-30T02:23:37.896470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files are uploaded to --- >: s3://sagemaker-us-west-2-992382553328/lmi/whisper-large-v2\n"
     ]
    }
   ],
   "source": [
    "# upload uncompress model files to s3\n",
    "model_artifact_uri = S3Uploader.upload(\n",
    "    local_path=\"./model-files\",\n",
    "    desired_s3_uri=f\"s3://{bucket}/lmi/{base_name}\"\n",
    ")\n",
    "print(f\"Model files are uploaded to --- >: {model_artifact_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb950b49-deeb-4138-a909-5e216f05fe72",
   "metadata": {},
   "source": [
    "> **Note**: Here S3 URI points to the configuration files S3 location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398ac00-4ae7-4b69-92f0-90584613d26f",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying whisper-large-v2, we'll use:\n",
    "- **DLC Hugging Face Pytorch  Inference Container**: A container optimized for large language model inference\n",
    "- **[G5 Instance](https://aws.amazon.com/ec2/instance-types/g5/)**: High performance GPU-based instances for graphics-intensive applications and machine learning inference\n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.g5.4xlarge` instance\n",
    "> **Note**: The region in the container URI should match your AWS region.\n",
    ">\n",
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- Container image (LMI)\n",
    "- code artifacts (configuration files)\n",
    "- IAM role (for permissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1df4d0d2-95b3-4916-9985-810fc5624dfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:26:35.772589Z",
     "iopub.status.busy": "2025-08-30T02:26:35.772325Z",
     "iopub.status.idle": "2025-08-30T02:32:08.870587Z",
     "shell.execute_reply": "2025-08-30T02:32:08.870023Z",
     "shell.execute_reply.started": "2025-08-30T02:26:35.772568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "# OVERRIDE:\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:2.6.0-transformers4.49.0-gpu-py312-cu124-ubuntu22.04\"\n",
    "model_name = name_from_base(base_name, short=True)\n",
    "endpoint_name = model_name\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "health_check_timeout = 900\n",
    "\n",
    "model = sagemaker.Model(\n",
    "\trole=role, \n",
    "    name=model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data={\n",
    "        'S3DataSource': {\n",
    "            'S3Uri': f\"{model_artifact_uri}/\",\n",
    "            'S3DataType': 'S3Prefix',\n",
    "            'CompressionType': 'None'\n",
    "        }\n",
    "    },\n",
    "    env={\n",
    "        \"MMS_MAX_REQUEST_SIZE\": '2000000000',\n",
    "        \"MMS_MAX_RESPONSE_SIZE\": '2000000000',\n",
    "        \"MMS_DEFAULT_RESPONSE_TIMEOUT\": '900',\n",
    "        \"HF_TASK\": \"automatic-speech-recognition\",\n",
    "        \"SERVING_FAIL_FAST\": \"true\",\n",
    "        \"OPTION_MODEL_ID\": \"/opt/ml/model\",\n",
    "        \"OPTION_ASYNC_MODE\": \"false\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "        \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "        \"OPTION_ENTRYPOINT\": \"inference.py\"\n",
    "    },\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Deploy model to an endpoint\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "72ef2182-dc1a-4689-936b-9c277a7659aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:32:18.624098Z",
     "iopub.status.busy": "2025-08-30T02:32:18.623808Z",
     "iopub.status.idle": "2025-08-30T02:32:18.627257Z",
     "shell.execute_reply": "2025-08-30T02:32:18.626734Z",
     "shell.execute_reply.started": "2025-08-30T02:32:18.624077Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import DataSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "# Define serializers and deserializer\n",
    "audio_serializer = DataSerializer(content_type=\"audio/x-audio\")\n",
    "deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e3d7a5-9fb9-4a32-9b06-33a5f84f2778",
   "metadata": {},
   "source": [
    "### Create a predictor from our existing endpoint and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "56b32be8-6561-4aad-8b59-4749b3fea67a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:32:24.203400Z",
     "iopub.status.busy": "2025-08-30T02:32:24.203138Z",
     "iopub.status.idle": "2025-08-30T02:32:24.206115Z",
     "shell.execute_reply": "2025-08-30T02:32:24.205618Z",
     "shell.execute_reply.started": "2025-08-30T02:32:24.203381Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=audio_serializer,\n",
    "    deserializer=deserializer,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d0c37f0-6a2e-4a0c-8630-70429dacaabf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:32:27.007482Z",
     "iopub.status.busy": "2025-08-30T02:32:27.007226Z",
     "iopub.status.idle": "2025-08-30T02:32:27.010618Z",
     "shell.execute_reply": "2025-08-30T02:32:27.010114Z",
     "shell.execute_reply.started": "2025-08-30T02:32:27.007461Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_endpoint(body, content_type):\n",
    "    response = smr_client.invoke_endpoint(EndpointName=endpoint_name, ContentType=content_type, Body=body)\n",
    "    model_predictions = json.loads(response['Body'].read())\n",
    "    print(json.dumps(model_predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a0d306-d4b1-40a8-bc7c-cf67215537c0",
   "metadata": {},
   "source": [
    "### Speech to transcribed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "72c6a984-289d-44f2-9a42-5ecbf35744ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:32:30.291480Z",
     "iopub.status.busy": "2025-08-30T02:32:30.291219Z",
     "iopub.status.idle": "2025-08-30T02:32:32.347011Z",
     "shell.execute_reply": "2025-08-30T02:32:32.346520Z",
     "shell.execute_reply.started": "2025-08-30T02:32:30.291460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": [\n",
      "    \" We are living in very exciting times with machine learning. The speed of ML model development will really actually increase. But you won't get to that end state that we want in the next coming years unless we actually make these models more accessible to everybody.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input_audio_file_name = \"sample1.wav\"\n",
    "\n",
    "with open(input_audio_file_name, \"rb\") as file:\n",
    "    wav_file_read = file.read()\n",
    "\n",
    "query_endpoint(wav_file_read, \"audio/wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0638ce-ea6a-4d87-b4b8-3a44348bf085",
   "metadata": {},
   "source": [
    "### Speech to transcribed text in original language(French) and translated to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "95772fc1-b49c-467e-86be-e4bf5df18dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:32:36.911566Z",
     "iopub.status.busy": "2025-08-30T02:32:36.911292Z",
     "iopub.status.idle": "2025-08-30T02:32:39.937537Z",
     "shell.execute_reply": "2025-08-30T02:32:39.937042Z",
     "shell.execute_reply.started": "2025-08-30T02:32:36.911547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": [\n",
      "    \" Bienvenue chez JPB Syst\\u00e8mes, ici. C'est plus de 150 collaborateurs, c'est plus de 90% de chiffre d'affaires \\u00e0 l'export et d'un produit, c'est une quinzaine de preuves que nous avons d\\u00e9velopp\\u00e9es.\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"text\": [\n",
      "    \" Welcome to JPBSystem. We have more than 150 employees and 90% of sales. We have developed about 15 patents.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input_audio_file_name = \"sample_french1.wav\"\n",
    "\n",
    "with open(input_audio_file_name, \"rb\") as file:\n",
    "    wav_file_read = file.read()\n",
    "\n",
    "payload = {\"audio_input\": wav_file_read.hex()}\n",
    "query_endpoint(json.dumps(payload).encode('utf-8'), \"application/json\")\n",
    "\n",
    "payload = {\"audio_input\": wav_file_read.hex(),\n",
    "           \"language\": \"french\",\n",
    "           \"task\": \"translate\"}\n",
    "\n",
    "query_endpoint(json.dumps(payload).encode('utf-8'), \"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d016907e-768c-4485-b49b-ad50a20a4e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T02:32:42.759137Z",
     "iopub.status.busy": "2025-08-30T02:32:42.758881Z",
     "iopub.status.idle": "2025-08-30T02:32:43.302450Z",
     "shell.execute_reply": "2025-08-30T02:32:43.301949Z",
     "shell.execute_reply.started": "2025-08-30T02:32:42.759118Z"
    }
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88643096-3c39-467e-8229-5147bad751b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
