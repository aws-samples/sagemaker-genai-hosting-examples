{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b7733e-f3ef-4fde-87d2-c4e9b14e6d54",
   "metadata": {},
   "source": [
    "# Deploy and benchmark reranker models on Amazon SageMaker\n",
    "\n",
    "In information retrieval and natural language processing applications, rerankers have emerged as powerful tools to enhance the accuracy and relevance of search results. Rerankers are specialized techniques or machine learning models designed to optimize the ordering of a set of retrieved items to improve the overall quality of information retrieval systems.\n",
    "\n",
    "The objective of this notebook is to demonstrate how you can deploy and scale reranker models using Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b234a66-a1e9-40ff-ab05-8e8da8a82ea1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc439ca3-1bec-433d-ba81-4418fcd2c0e7",
   "metadata": {},
   "source": [
    "Upgrade the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117db275-70b7-46b6-816f-15fac67190f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -U transformers hf_transfer sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a7bbb-7dd2-4a0c-9e49-c8d7bc2be342",
   "metadata": {},
   "source": [
    "Instantiate the necessary session paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db36da9-413f-415d-a3bd-6ff4337d022d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.djl_inference.model import DJLModel\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    sagemaker_runtime_client=boto3.client(\n",
    "        \"sagemaker-runtime\",\n",
    "        config=Config(connect_timeout=10, retries={\"mode\": \"standard\", \"total_max_attempts\": 20}),\n",
    "    )\n",
    ")\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d11b9-f78c-4193-8a8f-2d1fdf0e0822",
   "metadata": {},
   "source": [
    "## Create Model Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f3b65-296c-4dcb-a24b-fd3d057177db",
   "metadata": {},
   "source": [
    "In this section, we create the SageMaker Model object. You can test the 3 options below:\n",
    "1. Create a SageMaker Jumpstart Model Object\n",
    "2. Create a HuggingFace SageMaker Model Object from a model downloaded from HuggingFace Hub\n",
    "3. Create a DJL Model Object that uses SageMaker Large Model Inference (LMI) container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9518168-b5cc-43c1-9691-9f38f6c7ee12",
   "metadata": {},
   "source": [
    "### Option 1: Create a SageMaker Jumpstart Model Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc1a0c-4d09-4d2e-b6c8-b0ff4587d8b9",
   "metadata": {},
   "source": [
    "Amazon SageMaker JumpStart is a machine learning (ML) hub that can help you accelerate your ML journey where you can can compare, and select foundation models (FMs) based on your use case like article summarization and image generation. You can fine-tune or deploy FMs in SageMaker Jumpstart via SageMaker Studio or SDK. You can find the full list of foundation models [here](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68117b3-70da-4c4d-a9bd-7b986f213986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Jumpstart Model object\n",
    "model_reranker_js = JumpStartModel(model_id=\"cohere-rerank-multilingual-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613ab31-09bd-47b3-8574-5d643896e013",
   "metadata": {},
   "source": [
    "### Option 2: Create a HuggingFace SageMaker Model Object from a model downloaded from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8cf13-51e5-4254-8038-3ba41d5a2f08",
   "metadata": {},
   "source": [
    "To deploy a model directly from the ü§ó Hub to SageMaker, define an environment variable when you create a HuggingFaceModel:\n",
    "\n",
    "* HF_MODEL_ID defines the model ID which is automatically loaded from [huggingface.co/models](huggingface.co/models) when you create a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d5671-b493-41e7-a7fe-0eb33188e399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_hf = {\"HF_MODEL_ID\": \"BAAI/bge-reranker-v2-m3\", \"DTYPE\": \"float16\"}\n",
    "\n",
    "model_name = sagemaker.utils.name_from_base(config_hf[\"HF_MODEL_ID\"].replace(\"/\", \"-\"))\n",
    "\n",
    "model_reranker_tei = HuggingFaceModel(\n",
    "    image_uri=get_huggingface_llm_image_uri(\"huggingface-tei\"),\n",
    "    env=config_hf,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b76a00-2293-4756-af2f-d5e923057cff",
   "metadata": {},
   "source": [
    "### Option 3: Create a DJL Model Object that uses SageMaker Large Model Inference (LMI) container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007e748-b75e-461f-9166-09642dbb08c4",
   "metadata": {},
   "source": [
    "DJL Serving is a high performance universal stand-alone model serving solution. It takes a deep learning model, several models, or workflows and makes them available through an HTTP endpoint.\n",
    "\n",
    "You can use one of the DJL Serving [Deep Learning Containers (DLCs)](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.html) to serve your models on AWS. To learn about the supported model types and frameworks, see the [DJL Serving documentation](https://docs.djl.ai/master/index.html).\n",
    "\n",
    "In this notebook, we will use Large Model Inference (LMI) containers which are a set of high-performance Docker Containers purpose built for large language model (LLM) inference. With these containers, you can leverage high performance open-source inference libraries like vLLM, TensorRT-LLM, Transformers NeuronX to deploy LLMs on AWS SageMaker Endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b2082-e062-4ece-8102-7ebe0e6c44a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"BAAI/bge-reranker-v2-m3\"  # model will be download form Huggingface hub\n",
    "image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124\"\n",
    "\n",
    "env = {\n",
    "    \"SERVING_MIN_WORKERS\": \"1\",  # make sure min and max Workers are equals when deploy model on GPU\n",
    "    \"SERVING_MAX_WORKERS\": \"1\",\n",
    "    \"OPTION_ENGINE\": \"OnnxRuntime\",\n",
    "    \"SERVING_BATCH_SIZE\": \"32\",\n",
    "}\n",
    "\n",
    "# create DJL Model object\n",
    "model_reranker_djl = DJLModel(\n",
    "    model_id=model_id, task=\"text-embedding\", image_uri=image_uri, env=env, role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00865c9-711c-4fdf-a34c-1989e9aef7a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy the Model to an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008126b4-8d17-40de-9f2b-b3ea5c508c0c",
   "metadata": {},
   "source": [
    "Choose the model that you want to deploy by copying and running one of the code snippents below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950e40c-ad7a-48ee-97f1-c0a4eee44fa1",
   "metadata": {},
   "source": [
    "To deploy a SageMaker Jumpstart Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105779a-0200-481b-b01d-c1c7cda6880c",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "deployment_type = \"js\"\n",
    "model_reranker_predictor = model_reranker_js.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    container_startup_health_check_timeout=300,\n",
    "    wait=False,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c27c9-08e2-4b07-a801-a744a7396363",
   "metadata": {},
   "source": [
    "To deploy the model using Hugging Face Text Embedding Inference (TEI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60863b-f6cf-472c-b0ee-3a48280ff934",
   "metadata": {},
   "source": [
    "```python\n",
    "deployment_type = \"tei\"\n",
    "model_reranker_predictor = model_reranker_tei.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    container_startup_health_check_timeout=300,\n",
    "    wait=False,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c88ac-fca7-4126-8725-112c75d5ef4b",
   "metadata": {},
   "source": [
    "To deploy the model using Deep Java Library (DJL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a9b51-57ca-43f8-882b-82256ad1f7fc",
   "metadata": {},
   "source": [
    "```python\n",
    "deployment_type = \"djl\"\n",
    "model_reranker_predictor = model_reranker_djl.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    container_startup_health_check_timeout=300,\n",
    "    wait=False,\n",
    ")\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1aa68d-425b-44dd-b462-4deba1e683c1",
   "metadata": {},
   "source": [
    "Once the model is deployed, test the model invocation.\n",
    "\n",
    "In the case of DJL, we use the `XLMRobertaTokenizer` seperator token `</s></s>` to let the model compute the similarity between the seperated sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacfe00-6080-410f-a5c4-bf31f88692f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if deployment_type == \"djl\":\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            \"what is panda?</s></s>A panda is a type of bear that is known for its distinctive black and white coloring.\",\n",
    "            \"what is panda?</s></s>Pandas are native to China and are known for their diet, which consists mostly of bamboo.\",\n",
    "            \"what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\n",
    "            \"what is panda?</s></s>Pandas have become a symbol of conservation due to their status as an endangered species.\",\n",
    "            \"what is panda?</s></s>The panda's distinctive black and white coat serves as camouflage in its natural habitat.\",\n",
    "            \"what is panda?</s></s>Pandas are known for their playful behavior and are a favorite among zoo visitors.\",\n",
    "            \"what is panda?</s></s>There are two main species of panda: the giant panda and the red panda, which are not closely related.\",\n",
    "            \"what is panda?</s></s>Pandas primarily live in temperate forests high in the mountains of southwest China.\",\n",
    "            \"what is panda?</s></s>The giant panda has a large head, heavy body, rounded ears, and a short tail.\",\n",
    "            \"what is panda?</s></s>Efforts to protect panda habitats have led to the establishment of several panda reserves in China.\"\n",
    "        ]\n",
    "    }\n",
    "elif deployment_type == \"js\":\n",
    "    payload = {\n",
    "        \"query\": \"What is the capital of the United States?\", \n",
    "        \"rank_fields\": [\"Title\", \"Content\"],\n",
    "        \"documents\": [\n",
    "            {\"Title\": \"Facts about Carson City\",\"Content\": \"Carson City is the capital city of the American state of Nevada. \"}, \n",
    "            {\"Title\": \"‡§â‡§§‡•ç‡§§‡§∞‡•Ä ‡§Æ‡§æ‡§∞‡§ø‡§Ø‡§æ‡§®‡§æ ‡§¶‡•ç‡§µ‡•Ä‡§™ ‡§∏‡§Æ‡•Ç‡§π ‡§ï‡•á ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§Æ‡§Ç‡§°‡§≤ ‡§ï‡§æ ‡§á‡§§‡§ø‡§π‡§æ‡§∏\",\"Content\" : \"‡§â‡§§‡•ç‡§§‡§∞‡•Ä ‡§Æ‡§æ‡§∞‡§ø‡§Ø‡§æ‡§®‡§æ ‡§¶‡•ç‡§µ‡•Ä‡§™ ‡§∏‡§Æ‡•Ç‡§π ‡§ï‡§æ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§Æ‡§Ç‡§°‡§≤ ‡§™‡•ç‡§∞‡§∂‡§æ‡§Ç‡§§ ‡§Æ‡§π‡§æ‡§∏‡§æ‡§ó‡§∞ ‡§Æ‡•á‡§Ç ‡§¶‡•ç‡§µ‡•Ä‡§™‡•ã‡§Ç ‡§ï‡§æ ‡§è‡§ï ‡§∏‡§Æ‡•Ç‡§π ‡§π‡•à‡•§\"}, \n",
    "            {\"Title\": \"Los Estados Unidos\",\"Content\" : \"Washington, DC es la capital de los Estados Unidos.\"}\n",
    "        ]\n",
    "    }\n",
    "elif deployment_type == \"tei\":\n",
    "    payload = {\n",
    "    \"query\": \"what is panda?\",\n",
    "    \"texts\": [\n",
    "            \"A panda is a type of bear that is known for its distinctive black and white coloring.\",\n",
    "            \"Pandas are native to China and are known for their diet, which consists mostly of bamboo.\",\n",
    "            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\n",
    "            \"Pandas have become a symbol of conservation due to their status as an endangered species.\",\n",
    "            \"The panda's distinctive black and white coat serves as camouflage in its natural habitat.\",\n",
    "            \"Pandas are known for their playful behavior and are a favorite among zoo visitors.\",\n",
    "            \"There are two main species of panda: the giant panda and the red panda, which are not closely related.\",\n",
    "            \"Pandas primarily live in temperate forests high in the mountains of southwest China.\",\n",
    "            \"The giant panda has a large head, heavy body, rounded ears, and a short tail.\",\n",
    "            \"Efforts to protect panda habitats have led to the establishment of several panda reserves in China.\"\n",
    "        ]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa500ed-e3c8-4d57-ab4a-b92ce788b678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_reranker_predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db3f0d-4b86-49f8-aeea-5f270d15df4b",
   "metadata": {},
   "source": [
    "## Benchmark the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619a4df-27af-4f94-93df-39abfd48db2f",
   "metadata": {},
   "source": [
    "Create a benchmark scrip that sends concurrent requests, stores and plots the latencies and throughputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8365d-5cc5-4f9e-873a-bd435cf76dd6",
   "metadata": {},
   "source": [
    "Benchmark the endpoint and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61353888-25e1-4549-be45-55b9500ac062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming model_reranker_predictor is already defined and initialized\n",
    "# and model_reranker_predictor.predict(data=payload) is the method to be benchmarked\n",
    "\n",
    "\n",
    "def benchmark_predictor(predictor, payload, steps, iterations=5):\n",
    "    \"\"\"\n",
    "    Benchmarks a predictor's performance by measuring latency and throughput\n",
    "    under varying levels of concurrent requests.\n",
    "\n",
    "    Args:\n",
    "        predictor (object): The predictor object with a `predict` method.\n",
    "        payload (any): The input data to be sent to the predictor.\n",
    "        steps (list): A list of different numbers of concurrent requests to test.\n",
    "        iterations (int, optional): The number of iterations for each concurrency level. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Three lists containing the request counts, latencies, and throughputs.\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    throughputs = []\n",
    "    request_counts = []\n",
    "\n",
    "    def send_request():\n",
    "        \"\"\"Sends a single request to the predictor and measures its latency.\"\"\"\n",
    "        start_time = time.time()\n",
    "        resp = predictor.predict(data=payload)\n",
    "        latency = time.time() - start_time\n",
    "        return latency\n",
    "\n",
    "    for num_requests in steps:\n",
    "        iter_latencies = []\n",
    "        iter_throughputs = []\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Use ThreadPoolExecutor to send concurrent requests\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=num_requests) as executor:\n",
    "                futures = [executor.submit(send_request) for _ in range(num_requests)]\n",
    "                latencies_batch = [\n",
    "                    future.result() for future in concurrent.futures.as_completed(futures)\n",
    "                ]\n",
    "\n",
    "            total_time = time.time() - start_time\n",
    "\n",
    "            # Calculate average latency for this iteration\n",
    "            latency = np.mean(latencies_batch)\n",
    "            # Calculate throughput for this iteration\n",
    "            throughput = num_requests / total_time\n",
    "\n",
    "            iter_latencies.append(latency)\n",
    "            iter_throughputs.append(throughput)\n",
    "\n",
    "        # Calculate average latency and throughput over all iterations\n",
    "        avg_latency = np.mean(iter_latencies)\n",
    "        avg_throughput = np.mean(iter_throughputs)\n",
    "\n",
    "        latencies.append(avg_latency)\n",
    "        throughputs.append(avg_throughput)\n",
    "        request_counts.append(num_requests)\n",
    "\n",
    "        # Print results for the current number of requests\n",
    "        print(\n",
    "            f\"Requests: {num_requests}, Average Latency: {avg_latency:.4f}s, Average Throughput: {avg_throughput:.2f} req/s\"\n",
    "        )\n",
    "\n",
    "    return request_counts, latencies, throughputs\n",
    "\n",
    "\n",
    "def plot_metrics(request_counts, latencies, throughputs):\n",
    "    \"\"\"\n",
    "    Plots the benchmarking results, showing the average latency and throughput\n",
    "    as a function of the number of concurrent requests.\n",
    "\n",
    "    Args:\n",
    "        request_counts (list): The list of different numbers of concurrent requests tested.\n",
    "        latencies (list): The list of average latencies corresponding to the request counts.\n",
    "        throughputs (list): The list of average throughputs corresponding to the request counts.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = \"tab:blue\"\n",
    "    ax1.set_xlabel(\"Number of Concurrent Requests\")\n",
    "    ax1.set_ylabel(\"Average Latency (s)\", color=color)\n",
    "    ax1.plot(request_counts, latencies, color=color)\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = \"tab:green\"\n",
    "    ax2.set_ylabel(\n",
    "        \"Throughput (requests/s)\", color=color\n",
    "    )  # we already handled the x-label with ax1\n",
    "    ax2.plot(request_counts, throughputs, color=color)\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(\"Latency and Throughput Benchmarking\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_latency_vs_throughput(latencies, throughputs, request_counts):\n",
    "    \"\"\"\n",
    "    Plots latency against throughput, with annotations for the number of concurrent requests.\n",
    "\n",
    "    Args:\n",
    "        latencies (list): The list of average latencies.\n",
    "        throughputs (list): The list of average throughputs.\n",
    "        request_counts (list): The list of different numbers of concurrent requests tested.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(throughputs, latencies, \"o-\")\n",
    "    plt.xlabel(\"Throughput (requests/s)\")\n",
    "    plt.ylabel(\"Average Latency (s)\")\n",
    "    plt.title(\"Latency vs Throughput\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Label each point with the request count\n",
    "    for i, request_count in enumerate(request_counts):\n",
    "        plt.annotate(\n",
    "            request_count,\n",
    "            (throughputs[i], latencies[i]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 10),\n",
    "            ha=\"center\",\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3d5f2-698c-4c25-b8d0-df67add71e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_benchmark = pd.DataFrame(columns=[\"client_batch_size\", \"concurrent_request_counts\", \"latencies\", \"throughputs\"])\n",
    "\n",
    "min_requests=0 # 2^0=1\n",
    "max_requests = 9 # 2^8=512\n",
    "step_size = 1\n",
    "iterations = 20\n",
    "client_batch_size = 32\n",
    "\n",
    "steps = list(map(lambda x:2**x,range(min_requests, max_requests, step_size)))\n",
    "\n",
    "if deployment_type == \"djl\":\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            \"what is panda?</s></s>A panda is a type of bear that is known for its distinctive black and white coloring.\"\n",
    "        ] * client_batch_size\n",
    "    }\n",
    "elif deployment_type == \"js\":\n",
    "    payload = {\n",
    "        \"query\": \"What is the capital of the United States?\", \n",
    "        \"rank_fields\": [\"Title\", \"Content\"],\n",
    "        \"documents\": [\n",
    "        ] * client_batch_size\n",
    "    }\n",
    "elif deployment_type == \"tei\":\n",
    "    payload = {\n",
    "        \"query\": \"what is panda?\",\n",
    "        \"texts\": [\n",
    "            \"A panda is a type of bear that is known for its distinctive black and white coloring.\"\n",
    "        ] * client_batch_size\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "request_counts, latencies, throughputs = benchmark_predictor(model_reranker_predictor, payload, steps, iterations)\n",
    "plot_metrics(request_counts, latencies, throughputs)\n",
    "plot_latency_vs_throughput(latencies, throughputs, request_counts)\n",
    "\n",
    "new_data = {\n",
    "    'client_batch_size': [client_batch_size] * len(request_counts),\n",
    "    'concurrent_request_counts': request_counts,\n",
    "    'latencies': latencies,\n",
    "    'throughputs' : throughputs\n",
    "}\n",
    "\n",
    "df_benchmark = pd.DataFrame(columns=[\"client_batch_size\", \"concurrent_request_counts\", \"latencies\", \"throughputs\"])\n",
    "df_benchmark = df_benchmark._append(pd.DataFrame(new_data), ignore_index=True)\n",
    "df_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e53c1-2a48-4a33-af64-6d42918e2653",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8e990-bcb2-49f0-935e-0768829a1b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_reranker_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d012f4-dc42-4f40-a0eb-abd3ec712cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
