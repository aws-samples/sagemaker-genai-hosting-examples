# Build Agentic Workflows with Small Language Models and SageMaker AI

This workshop contains examples of how to build Agentic Workflows using reasoning models deployed on Amazon SageMaker AI and deploy the worflow to Bedrock AgentCore runtime.

In [Lab 1](./lab1/) we will deploy [Qwen3-4B](https://huggingface.co/Qwen/Qwen3-4B) model on real-time Amazon SageMaker AI endpoint using Large Model Inference (LMI) container.

In [Lab 2](./lab2/) we will create a simple MCP server, integrate it with MCP gateway, create research agent and finally deploy the agent to Amazon Bedrock AgentCore

## Prerequisites
- SageMaker Studio environment
- AWS CLI configured with appropriate permissions
- Python 3.8+ environment
- Docker support in SageMaker Studio

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Support
- üìñ **Documentation**: see [Amazon SageMaker AI](https://docs.aws.amazon.com/sagemaker/) and [Amazon Bedrock AgentCore](https://docs.aws.amazon.com/bedrock-agentcore/)
- üêõ **Issues**: Open GitHub issue with detailed description
- üí¨ **Discussions**: Use GitHub Discussions for questions

## Acknowledgments
- Amazon SageMaker team for endpoint infrastructure
- Qwen model developers for the language model
- Amazon Bedrock Agent Core team for the deployment framework


