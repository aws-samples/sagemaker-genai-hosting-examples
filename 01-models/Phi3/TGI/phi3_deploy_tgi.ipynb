{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Phi-3 Model on SageMaker using Text Generation Inference (TGI)\n",
    "\n",
    "This notebook demonstrates how to deploy Microsoft's Phi-3 model on Amazon SageMaker using the Hugging Face Text Generation Inference (TGI) container.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Model**: microsoft/Phi-3-mini-4k-instruct (3.8B parameters)\n",
    "- **Container**: Hugging Face TGI Deep Learning Container\n",
    "- **Instance Type**: ml.g5.2xlarge (1 GPU)\n",
    "- **Features**: Streaming responses, optimized inference, token-level details\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS Account with SageMaker access\n",
    "- Appropriate IAM role with SageMaker permissions\n",
    "- Sufficient service quota for ml.g5.2xlarge instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install and upgrade the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade --quiet\n",
    "!pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize SageMaker Session\n",
    "\n",
    "Set up the SageMaker session and get the execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"AWS region: {region}\")\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get TGI Container Image URI\n",
    "\n",
    "Retrieve the Hugging Face TGI container image URI using the SageMaker helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# Get the TGI image URI\n",
    "image_uri = get_huggingface_llm_image_uri(\n",
    "    backend=\"huggingface\",  # or \"lmi\" for DJL serving\n",
    "    region=region\n",
    ")\n",
    "\n",
    "print(f\"Image URI: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Model Settings\n",
    "\n",
    "Define the model configuration including the Hugging Face model ID and deployment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import time\n",
    "\n",
    "# Model configuration\n",
    "model_name = f\"phi3-mini-4k-tgi-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "endpoint_name = f\"{model_name}-ep\"\n",
    "\n",
    "# Environment variables for TGI\n",
    "hub_config = {\n",
    "    'HF_MODEL_ID': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "    'SM_NUM_GPUS': '1',\n",
    "    'MAX_INPUT_LENGTH': '3072',\n",
    "    'MAX_TOTAL_TOKENS': '4096',\n",
    "    'MAX_BATCH_PREFILL_TOKENS': '4096',\n",
    "    'MAX_BATCH_TOTAL_TOKENS': '8192',\n",
    "    # Optional: Add HF token for gated models\n",
    "    # 'HUGGING_FACE_HUB_TOKEN': '<YOUR_HF_TOKEN>',\n",
    "}\n",
    "\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "print(f\"Model configuration: {json.dumps(hub_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create SageMaker Model\n",
    "\n",
    "Create a HuggingFaceModel object with the TGI configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace Model\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    env=hub_config,\n",
    "    role=role,\n",
    "    image_uri=image_uri\n",
    ")\n",
    "\n",
    "print(f\"HuggingFace Model created: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Deploy the model to a real-time SageMaker endpoint. This step will take approximately 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "predictor = huggingface_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    container_startup_health_check_timeout=600,\n",
    ")\n",
    "\n",
    "print(f\"Endpoint deployed successfully: {endpoint_name}\")\n",
    "print(f\"Endpoint ARN: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Endpoint - Simple Inference\n",
    "\n",
    "Test the deployed model with a simple text generation request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple inference request\n",
    "input_data = {\n",
    "    \"inputs\": \"What is machine learning? Explain in simple terms.\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "        \"return_full_text\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "response = predictor.predict(input_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response[0]['generated_text'])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test with Chat Format\n",
    "\n",
    "Phi-3 models work well with chat-based interactions. Let's test with a formatted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi-3 chat template format\n",
    "def format_phi3_chat(messages):\n",
    "    \"\"\"\n",
    "    Format messages for Phi-3 chat template.\n",
    "    Messages should be a list of dicts with 'role' and 'content'.\n",
    "    \"\"\"\n",
    "    formatted_prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        if role == 'system':\n",
    "            formatted_prompt += f\"<|system|>\\n{content}<|end|>\\n\"\n",
    "        elif role == 'user':\n",
    "            formatted_prompt += f\"<|user|>\\n{content}<|end|>\\n\"\n",
    "        elif role == 'assistant':\n",
    "            formatted_prompt += f\"<|assistant|>\\n{content}<|end|>\\n\"\n",
    "    \n",
    "    # Add assistant prefix for response\n",
    "    formatted_prompt += \"<|assistant|>\\n\"\n",
    "    return formatted_prompt\n",
    "\n",
    "# Example chat conversation\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI assistant that provides clear and concise answers.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What are the key differences between Python and JavaScript?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "formatted_prompt = format_phi3_chat(messages)\n",
    "\n",
    "chat_data = {\n",
    "    \"inputs\": formatted_prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 300,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "        \"return_full_text\": False,\n",
    "        \"stop\": [\"<|end|>\", \"<|endoftext|>\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(chat_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHAT RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response[0]['generated_text'])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Streaming Inference\n",
    "\n",
    "TGI supports streaming responses for real-time token generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Create SageMaker runtime client for streaming\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "\n",
    "def stream_response(endpoint_name, payload):\n",
    "    \"\"\"\n",
    "    Stream responses from the SageMaker endpoint.\n",
    "    \"\"\"\n",
    "    response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "    \n",
    "    event_stream = response['Body']\n",
    "    \n",
    "    print(\"\\nStreaming response:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for event in event_stream:\n",
    "        if 'PayloadPart' in event:\n",
    "            payload_part = event['PayloadPart']['Bytes'].decode('utf-8')\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            try:\n",
    "                data = json.loads(payload_part)\n",
    "                if 'token' in data:\n",
    "                    token_text = data['token']['text']\n",
    "                    print(token_text, end='', flush=True)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# Streaming request\n",
    "stream_payload = {\n",
    "    \"inputs\": \"Write a short poem about artificial intelligence.\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 150,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "        \"return_full_text\": False\n",
    "    },\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "stream_response(endpoint_name, stream_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Batch Processing Example\n",
    "\n",
    "Process multiple prompts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing\n",
    "prompts = [\n",
    "    \"Explain quantum computing in one sentence.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Write a haiku about nature.\",\n",
    "    \"What are the benefits of exercise?\"\n",
    "]\n",
    "\n",
    "print(\"Processing batch of prompts...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 100,\n",
    "            \"temperature\": 0.7,\n",
    "            \"do_sample\": True,\n",
    "            \"return_full_text\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = predictor.predict(payload)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(response[0]['generated_text'])\n",
    "    print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Monitor Endpoint Performance\n",
    "\n",
    "Check endpoint metrics and status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "# Get endpoint details\n",
    "endpoint_description = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "print(\"Endpoint Status:\")\n",
    "print(f\"  Endpoint Name: {endpoint_description['EndpointName']}\")\n",
    "print(f\"  Status: {endpoint_description['EndpointStatus']}\")\n",
    "print(f\"  Creation Time: {endpoint_description['CreationTime']}\")\n",
    "print(f\"  Instance Type: {endpoint_description['ProductionVariants'][0]['InstanceType']}\")\n",
    "print(f\"  Current Instance Count: {endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced Features - Details and Metadata\n",
    "\n",
    "Get detailed information about generated tokens including log probabilities and finish reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request with details\n",
    "detailed_request = {\n",
    "    \"inputs\": \"Explain the concept of neural networks.\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 150,\n",
    "        \"temperature\": 0.7,\n",
    "        \"do_sample\": True,\n",
    "        \"return_full_text\": False,\n",
    "        \"details\": True  # Request detailed information\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(detailed_request)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DETAILED RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Generated Text: {response[0]['generated_text']}\")\n",
    "print(f\"\\nDetails:\")\n",
    "if 'details' in response[0]:\n",
    "    details = response[0]['details']\n",
    "    print(f\"  Finish Reason: {details.get('finish_reason', 'N/A')}\")\n",
    "    print(f\"  Generated Tokens: {details.get('generated_tokens', 'N/A')}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup Resources\n",
    "\n",
    "**Important**: Delete the endpoint when you're done to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "print(f\"Endpoint {endpoint_name} deleted successfully\")\n",
    "\n",
    "# Optionally, delete the model\n",
    "predictor.delete_model()\n",
    "print(f\"Model {model_name} deleted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. ✅ Set up the SageMaker environment\n",
    "2. ✅ Retrieved the Hugging Face TGI container image\n",
    "3. ✅ Configured the Phi-3 model for deployment\n",
    "4. ✅ Deployed the model to a SageMaker endpoint\n",
    "5. ✅ Tested simple text generation\n",
    "6. ✅ Used chat-formatted prompts\n",
    "7. ✅ Implemented streaming inference\n",
    "8. ✅ Performed batch processing\n",
    "9. ✅ Monitored endpoint performance\n",
    "10. ✅ Retrieved detailed generation metadata\n",
    "11. ✅ Cleaned up resources\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Experiment with different Phi-3 variants (mini-128k, medium, small)\n",
    "- Implement auto-scaling policies for production workloads\n",
    "- Integrate with CloudWatch for monitoring and alerting\n",
    "- Set up A/B testing with multiple model versions\n",
    "- Implement custom inference code for specialized use cases\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Hugging Face TGI Documentation](https://huggingface.co/docs/text-generation-inference)\n",
    "- [SageMaker Inference Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/inference.html)\n",
    "- [Phi-3 Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "- [AWS Deep Learning Containers](https://github.com/aws/deep-learning-containers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
