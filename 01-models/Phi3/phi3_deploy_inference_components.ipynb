{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Phi-3 with SageMaker Inference Components\n",
    "\n",
    "This notebook demonstrates deploying Phi-3 using SageMaker Inference Components for optimized resource utilization and cost savings.\n",
    "\n",
    "## What are Inference Components?\n",
    "\n",
    "Inference Components (ICs) allow you to:\n",
    "- **Right-size resources**: Allocate exact compute needed per model\n",
    "- **Model packing**: Run multiple models on shared infrastructure  \n",
    "- **Independent scaling**: Scale each model independently\n",
    "- **Cost optimization**: Pay only for resources you use\n",
    "- **Zero-downtime updates**: Rolling updates without downtime\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- Multi-tenant deployments\n",
    "- A/B testing different model versions\n",
    "- Cost-effective production deployments\n",
    "- Scale-to-zero for intermittent workloads\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS Account with SageMaker access\n",
    "- IAM role with appropriate permissions\n",
    "- GPU instance quota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.256.0\n",
    "!pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# Initialize clients\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get TGI Container Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TGI image URI\n",
    "image_uri = get_huggingface_llm_image_uri(\n",
    "    backend=\"huggingface\",\n",
    "    region=region\n",
    ")\n",
    "\n",
    "print(f\"Image URI: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Endpoint (without model)\n",
    "\n",
    "First, create an endpoint without any models. We'll add models via Inference Components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"phi3-ic-endpoint-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "# Endpoint configuration with Managed Instance Scaling\n",
    "endpoint_config = {\n",
    "    \"EndpointName\": endpoint_name,\n",
    "    \"ProductionVariants\": [\n",
    "        {\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 600,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": 1,\n",
    "                \"MaxInstanceCount\": 4\n",
    "            },\n",
    "            \"RoutingConfig\": {\n",
    "                \"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Creating endpoint: {endpoint_name}\")\n",
    "print(\"This will take 5-10 minutes...\")\n",
    "\n",
    "# Create endpoint\n",
    "sm_client.create_endpoint(**endpoint_config)\n",
    "\n",
    "# Wait for endpoint to be in service\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "print(f\"‚úÖ Endpoint created: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Inference Component with Phi-3\n",
    "\n",
    "Now create an Inference Component to deploy Phi-3 on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_name = f\"phi3-mini-ic-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    'HF_MODEL_ID': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "    'SM_NUM_GPUS': '1',\n",
    "    'MAX_INPUT_LENGTH': '3072',\n",
    "    'MAX_TOTAL_TOKENS': '4096',\n",
    "    'MESSAGES_API_ENABLED': 'true',  # Enable OpenAI-compatible API\n",
    "    # Optional: HF token for gated models\n",
    "    # 'HUGGING_FACE_HUB_TOKEN': '<YOUR_TOKEN>',\n",
    "}\n",
    "\n",
    "# Create inference component\n",
    "ic_config = {\n",
    "    \"InferenceComponentName\": ic_name,\n",
    "    \"EndpointName\": endpoint_name,\n",
    "    \"VariantName\": \"AllTraffic\",\n",
    "    \"Specification\": {\n",
    "        \"ModelName\": ic_name,  # Will create model automatically\n",
    "        \"Container\": {\n",
    "            \"Image\": image_uri,\n",
    "            \"Environment\": model_config\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,  # Use 1 GPU\n",
    "            \"MinMemoryRequiredInMb\": 8192  # 8GB RAM\n",
    "        }\n",
    "    },\n",
    "    \"RuntimeConfig\": {\n",
    "        \"CopyCount\": 2  # Run 2 copies for availability\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Creating inference component: {ic_name}\")\n",
    "print(\"This will take 3-5 minutes...\")\n",
    "\n",
    "sm_client.create_inference_component(**ic_config)\n",
    "\n",
    "# Wait for IC to be in service\n",
    "while True:\n",
    "    response = sm_client.describe_inference_component(\n",
    "        InferenceComponentName=ic_name\n",
    "    )\n",
    "    status = response['InferenceComponentStatus']\n",
    "    \n",
    "    if status == 'InService':\n",
    "        print(f\"‚úÖ Inference Component is InService\")\n",
    "        break\n",
    "    elif status in ['Failed', 'Unknown']:\n",
    "        print(f\"‚ùå Failed to create IC: {status}\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Status: {status}... waiting\")\n",
    "        import time\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Invoke Inference Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_ic(ic_name, payload):\n",
    "    \"\"\"\n",
    "    Invoke a specific inference component.\n",
    "    \"\"\"\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        InferenceComponentName=ic_name,  # Target specific IC\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    return result\n",
    "\n",
    "# Test inference\n",
    "test_payload = {\n",
    "    \"inputs\": \"What is the meaning of life?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 150,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response = invoke_ic(ic_name, test_payload)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response[0]['generated_text'])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test with Messages API Format\n",
    "\n",
    "Since we enabled `MESSAGES_API_ENABLED`, we can use OpenAI-compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI-compatible format\n",
    "messages_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful AI assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain quantum computing in simple terms.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = invoke_ic(ic_name, messages_payload)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MESSAGES API RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy Second Model (A/B Testing)\n",
    "\n",
    "Deploy another model variant on the same endpoint for A/B testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_name_v2 = f\"phi3-mini-v2-ic-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "# Configuration for second variant (with different parameters)\n",
    "model_config_v2 = {\n",
    "    'HF_MODEL_ID': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "    'SM_NUM_GPUS': '1',\n",
    "    'MAX_INPUT_LENGTH': '3072',\n",
    "    'MAX_TOTAL_TOKENS': '4096',\n",
    "    'MESSAGES_API_ENABLED': 'true',\n",
    "    # Different quantization or parameters for testing\n",
    "}\n",
    "\n",
    "ic_config_v2 = {\n",
    "    \"InferenceComponentName\": ic_name_v2,\n",
    "    \"EndpointName\": endpoint_name,\n",
    "    \"VariantName\": \"AllTraffic\",\n",
    "    \"Specification\": {\n",
    "        \"ModelName\": ic_name_v2,\n",
    "        \"Container\": {\n",
    "            \"Image\": image_uri,\n",
    "            \"Environment\": model_config_v2\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 8192\n",
    "        }\n",
    "    },\n",
    "    \"RuntimeConfig\": {\n",
    "        \"CopyCount\": 1  # Start with 1 copy\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Creating second inference component: {ic_name_v2}\")\n",
    "sm_client.create_inference_component(**ic_config_v2)\n",
    "\n",
    "# Wait for IC to be ready\n",
    "import time\n",
    "while True:\n",
    "    response = sm_client.describe_inference_component(\n",
    "        InferenceComponentName=ic_name_v2\n",
    "    )\n",
    "    status = response['InferenceComponentStatus']\n",
    "    \n",
    "    if status == 'InService':\n",
    "        print(f\"‚úÖ Second IC is InService\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Status: {status}... waiting\")\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. A/B Test Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both models with same prompt\n",
    "test_prompt = {\n",
    "    \"inputs\": \"Write a haiku about artificial intelligence.\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"temperature\": 0.8,\n",
    "        \"do_sample\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"A/B TEST COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Model V1\n",
    "response_v1 = invoke_ic(ic_name, test_prompt)\n",
    "print(f\"\\nModel V1 ({ic_name}):\")\n",
    "print(\"-\" * 50)\n",
    "print(response_v1[0]['generated_text'])\n",
    "\n",
    "# Model V2  \n",
    "response_v2 = invoke_ic(ic_name_v2, test_prompt)\n",
    "print(f\"\\nModel V2 ({ic_name_v2}):\")\n",
    "print(\"-\" * 50)\n",
    "print(response_v2[0]['generated_text'])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Scale Inference Component\n",
    "\n",
    "Dynamically scale the number of copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale up to 3 copies\n",
    "print(f\"Scaling {ic_name} to 3 copies...\")\n",
    "\n",
    "sm_client.update_inference_component(\n",
    "    InferenceComponentName=ic_name,\n",
    "    RuntimeConfig={\n",
    "        \"CopyCount\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wait for update\n",
    "import time\n",
    "while True:\n",
    "    response = sm_client.describe_inference_component(\n",
    "        InferenceComponentName=ic_name\n",
    "    )\n",
    "    status = response['InferenceComponentStatus']\n",
    "    \n",
    "    if status == 'InService':\n",
    "        runtime_config = response['RuntimeConfig']\n",
    "        print(f\"‚úÖ Scaled to {runtime_config['CurrentCopyCount']} copies\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Status: {status}... waiting\")\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Rolling Update Example\n",
    "\n",
    "Update an IC to a new model version with zero downtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to Phi-3 small model (or any other variant)\n",
    "new_model_config = {\n",
    "    'HF_MODEL_ID': 'microsoft/Phi-3-small-8k-instruct',  # Different model\n",
    "    'SM_NUM_GPUS': '1',\n",
    "    'MAX_INPUT_LENGTH': '7168',\n",
    "    'MAX_TOTAL_TOKENS': '8192',\n",
    "    'MESSAGES_API_ENABLED': 'true',\n",
    "}\n",
    "\n",
    "print(f\"Performing rolling update on {ic_name}...\")\n",
    "print(\"This maintains availability during update.\")\n",
    "\n",
    "sm_client.update_inference_component(\n",
    "    InferenceComponentName=ic_name,\n",
    "    Specification={\n",
    "        \"Container\": {\n",
    "            \"Image\": image_uri,\n",
    "            \"Environment\": new_model_config\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 8192\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Monitor update progress\n",
    "while True:\n",
    "    response = sm_client.describe_inference_component(\n",
    "        InferenceComponentName=ic_name\n",
    "    )\n",
    "    status = response['InferenceComponentStatus']\n",
    "    \n",
    "    if status == 'InService':\n",
    "        print(\"‚úÖ Rolling update complete\")\n",
    "        break\n",
    "    elif status == 'Updating':\n",
    "        print(\"Update in progress...\")\n",
    "        time.sleep(30)\n",
    "    else:\n",
    "        print(f\"Status: {status}\")\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Monitor Inference Component Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IC details\n",
    "ic_details = sm_client.describe_inference_component(\n",
    "    InferenceComponentName=ic_name\n",
    ")\n",
    "\n",
    "print(\"Inference Component Details:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Name: {ic_details['InferenceComponentName']}\")\n",
    "print(f\"Status: {ic_details['InferenceComponentStatus']}\")\n",
    "print(f\"Endpoint: {ic_details['EndpointName']}\")\n",
    "print(f\"\\nCompute Resources:\")\n",
    "print(f\"  GPUs: {ic_details['Specification']['ComputeResourceRequirements']['NumberOfAcceleratorDevicesRequired']}\")\n",
    "print(f\"  Memory: {ic_details['Specification']['ComputeResourceRequirements']['MinMemoryRequiredInMb']} MB\")\n",
    "print(f\"\\nRuntime:\")\n",
    "print(f\"  Desired Copies: {ic_details['RuntimeConfig']['DesiredCopyCount']}\")\n",
    "print(f\"  Current Copies: {ic_details['RuntimeConfig']['CurrentCopyCount']}\")\n",
    "print(f\"\\nModel:\")\n",
    "env = ic_details['Specification']['Container']['Environment']\n",
    "print(f\"  Model ID: {env.get('HF_MODEL_ID', 'N/A')}\")\n",
    "print(f\"  Max Tokens: {env.get('MAX_TOTAL_TOKENS', 'N/A')}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. CloudWatch Metrics for Inference Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "cloudwatch = boto3.client('cloudwatch', region_name=region)\n",
    "\n",
    "def get_ic_metrics(ic_name, minutes=60):\n",
    "    \"\"\"\n",
    "    Get CloudWatch metrics for Inference Component.\n",
    "    \"\"\"\n",
    "    end_time = dt.datetime.utcnow()\n",
    "    start_time = end_time - dt.timedelta(minutes=minutes)\n",
    "    \n",
    "    # IC-specific metrics\n",
    "    metrics = [\n",
    "        'InferenceComponentInvocations',\n",
    "        'InferenceComponentConcurrentRequestsPerCopy',\n",
    "        'InferenceComponent4XXErrors',\n",
    "        'InferenceComponent5XXErrors'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nMetrics for {ic_name}:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for metric_name in metrics:\n",
    "        response = cloudwatch.get_metric_statistics(\n",
    "            Namespace='AWS/SageMaker',\n",
    "            MetricName=metric_name,\n",
    "            Dimensions=[\n",
    "                {'Name': 'InferenceComponentName', 'Value': ic_name}\n",
    "            ],\n",
    "            StartTime=start_time,\n",
    "            EndTime=end_time,\n",
    "            Period=300,\n",
    "            Statistics=['Sum', 'Average']\n",
    "        )\n",
    "        \n",
    "        if response['Datapoints']:\n",
    "            latest = sorted(response['Datapoints'], \n",
    "                          key=lambda x: x['Timestamp'])[-1]\n",
    "            print(f\"{metric_name}: {latest.get('Sum', latest.get('Average', 0))}\")\n",
    "        else:\n",
    "            print(f\"{metric_name}: No data\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Get metrics\n",
    "get_ic_metrics(ic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Auto-Scaling Configuration\n",
    "\n",
    "Set up auto-scaling for the Inference Component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscaling = boto3.client('application-autoscaling', region_name=region)\n",
    "\n",
    "# Register scalable target\n",
    "resource_id = f\"inference-component/{ic_name}\"\n",
    "\n",
    "autoscaling.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:inference-component:DesiredCopyCount',\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=5\n",
    ")\n",
    "\n",
    "print(f\"Registered scalable target for {ic_name}\")\n",
    "\n",
    "# Create target tracking policy\n",
    "policy_name = f\"target-tracking-{ic_name}\"\n",
    "\n",
    "autoscaling.put_scaling_policy(\n",
    "    PolicyName=policy_name,\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:inference-component:DesiredCopyCount',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 5.0,  # Target 5 concurrent requests per copy\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution'\n",
    "        },\n",
    "        'ScaleInCooldown': 300,  # 5 minutes\n",
    "        'ScaleOutCooldown': 60   # 1 minute\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Auto-scaling policy created: {policy_name}\")\n",
    "print(\"  Min copies: 1\")\n",
    "print(\"  Max copies: 5\")\n",
    "print(\"  Target: 5 concurrent requests per copy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cost savings with Inference Components\n",
    "def calculate_cost_comparison():\n",
    "    \"\"\"\n",
    "    Compare costs: Traditional deployment vs Inference Components\n",
    "    \"\"\"\n",
    "    # ml.g5.2xlarge pricing (example: $1.50/hour)\n",
    "    hourly_rate = 1.50\n",
    "    \n",
    "    print(\"\\nCost Comparison Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Traditional: 2 separate endpoints\n",
    "    traditional_instances = 2  # One per model\n",
    "    traditional_monthly = traditional_instances * hourly_rate * 730  # 730 hours/month\n",
    "    \n",
    "    print(f\"Traditional Deployment (2 models, 2 endpoints):\")\n",
    "    print(f\"  Instances: {traditional_instances}\")\n",
    "    print(f\"  Monthly Cost: ${traditional_monthly:.2f}\")\n",
    "    \n",
    "    # With ICs: 1 endpoint, shared infrastructure\n",
    "    ic_instances = 1  # Shared endpoint\n",
    "    ic_monthly = ic_instances * hourly_rate * 730\n",
    "    \n",
    "    print(f\"\\nInference Components (2 models, 1 endpoint):\")\n",
    "    print(f\"  Instances: {ic_instances}\")\n",
    "    print(f\"  Monthly Cost: ${ic_monthly:.2f}\")\n",
    "    \n",
    "    savings = traditional_monthly - ic_monthly\n",
    "    savings_pct = (savings / traditional_monthly) * 100\n",
    "    \n",
    "    print(f\"\\nüí∞ Monthly Savings: ${savings:.2f} ({savings_pct:.1f}%)\")\n",
    "    print(f\"üí∞ Annual Savings: ${savings * 12:.2f}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "calculate_cost_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete auto-scaling policy\n",
    "try:\n",
    "    autoscaling.deregister_scalable_target(\n",
    "        ServiceNamespace='sagemaker',\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension='sagemaker:inference-component:DesiredCopyCount'\n",
    "    )\n",
    "    print(\"‚úÖ Auto-scaling policy deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "# Delete inference components\n",
    "for ic in [ic_name, ic_name_v2]:\n",
    "    try:\n",
    "        print(f\"Deleting IC: {ic}\")\n",
    "        sm_client.delete_inference_component(\n",
    "            InferenceComponentName=ic\n",
    "        )\n",
    "        print(f\"‚úÖ {ic} deleted\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: {e}\")\n",
    "\n",
    "# Wait for ICs to be deleted\n",
    "import time\n",
    "time.sleep(60)\n",
    "\n",
    "# Delete endpoint\n",
    "print(f\"\\nDeleting endpoint: {endpoint_name}\")\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "print(\"‚úÖ Endpoint deleted\")\n",
    "\n",
    "print(\"\\n‚úÖ All resources cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "1. ‚úÖ Created endpoint without models\n",
    "2. ‚úÖ Deployed Phi-3 via Inference Component\n",
    "3. ‚úÖ Used Messages API format\n",
    "4. ‚úÖ Deployed second model for A/B testing\n",
    "5. ‚úÖ Scaled Inference Components dynamically\n",
    "6. ‚úÖ Performed rolling updates\n",
    "7. ‚úÖ Monitored IC-specific metrics\n",
    "8. ‚úÖ Configured auto-scaling\n",
    "9. ‚úÖ Analyzed cost savings\n",
    "\n",
    "### Key Benefits of Inference Components\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| Resource Optimization | 50%+ cost savings on average |\n",
    "| Model Packing | Multiple models on shared infrastructure |\n",
    "| Independent Scaling | Scale each model based on demand |\n",
    "| Zero Downtime | Rolling updates without service interruption |\n",
    "| A/B Testing | Easy model comparison on same endpoint |\n",
    "| Granular Metrics | Per-model monitoring and alerting |\n",
    "\n",
    "### When to Use Inference Components\n",
    "\n",
    "‚úÖ **Use ICs when:**\n",
    "- Running multiple models\n",
    "- Need cost optimization\n",
    "- Require A/B testing\n",
    "- Want independent scaling\n",
    "- Need zero-downtime updates\n",
    "\n",
    "‚ùå **Skip ICs when:**\n",
    "- Single model deployment\n",
    "- Maximum simplicity needed\n",
    "- Legacy workflows\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Implement scale-to-zero for intermittent workloads\n",
    "- Set up CloudWatch alarms for IC metrics\n",
    "- Deploy quantized models (AWQ/GPTQ)\n",
    "- Integrate with API Gateway\n",
    "- Build production MLOps pipeline\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Inference Components Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-components.html)\n",
    "- [Cost Optimization Blog](https://aws.amazon.com/blogs/machine-learning/reduce-model-deployment-costs-by-50-on-average-using-sagemakers-latest-features/)\n",
    "- [Rolling Updates Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-component-rolling-updates.html)\n",
    "- [Auto-scaling Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
