{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Phi-3 Model on SageMaker using Large Model Inference (LMI) Container\n",
    "\n",
    "This notebook demonstrates how to deploy Microsoft's Phi-3 model on Amazon SageMaker using the SageMaker Large Model Inference (LMI) container with vLLM backend.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Model**: microsoft/Phi-3-mini-4k-instruct (3.8B parameters)\n",
    "- **Container**: DJL LMI Container with vLLM backend\n",
    "- **Instance Type**: ml.g5.2xlarge (1 GPU)\n",
    "- **Backend**: vLLM for optimized inference\n",
    "\n",
    "## What is LMI?\n",
    "\n",
    "SageMaker Large Model Inference (LMI) containers are purpose-built Docker containers for LLM inference. They provide:\n",
    "\n",
    "- **Multiple Backend Support**: vLLM, TensorRT-LLM, Transformers NeuronX\n",
    "- **Optimized Performance**: Continuous batching, quantization, tensor parallelism\n",
    "- **Easy Configuration**: Environment variables or serving.properties file\n",
    "- **Flexibility**: Works with HuggingFace models or custom S3 artifacts\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS Account with SageMaker access\n",
    "- Appropriate IAM role\n",
    "- GPU instance quota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade --quiet\n",
    "!pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sagemaker import Model\n",
    "\n",
    "# Initialize session\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"AWS region: {region}\")\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get LMI Container Image URI\n",
    "\n",
    "We'll use the latest DJL LMI container with vLLM backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMI container configuration\n",
    "# Check for latest version at: https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "LMI_VERSION = '0.31.0-lmi13.0.0-cu124'\n",
    "\n",
    "# Construct the image URI\n",
    "inference_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{LMI_VERSION}\"\n",
    "\n",
    "print(f\"Using LMI container: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method 1: Deploy using Environment Variables\n",
    "\n",
    "This is the most flexible approach as configuration is stored in the SageMaker Model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name_env = f\"phi3-mini-lmi-env-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "endpoint_name_env = f\"{model_name_env}-ep\"\n",
    "\n",
    "# LMI configuration via environment variables\n",
    "lmi_env_config = {\n",
    "    # Model configuration\n",
    "    'HF_MODEL_ID': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "    \n",
    "    # Backend configuration\n",
    "    'OPTION_ROLLING_BATCH': 'vllm',  # Use vLLM backend\n",
    "    'OPTION_MAX_ROLLING_BATCH_SIZE': '8',\n",
    "    \n",
    "    # GPU configuration\n",
    "    'OPTION_TENSOR_PARALLEL_DEGREE': '1',  # Number of GPUs\n",
    "    \n",
    "    # Context and generation limits\n",
    "    'OPTION_MAX_MODEL_LEN': '4096',\n",
    "    'OPTION_MAX_INPUT_LEN': '3072',\n",
    "    \n",
    "    # Performance tuning\n",
    "    'OPTION_DTYPE': 'fp16',  # or 'bf16' for better precision\n",
    "    'OPTION_GPU_MEMORY_UTILIZATION': '0.9',\n",
    "    \n",
    "    # Optional: Quantization for memory efficiency\n",
    "    # 'OPTION_QUANTIZE': 'awq',  # or 'gptq'\n",
    "    \n",
    "    # Optional: HuggingFace token for gated models\n",
    "    # 'HUGGING_FACE_HUB_TOKEN': '<YOUR_HF_TOKEN>',\n",
    "}\n",
    "\n",
    "print(f\"Model name: {model_name_env}\")\n",
    "print(f\"Endpoint name: {endpoint_name_env}\")\n",
    "print(f\"\\nLMI Configuration:\")\n",
    "print(json.dumps(lmi_env_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SageMaker Model with environment variables\n",
    "model_env = Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=None,  # Model will be downloaded from HuggingFace\n",
    "    role=role,\n",
    "    name=model_name_env,\n",
    "    env=lmi_env_config,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "print(f\"Model object created: {model_name_env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "print(\"Deploying model... This will take 5-10 minutes.\")\n",
    "\n",
    "predictor_env = model_env.deploy(\n",
    "    endpoint_name=endpoint_name_env,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    container_startup_health_check_timeout=600,\n",
    "    model_data_download_timeout=900,\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Endpoint deployed successfully: {endpoint_name_env}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Deployed Model\n",
    "\n",
    "Test basic inference with the deployed endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test\n",
    "test_payload = {\n",
    "    \"inputs\": \"What is the capital of France?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor_env.predict(test_payload)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response['generated_text'])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Method 2: Deploy using serving.properties File\n",
    "\n",
    "This method packages configuration with the model artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "# Create directory for model artifacts\n",
    "model_dir = \"phi3_model_artifacts\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Create serving.properties file\n",
    "serving_properties = \"\"\"# Phi-3 Model Configuration\n",
    "engine=Python\n",
    "option.model_id=microsoft/Phi-3-mini-4k-instruct\n",
    "\n",
    "# Backend configuration\n",
    "option.rolling_batch=vllm\n",
    "option.max_rolling_batch_size=8\n",
    "\n",
    "# GPU configuration\n",
    "option.tensor_parallel_degree=1\n",
    "\n",
    "# Context limits\n",
    "option.max_model_len=4096\n",
    "option.max_input_len=3072\n",
    "\n",
    "# Performance settings\n",
    "option.dtype=fp16\n",
    "option.gpu_memory_utilization=0.9\n",
    "\n",
    "# Optional: Quantization\n",
    "# option.quantize=awq\n",
    "\n",
    "# Optional: HuggingFace token\n",
    "# option.huggingface_token=<YOUR_HF_TOKEN>\n",
    "\"\"\"\n",
    "\n",
    "# Write serving.properties\n",
    "with open(f\"{model_dir}/serving.properties\", 'w') as f:\n",
    "    f.write(serving_properties)\n",
    "\n",
    "print(\"serving.properties created:\")\n",
    "print(serving_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tar.gz archive\n",
    "tarball_name = \"phi3_model.tar.gz\"\n",
    "\n",
    "with tarfile.open(tarball_name, \"w:gz\") as tar:\n",
    "    tar.add(model_dir, arcname=\".\")\n",
    "\n",
    "print(f\"Created tarball: {tarball_name}\")\n",
    "\n",
    "# Upload to S3\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"phi3-lmi-models\"\n",
    "s3_model_uri = f\"s3://{bucket}/{prefix}/{tarball_name}\"\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(tarball_name, bucket, f\"{prefix}/{tarball_name}\")\n",
    "\n",
    "print(f\"Model artifacts uploaded to: {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with serving.properties\n",
    "model_name_props = f\"phi3-mini-lmi-props-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "endpoint_name_props = f\"{model_name_props}-ep\"\n",
    "\n",
    "model_props = Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=s3_model_uri,\n",
    "    role=role,\n",
    "    name=model_name_props,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "print(f\"Model with serving.properties created: {model_name_props}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Inference Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat-based inference with Phi-3 format\n",
    "def create_phi3_prompt(system_msg, user_msg):\n",
    "    \"\"\"\n",
    "    Create a properly formatted Phi-3 prompt.\n",
    "    \"\"\"\n",
    "    prompt = f\"<|system|>\\n{system_msg}<|end|>\\n\"\n",
    "    prompt += f\"<|user|>\\n{user_msg}<|end|>\\n\"\n",
    "    prompt += \"<|assistant|>\\n\"\n",
    "    return prompt\n",
    "\n",
    "system_message = \"You are a helpful AI assistant specialized in explaining technical concepts.\"\n",
    "user_message = \"Explain how transformers work in machine learning.\"\n",
    "\n",
    "formatted_prompt = create_phi3_prompt(system_message, user_message)\n",
    "\n",
    "chat_payload = {\n",
    "    \"inputs\": formatted_prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 300,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 50,\n",
    "        \"do_sample\": True,\n",
    "        \"stop\": [\"<|end|>\", \"<|endoftext|>\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor_env.predict(chat_payload)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHAT RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response['generated_text'])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Streaming Inference with LMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_lmi_response(endpoint_name, payload):\n",
    "    \"\"\"\n",
    "    Stream responses from LMI endpoint.\n",
    "    \"\"\"\n",
    "    # Add stream parameter\n",
    "    payload['parameters']['stream'] = True\n",
    "    \n",
    "    response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "    \n",
    "    event_stream = response['Body']\n",
    "    \n",
    "    print(\"\\nStreaming response:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    full_text = \"\"\n",
    "    \n",
    "    for event in event_stream:\n",
    "        if 'PayloadPart' in event:\n",
    "            chunk = event['PayloadPart']['Bytes'].decode('utf-8')\n",
    "            \n",
    "            # Parse JSON lines\n",
    "            for line in chunk.strip().split('\\n'):\n",
    "                if line.startswith('data:'):\n",
    "                    data_str = line[5:].strip()\n",
    "                    if data_str == '[DONE]':\n",
    "                        break\n",
    "                    try:\n",
    "                        data = json.loads(data_str)\n",
    "                        if 'token' in data:\n",
    "                            token_text = data['token']['text']\n",
    "                            print(token_text, end='', flush=True)\n",
    "                            full_text += token_text\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    return full_text\n",
    "\n",
    "# Stream a response\n",
    "stream_payload = {\n",
    "    \"inputs\": \"Write a short story about a robot learning to paint.\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "result = stream_lmi_response(endpoint_name_env, stream_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Benchmark Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def benchmark_endpoint(endpoint_name, num_requests=10):\n",
    "    \"\"\"\n",
    "    Benchmark endpoint performance.\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    \n",
    "    test_prompt = \"Explain artificial intelligence in simple terms.\"\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": test_prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 100,\n",
    "            \"temperature\": 0.7,\n",
    "            \"do_sample\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Running {num_requests} inference requests...\\n\")\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(payload),\n",
    "            ContentType='application/json'\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        latency = (end_time - start_time) * 1000  # Convert to ms\n",
    "        latencies.append(latency)\n",
    "        \n",
    "        print(f\"Request {i+1}: {latency:.2f} ms\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Benchmark Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Mean Latency: {np.mean(latencies):.2f} ms\")\n",
    "    print(f\"Median Latency: {np.median(latencies):.2f} ms\")\n",
    "    print(f\"Min Latency: {np.min(latencies):.2f} ms\")\n",
    "    print(f\"Max Latency: {np.max(latencies):.2f} ms\")\n",
    "    print(f\"Std Dev: {np.std(latencies):.2f} ms\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_endpoint(endpoint_name_env, num_requests=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Multi-turn Conversation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_turn_chat(endpoint_name, conversation_history):\n",
    "    \"\"\"\n",
    "    Handle multi-turn conversations.\n",
    "    conversation_history: list of {'role': 'user'/'assistant', 'content': 'text'}\n",
    "    \"\"\"\n",
    "    # Build the prompt\n",
    "    prompt = \"<|system|>\\nYou are a helpful AI assistant.<|end|>\\n\"\n",
    "    \n",
    "    for turn in conversation_history:\n",
    "        role = turn['role']\n",
    "        content = turn['content']\n",
    "        \n",
    "        if role == 'user':\n",
    "            prompt += f\"<|user|>\\n{content}<|end|>\\n\"\n",
    "        elif role == 'assistant':\n",
    "            prompt += f\"<|assistant|>\\n{content}<|end|>\\n\"\n",
    "    \n",
    "    # Add assistant prefix for next response\n",
    "    prompt += \"<|assistant|>\\n\"\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 200,\n",
    "            \"temperature\": 0.7,\n",
    "            \"do_sample\": True,\n",
    "            \"stop\": [\"<|end|>\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    return result['generated_text']\n",
    "\n",
    "# Example conversation\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "]\n",
    "\n",
    "print(\"Turn 1:\")\n",
    "response1 = multi_turn_chat(endpoint_name_env, conversation)\n",
    "print(f\"Assistant: {response1}\\n\")\n",
    "\n",
    "# Add to conversation history\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response1})\n",
    "conversation.append({\"role\": \"user\", \"content\": \"Can you give me an example?\"})\n",
    "\n",
    "print(\"Turn 2:\")\n",
    "response2 = multi_turn_chat(endpoint_name_env, conversation)\n",
    "print(f\"Assistant: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Handling and Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "\n",
    "def invoke_with_retry(endpoint_name, payload, max_retries=3):\n",
    "    \"\"\"\n",
    "    Invoke endpoint with retry logic.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = sagemaker_runtime.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                Body=json.dumps(payload),\n",
    "                ContentType='application/json'\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            return result\n",
    "            \n",
    "        except ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            print(f\"Attempt {attempt + 1} failed: {error_code}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Max retries reached. Error: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "# Test retry logic\n",
    "test_payload = {\n",
    "    \"inputs\": \"Test retry logic.\",\n",
    "    \"parameters\": {\"max_new_tokens\": 50}\n",
    "}\n",
    "\n",
    "result = invoke_with_retry(endpoint_name_env, test_payload)\n",
    "print(f\"\\nResponse: {result['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Monitor CloudWatch Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "cloudwatch = boto3.client('cloudwatch', region_name=region)\n",
    "\n",
    "def get_endpoint_metrics(endpoint_name, metric_name, minutes=60):\n",
    "    \"\"\"\n",
    "    Get CloudWatch metrics for the endpoint.\n",
    "    \"\"\"\n",
    "    end_time = dt.datetime.utcnow()\n",
    "    start_time = end_time - dt.timedelta(minutes=minutes)\n",
    "    \n",
    "    response = cloudwatch.get_metric_statistics(\n",
    "        Namespace='AWS/SageMaker',\n",
    "        MetricName=metric_name,\n",
    "        Dimensions=[\n",
    "            {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "            {'Name': 'VariantName', 'Value': 'AllTraffic'}\n",
    "        ],\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time,\n",
    "        Period=300,  # 5 minutes\n",
    "        Statistics=['Average', 'Sum', 'Maximum']\n",
    "    )\n",
    "    \n",
    "    return response['Datapoints']\n",
    "\n",
    "# Get invocation metrics\n",
    "print(\"Fetching endpoint metrics...\\n\")\n",
    "\n",
    "metrics = ['Invocations', 'ModelLatency', 'Invocation4XXErrors', 'Invocation5XXErrors']\n",
    "\n",
    "for metric in metrics:\n",
    "    datapoints = get_endpoint_metrics(endpoint_name_env, metric)\n",
    "    if datapoints:\n",
    "        print(f\"\\n{metric}:\")\n",
    "        for dp in sorted(datapoints, key=lambda x: x['Timestamp'])[-5:]:\n",
    "            print(f\"  {dp['Timestamp']}: {dp.get('Sum', dp.get('Average', 'N/A'))}\")\n",
    "    else:\n",
    "        print(f\"\\n{metric}: No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint\n",
    "print(f\"Deleting endpoint: {endpoint_name_env}\")\n",
    "predictor_env.delete_endpoint(delete_endpoint_config=True)\n",
    "\n",
    "# Delete model\n",
    "print(f\"Deleting model: {model_name_env}\")\n",
    "predictor_env.delete_model()\n",
    "\n",
    "# Optionally delete S3 artifacts\n",
    "# s3_client.delete_object(Bucket=bucket, Key=f\"{prefix}/{tarball_name}\")\n",
    "\n",
    "print(\"\\n✅ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. ✅ Understanding LMI containers and vLLM backend\n",
    "2. ✅ Two deployment methods (environment variables vs serving.properties)\n",
    "3. ✅ Chat-based inference with proper formatting\n",
    "4. ✅ Streaming responses\n",
    "5. ✅ Performance benchmarking\n",
    "6. ✅ Multi-turn conversations\n",
    "7. ✅ Error handling and retry logic\n",
    "8. ✅ CloudWatch metrics monitoring\n",
    "9. ✅ Resource cleanup\n",
    "\n",
    "## Key Differences: LMI vs TGI\n",
    "\n",
    "| Feature | LMI (vLLM) | TGI |\n",
    "|---------|------------|-----|\n",
    "| Backend | vLLM, TensorRT-LLM, Transformers | HuggingFace TGI |\n",
    "| Configuration | serving.properties or env vars | Env vars |\n",
    "| Flexibility | Multiple backends | Single backend |\n",
    "| Quantization | AWQ, GPTQ, FP8 | AWQ, GPTQ |\n",
    "| Best For | Production, multi-backend | Quick deployment |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Deploy Phi-3 medium (14B) or small (7B) models\n",
    "- Implement auto-scaling with CloudWatch alarms\n",
    "- Test quantization (AWQ/GPTQ) for memory efficiency\n",
    "- Set up A/B testing with multiple endpoints\n",
    "- Integrate with Lambda for serverless inference\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [DJL LMI Documentation](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html)\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [SageMaker LMI Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-inference.html)\n",
    "- [Phi-3 on HuggingFace](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
