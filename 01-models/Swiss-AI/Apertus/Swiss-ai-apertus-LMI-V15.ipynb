{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4cae57",
   "metadata": {},
   "source": [
    "\n",
    "# Deploying Swiss LLM Apertus on SageMaker AI with LMI v16 powered by vLLM\n",
    "\n",
    "This notebook demonstrates deploying and running inference with the Apertus model. We will cover \n",
    "\n",
    "1. Installing SageMaker python SDK, Setting up SageMaker resources and permissions\n",
    "2. Deploying the model using SageMaker LMI (Large Model Inference Container powered by Vllm)\n",
    "3. Invoking the model using streaming responses\n",
    "\n",
    "Apertus is also deployable through Amazon SageMaker JumpStart. You can learn more about Apertus on Amazon SageMaker AI in our blog post: [Switzerland‚Äôs Open-Source Apertus LLMs now available on Amazon SageMaker AI](https://aws.amazon.com/blogs/alps/switzerlands-open-source-apertus-llms-now-available-on-amazon-sagemaker-ai/)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Accept the license for the model on HuggingFace Hub. The model comes in two different sizes (8B and 70B) and as Instruct and Non-Instruct variants. Go to the HuggingFace Hub page for the model that you want to deploy, accept the license agreement, and copy the respective model id.\n",
    "    * [HuggingFace Hub Apertus 8B Instruct 2509](https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509)\n",
    "    * [HuggingFace Hub Apertus 70B Instruct 2509](https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509)\n",
    "    * [HuggingFace Hub Apertus 8B 2509](https://huggingface.co/swiss-ai/Apertus-8B-2509)\n",
    "    * [HuggingFace Hub Apertus 70B 2509](https://huggingface.co/swiss-ai/Apertus-70B-2509)\n",
    "2. Make sure that you have sufficient Service Quota for SageMaker on-demand endpoint usage for G or P instances. Below is some general guidance which instance types to use.\n",
    "\n",
    "\n",
    "**SageMaker Instance Types for Model Deployment**\n",
    "\n",
    "| Model Size | Environment | Recommended Instance Types |\n",
    "|------------|-------------|---------------------------|\n",
    "| **70B** | Production | ml.p4d.24xlarge, ml.p5.48xlarge |\n",
    "| **70B** | Testing | ml.g5.48xlarge, ml.g6.48xlarge, ml.g6e.48xlarge |\n",
    "| **8B** | Production | ml.g5.4xlarge, ml.g6.4xlarge, ml.g5.48xlarge, ml.g6.48xlarge |\n",
    "| **8B** | Testing | ml.g5.xlarge, ml.g6.xlarge |\n",
    "\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, we'll install the SageMaker SDK to ensure compatibility with the latest features, particularly those needed for large language model deployment and streaming inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e49ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b6729-c48f-432e-a3f1-52cd5b60ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_mode = False  # if you have a local GPU you can also run the model locally using SageMaker SDK, e.g. for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e630a2",
   "metadata": {},
   "source": [
    "Replace the model id below with the one you want to deploy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e18afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"swiss-ai/Apertus-8B-Instruct-2509\" # PICK which model to deploy\n",
    "# MODEL_ID = \"swiss-ai/Apertus-70B-2509\"\n",
    "# MODEL_ID = \"swiss-ai/Apertus-70B-Instruct-2509\"\n",
    "# MODEL_ID = \"swiss-ai/Apertus-8B-2509\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320f48a-5d13-4bdc-a07f-73c3c851b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_mode:\n",
    "    %pip install sagemaker[local] --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca4505-aa3b-477b-9b90-aa3ef8f26576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import Model, Session, get_execution_role \n",
    "from sagemaker.utils import name_from_base\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "role = get_execution_role()  # execution role for the endpoint\n",
    "\n",
    "if local_mode:\n",
    "    from sagemaker.local import entities, LocalSession\n",
    "\n",
    "    # Extend LocalMode‚Äôs health-check timeout to 15 minutes\n",
    "    entities.HEALTH_CHECK_TIMEOUT_LIMIT = 15 * 60  # seconds\n",
    "\n",
    "    sess = LocalSession()\n",
    "    sess.config = {\"local\": {\"local_code\": True}}\n",
    "else:\n",
    "    sess = Session() # sagemaker session for interacting with different AWS APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb6e81",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Apertus, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container with vLLM** : A container optimized for large language model inference\n",
    "- **G or P Instance**: AWS's GPU instance type optimized for large model inference\n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.g6.48xlarge` instances which offer:\n",
    "  - 8 NVIDIA L4 GPUs with 192 GB GPU memory\n",
    "  - 768 GB of memory\n",
    "  - High network bandwidth for optimal inference performance\n",
    "\n",
    "> **Note**: REPLACE `eu-central-2` with your region if different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a91b2-29c3-4056-944f-130759ec5f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define region where you have capacity\n",
    "REGION = \"eu-central-2\"\n",
    "INSTANCE_TYPE = (\n",
    "    \"local_gpu\" if local_mode else \"ml.g6.48xlarge\"\n",
    ")  # Review the instance type. Find the one most suitable for your need with the guidance provided in the prerequisites section.\n",
    "\n",
    "# Select the latest container. Check the link for the latest available version https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers\n",
    "CONTAINER_VERSION = \"0.34.0-lmi16.0.0-cu128\"\n",
    "\n",
    "# Construct container URI\n",
    "if REGION == \"eu-central-2\":\n",
    "    container_account = 380420809688\n",
    "else:\n",
    "    container_account = 763104351884\n",
    "\n",
    "container_uri = f\"{container_account}.dkr.ecr.{REGION}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    "\n",
    "\n",
    "# Validate region and print configuration\n",
    "if REGION != sess.boto_region_name:\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è Warning: Container region ({REGION}) differs from session region ({sess.boto_region_name})\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"‚úÖ Region validation passed: {REGION}\")\n",
    "\n",
    "print(f\"üì¶ Container URI: {container_uri}\")\n",
    "print(f\"üñ•Ô∏è Instance Type: {INSTANCE_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfeb11-df3b-4b21-bb96-21c2390fad60",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- vllm env variables\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Env Variables** (`env`): Our variables for the model server\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371045b",
   "metadata": {},
   "source": [
    "> **Note**: DJL v16 comes with transformers version 4.55.2. The transformers implementation of Apertus is only available starting with transformers version 4.56.0 so we need to update the transformers version in the inference container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406b26d-5a1c-4a73-a336-c39c6bb04095",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = \"transformers==4.57.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb11867-8b40-41fa-9611-4652c2f72e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store requirements >requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa31f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_properties = f\"\"\"engine=Python\n",
    "option.async_mode=true\n",
    "option.rolling_batch=disable\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "option.model_id={MODEL_ID}\n",
    "\n",
    "# vLLM configuration\n",
    "# Update based on your needs.\n",
    "# Also view: https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/vllm_user_guide.html#quick-start-configurations\n",
    "option.max_model_len=4096\n",
    "option.model_loading_timeout=1500\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store serving_properties >serving.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d58ac",
   "metadata": {},
   "source": [
    "We combine the requirements file with the model weights into a single archive to upload to an Amazon S3 bucket. The Amazon SageMaker inference enpoint will download the archive from the Amazon S3 bucket and extract it into the inference container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd2331-33c6-4822-8508-917daddef0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir -p apertus-code\n",
    "mv requirements.txt apertus-code/\n",
    "mv serving.properties apertus-code/\n",
    "tar czvf apertus.tar.gz -C apertus-code ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73775db7",
   "metadata": {},
   "source": [
    "Replace `<your-bucket-name>` with your own Amazon S3 bucket name in the same region in which you plan to deploy the endpoint in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a1ac6-ced0-4690-afe2-e918becd8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model artifacts to S3\n",
    "bucket = \"<your-bucket-name>\" # REPLACE with the name of you Amazon S3 bucket\n",
    "\n",
    "s3_code_prefix = \"apertus-lmi\"\n",
    "s3_object_name = \"apertus.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba416c-354e-4762-8ce2-6c786f7d8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not bucket or bucket == \"<your-bucket-name>\": # DO NOT replace this string\n",
    "    raise ValueError(\"‚ùå Please set a valid S3 bucket name. Replace bucket='<your-bucket-name>'.\")\n",
    "\n",
    "code_artifact = sess.upload_data(, bucket, s3_code_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca723ce-e5f7-42fc-ac31-5e16c2393676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated vLLM configuration to use local model\n",
    "vllm_config = {\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\", # Faster downloads\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bada9f4",
   "metadata": {},
   "source": [
    "The Model object combines all the information on how to deploy the model to an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecd197-91b2-4f36-bcec-2afb8c7c370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    image_uri=container_uri,\n",
    "    role=role,\n",
    "    model_data=code_artifact,\n",
    "    sagemaker_session=sess,\n",
    "    env=vllm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba06783-5e04-4a7e-9dc9-0346535e85bc",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (G6 instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.g6.48xlarge` for high-performance inference\n",
    "- **Health Check Timeout**: 1800 seconds \n",
    "  - Extended timeout needed for large model loading\n",
    "  - Includes time for container setup and model initialization\n",
    "\n",
    "> ‚ö†Ô∏è **Important**: \n",
    "> - Deployment can take upto 15 minutes\n",
    "> - Monitor the endpoint status in SageMaker Console and CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c369f52-af94-4710-94f9-2d28bfd4d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_mode:\n",
    "    # To see progress\n",
    "    !docker pull $container_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2741a6-b7e1-4b75-8cb2-95555fd27968",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = name_from_base(MODEL_ID.replace(\"/\", \"-\"))\n",
    "\n",
    "print(endpoint_name)\n",
    "\n",
    "try:\n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=INSTANCE_TYPE,\n",
    "        endpoint_name=endpoint_name,\n",
    "        container_startup_health_check_timeout=1800,\n",
    "    )\n",
    "    print(f\"\\n‚úÖ Endpoint '{endpoint_name}' deployed successfully\")\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == 'ResourceLimitExceeded':\n",
    "        print(\n",
    "            \"‚ùå Resource limit exceeded.\"\n",
    "            + f\"Did you request the necessary Service Quotas for {INSTANCE_TYPE} in {REGION}?\"\n",
    "            + \"See also https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error\"\n",
    "        )\n",
    "    elif error_code == 'InsufficientInstanceCapacity':\n",
    "        print(\n",
    "            \"‚ùå Insufficient instance capacity. Try a different AZ or instance type\"\n",
    "            + \"See also https://repost.aws/knowledge-center/sagemaker-insufficient-capacity-error\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"‚ùå Deployment failed: {e}\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected deployment error: {e}\")\n",
    "    print(\"üí° Check CloudWatch logs for detailed error information\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e6565-53d5-4595-a0e1-4ed9ac799a71",
   "metadata": {},
   "source": [
    "## Running Inference requests to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799ae2f",
   "metadata": {},
   "source": [
    "Once you have deployed the model to the Amazon SageMaker inference endpoint you can invoke it. Replace `<your_endpoint_name>` below with the name of your SageMaker inference endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c823f6-de34-438e-b36b-ed21015d62eb",
   "metadata": {},
   "source": [
    "### Option 1: Invoke model with response streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a02b4d-1e4e-4fe6-b662-9e25e1c0f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps as json_dumps, loads as json_loads, JSONDecodeError\n",
    "from boto3 import client\n",
    "from time import time\n",
    "\n",
    "# Create SageMaker Runtime client\n",
    "smr_client = client(\"sagemaker-runtime\")\n",
    "\n",
    "endpoint_name = \"<your_endpoint_name>\" # REPLACE with your endpoint\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "if endpoint_name == \"<your_endpoint_name>\": # DO NOT replace this string\n",
    "    raise ValueError(\"‚ùå Please set a valid endpoint name\")\n",
    "\n",
    "# Invoke with messages format\n",
    "body = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain the the Swiss national sport Schwingen.\"}\n",
    "    ],\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 256,\n",
    "    \"stream\": True,\n",
    "}\n",
    "\n",
    "start_time = time()\n",
    "first_token_received = False\n",
    "ttft = None\n",
    "token_count = 0\n",
    "full_response = \"\"\n",
    "\n",
    "print(f\"Prompt: {body['messages'][0]['content']}\\n\")\n",
    "print(\"Response:\", end=\" \", flush=True)\n",
    "\n",
    "# Invoke endpoint with streaming\n",
    "\n",
    "try:\n",
    "    resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json_dumps(body),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == 'ValidationException':\n",
    "        print(\"‚ùå Validation Exception. Invalid request format or parameters\")\n",
    "    elif error_code == 'ModelError':\n",
    "        print(\"‚ùå Model error. Check model logs\")\n",
    "    else:\n",
    "        print(f\"‚ùå Inference failed: {e}\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected inference error: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Process streaming response\n",
    "for event in resp[\"Body\"]:\n",
    "    if \"PayloadPart\" in event:\n",
    "        payload = event[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "\n",
    "        try:\n",
    "\n",
    "            if payload.startswith(\"data: \"):\n",
    "                data = json_loads(payload[6:])  # Skip \"data: \" prefix\n",
    "            else:\n",
    "                data = json_loads(payload)\n",
    "\n",
    "            token_count += 1\n",
    "            if not first_token_received:\n",
    "                ttft = time() - start_time\n",
    "                first_token_received = True\n",
    "\n",
    "            # Handle different streaming response formats\n",
    "            if \"choices\" in data and len(data[\"choices\"]) > 0:\n",
    "                # Messages-compatible format\n",
    "                if (\n",
    "                    \"delta\" in data[\"choices\"][0]\n",
    "                    and \"content\" in data[\"choices\"][0][\"delta\"]\n",
    "                ):\n",
    "                    token_text = data[\"choices\"][0][\"delta\"][\"content\"]\n",
    "                    full_response += token_text\n",
    "                    print(token_text, end=\"\", flush=True)\n",
    "            elif \"token\" in data and \"text\" in data[\"token\"]:\n",
    "                # TGI format\n",
    "                token_text = data[\"token\"][\"text\"]\n",
    "                full_response += token_text\n",
    "                print(token_text, end=\"\", flush=True)\n",
    "\n",
    "        except JSONDecodeError:\n",
    "            # Skip invalid JSON\n",
    "            continue\n",
    "\n",
    "end_time = time()\n",
    "total_latency = end_time - start_time\n",
    "\n",
    "print(\"\\n\\nMetrics:\")\n",
    "if ttft:\n",
    "    print(\n",
    "        f\"Time to First Token (TTFT): {ttft:.2f} seconds\"\n",
    "    )\n",
    "else:\n",
    "    print('No tokens received')\n",
    "print(f\"Total Tokens Generated: {token_count}\")\n",
    "print(f\"Total Latency: {total_latency:.2f} seconds\")\n",
    "# print(f\"\\nFull Response:\\n{full_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461496e-4f59-4934-801c-917ab3b43123",
   "metadata": {},
   "source": [
    "### Option 2: Invoke without Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae097cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps as json_dumps, loads as json_loads, JSONDecodeError\n",
    "from boto3 import client\n",
    "\n",
    "\n",
    "# Create SageMaker Runtime client for invocation\n",
    "smr_client = client('sagemaker-runtime')\n",
    "\n",
    "endpoint_name = \"<your_endpoint_name>\" # REPLACE with your endpoint\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "if endpoint_name == \"<your_endpoint_name>\": # DO NOT replace this string\n",
    "    raise ValueError(\"‚ùå Please set a valid endpoint name\")\n",
    "\n",
    "\n",
    "\n",
    "# Invoke with messages format\n",
    "body = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain the the Swiss national sport Schwingen.\"}\n",
    "    ],\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 256,\n",
    "    \"stream\": False,\n",
    "}\n",
    "\n",
    "print(f\"Prompt: {body['messages'][0]['content']}\\n\")\n",
    "\n",
    "try:\n",
    "    # Non-streaming invocation\n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json_dumps(body)\n",
    "    )\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == 'ValidationException':\n",
    "        print(\"‚ùå Validation Exception. Invalid request format or parameters\")\n",
    "    elif error_code == 'ModelError':\n",
    "        print(\"‚ùå Model error. Check model logs\")\n",
    "    else:\n",
    "        print(f\"‚ùå Inference failed: {e}\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected inference error: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "result = json_loads(response['Body'].read().decode())\n",
    "print(result[\"choices\"][0][\"message\"][\"content\"])\n",
    "print(f\"\\nFull Response:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17e4f7",
   "metadata": {},
   "source": [
    "## Cleanup: Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c859617-8585-4cea-8fb3-c90b0f33c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"<your_endpoint_name>\" # REPLACE with your endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3f63a-643a-4fdc-a730-ec5c70425135",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"<your_model_name>\" # REPLACE with your model name\n",
    "if model and model.name:\n",
    "    model_name = model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d557536-b030-4667-8da3-33314e18418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The next cell deletes the following SageMaker resources: {endpoint_name} (endpoint & endpoint config) & {model_name} (model).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c37be7-fba1-4af0-a317-31436331a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker import Session\n",
    "\n",
    "# # Initialize session\n",
    "# sess = Session()\n",
    "\n",
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd402b07",
   "metadata": {},
   "source": [
    "Remove the local artifacts which contain the model weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9c846-fdc4-4167-af9d-67d7193d5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf apertus-code\n",
    "!rm apertus.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc836e-9429-4468-af8d-83989572b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from os import path\n",
    "\n",
    "Markdown(f\"The next cell deletes the code artidact at {path.join('s3://',bucket,s3_code_prefix,s3_object_name)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82068d6-5c87-41b1-a14c-6c0ce7eaf802",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = sess.boto_session.client(\"s3\")\n",
    "s3_client.delete_object(\n",
    "    Bucket=bucket,\n",
    "    Key=path.join(s3_code_prefix, s3_object_name)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
