{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Llama 3.1 to SageMaker with Custom Model Parser for Strands Agent\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will understand how to:\n",
    "\n",
    "1. **Set up a SageMaker environment** - Initialize sessions, retrieve credentials, and configure AWS resources automatically\n",
    "2. **Use ml-container-creator** - Generate complete deployment projects with infrastructure-as-code\n",
    "3. **Build and push containers** - Use AWS CodeBuild to create and publish Docker containers to ECR\n",
    "4. **Deploy models to SageMaker** - Create real-time inference endpoints for LLMs\n",
    "5. **Build custom model providers** - Extend SageMakerAIModel with custom response parsing logic\n",
    "6. **Create Strands agents** - Build lightweight AI agents that use your deployed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, please execute the `prerequisites.sh` script. This script installs [Node.JS](https://nodejs.org/en), [Yeoman](https://yeoman.io/) and the zip utility. It also clones the [ml-container-creator](https://github.com/awslabs/ml-container-creator) utility for deploying large language models to [SageMaker AI](https://aws.amazon.com/sagemaker/ai/).\n",
    "\n",
    "**Remember to delete your endpoint when finished!** Instructions are provided at the end of this notebook.\n",
    "\n",
    "We add the autoreleod extension for ease of use and to prevent kernel restarts on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "./prerequisites.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup\n",
    "\n",
    "In the next cell, we'll set up our SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "print(\"Initializing SageMaker environment...\\n\")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except ValueError:\n",
    "    print(\"‚ö†Ô∏è  Not running in SageMaker environment\")\n",
    "    print(\"   Please set role manually: role = 'arn:aws:iam::ACCOUNT:role/ROLE_NAME'\")\n",
    "    raise\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "# Display summary of configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"Environment Configuration Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Region:          {region}\")\n",
    "print(f\"S3 Bucket:       {bucket}\")\n",
    "print(f\"Execution Role:  {role}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ Environment setup complete! Ready to proceed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Generate Deployment Project\n",
    "\n",
    "### What is ml-container-creator?\n",
    "\n",
    "[ml-container-creator](https://github.com/awslabs/ml-container-creator) is an open-source tool from AWS Labs that automates the creation of SageMaker \"Bring Your Own Container\" (BYOC) deployment projects. It's a Yeoman generator that creates infrastructure code based on your ML framework and serving requirements. Review the [ml-container-creator documentation site](https://awslabs.github.io/ml-container-creator/) for more details on how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "yo --version\n",
    "yo --generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = int(time.time())\n",
    "project_name = f\"llama-31-deployment-{timestamp}\"\n",
    "\n",
    "output_dir = Path(\"./generated_projects\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "project_dir = output_dir / project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_TOKEN=\"<HUGGING_FACE_TOKEN>\"\n",
    "!export AWS_ROLE=$role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash -s project_name region HF_TOKEN\n",
    "!yo ml-container-creator $project_name \\\n",
    "--project-dir=$project_dir \\\n",
    "--framework=transformers \\\n",
    "--model-server=sglang \\\n",
    "--model-name=meta-llama/Llama-3.1-8B-Instruct \\\n",
    "--deploy-target=codebuild \\\n",
    "--region=$region \\\n",
    "--include_testing=true \\\n",
    "--test_types=hosted-model-endpoint \\\n",
    "--hf_token=$HF_TOKEN \\\n",
    "--role_arn=$role \\\n",
    "--skip-install \\\n",
    "--skip-prompts \\\n",
    "--force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Build and Push Container\n",
    "\n",
    "Let's start the build! üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./$project_dir && ./deploy/submit_build.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Deploy to SageMaker\n",
    "\n",
    "### Cost Awareness\n",
    "\n",
    "‚ö†Ô∏è **Important:** Once your endpoint is InService, you will begin to be charged for it!\n",
    "\n",
    "Let's deploy! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./$project_dir/deploy/deploy.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAB ENDPOINT NAME FROM DEPLOY.SH OUTPUT\n",
    "endpoint_name=\"<ENDPOINT_NAME_HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Endpoint\n",
    "1. Non-Streaming Test\n",
    "2. Multi-Turn Chat\n",
    "3. Streaming Response\n",
    "4. Agent Response (this should fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker Runtime client\n",
    "runtime_client = boto3.client('sagemaker-runtime', region_name=region)\n",
    "\n",
    "# Prepare the payload for sglang (OpenAI-compatible format)\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "print(\"Sending request to SageMaker endpoint...\")\n",
    "print(f\"Endpoint: {endpoint_name}\")\n",
    "print(f\"Payload: {json.dumps(payload, indent=2)}\\n\")\n",
    "\n",
    "try:\n",
    "    # Invoke the endpoint\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Accept='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    # Parse the response\n",
    "    response_body = response['Body'].read().decode('utf-8')\n",
    "    result = json.loads(response_body)\n",
    "    \n",
    "    print(\"‚úì Response received successfully!\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Full Response:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(json.dumps(result, indent=2))\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error invoking endpoint: {e}\")\n",
    "    print(f\"\\nError type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the three primary colors?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The three primary colors are red, blue, and yellow.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What happens if you mix the first two?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "print(\"Sending request to SageMaker endpoint...\")\n",
    "print(f\"Endpoint: {endpoint_name}\")\n",
    "print(f\"Payload: {json.dumps(conversation_payload, indent=2)}\\n\")\n",
    "\n",
    "try: \n",
    "    # Multi-turn conversation example    \n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Accept='application/json',\n",
    "        Body=json.dumps(conversation_payload)\n",
    "    )\n",
    "\n",
    "    response_body = response['Body'].read().decode('utf-8')\n",
    "    result = json.loads(response_body)\n",
    "\n",
    "    print(\"‚úì Response received successfully!\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Full Response:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(json.dumps(result, indent=2))\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "           \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error invoking endpoint: {e}\")\n",
    "    print(f\"\\nError type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming inference\n",
    "streaming_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Write a short poem about AI\"}\n",
    "    ],\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.8,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "print(\"Sending request to SageMaker endpoint...\")\n",
    "print(f\"Endpoint: {endpoint_name}\")\n",
    "print(f\"Payload: {json.dumps(streaming_payload, indent=2)}\\n\")\n",
    "\n",
    "try:\n",
    "    response = runtime_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Accept='application/json',\n",
    "        Body=json.dumps(streaming_payload)\n",
    "    )\n",
    "\n",
    "    # Process streaming response\n",
    "    print(\"Streaming response:\")\n",
    "    print(\"=\" * 60)\n",
    "    for event in response['Body']:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode('utf-8')\n",
    "        # Handle empty chunks\n",
    "        if not chunk.strip():\n",
    "            continue\n",
    "            \n",
    "        # Handle multiple JSON objects in the chunk\n",
    "        for line in chunk.split('\\n'):\n",
    "            if line.startswith('data: '):\n",
    "                try:\n",
    "                    json_str = line.replace('data: ', '').strip()\n",
    "                    if json_str:  # Check if there's actual content\n",
    "                        chunk_data = json.loads(json_str)\n",
    "                        if 'choices' in chunk_data and len(chunk_data['choices']) > 0:\n",
    "                            delta = chunk_data['choices'][0].get('delta', {})\n",
    "                            if 'content' in delta:\n",
    "                                print(delta['content'], end='', flush=True)\n",
    "                except json.JSONDecodeError as json_err:\n",
    "                    # Skip malformed JSON without breaking the stream\n",
    "                    continue\n",
    "    print(\"\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n\")\n",
    "    print(\"‚úì Response received successfully!\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error invoking endpoint: {e}\")\n",
    "    print(f\"\\nError type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from strands.agent import Agent\n",
    "from strands.models.sagemaker import SageMakerAIModel\n",
    "\n",
    "provider = SageMakerAIModel(\n",
    "    endpoint_config={\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"region_name\": region,\n",
    "    },\n",
    "    payload_config={\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"llama-assistant\",    \n",
    "    model=provider,\n",
    "    system_prompt=(\n",
    "        \"You are a helpful AI assistant powered by Llama 3.1, \"\n",
    "        \"deployed on Amazon SageMaker. You provide clear, accurate, \"\n",
    "        \"and friendly responses to user questions. When you don't know \"\n",
    "        \"something, you say so honestly rather than making things up.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "test_message = \"Hello! Can you tell me what you are and how you can help me?\"\n",
    "response = agent(test_message)\n",
    "    \n",
    "# Display the response\n",
    "print(\"-\" * 60)\n",
    "print(f\"Agent: {response.content}\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Display metadata about the response\n",
    "print(\"\\nResponse Metadata:\")\n",
    "if hasattr(response, 'usage'):\n",
    "    print(f\"  Tokens Used: {response.usage.get('total_tokens', 'N/A')}\")\n",
    "    print(f\"  Prompt Tokens: {response.usage.get('prompt_tokens', 'N/A')}\")\n",
    "    print(f\"  Completion Tokens: {response.usage.get('completion_tokens', 'N/A')}\")\n",
    "if hasattr(response, 'finish_reason'):\n",
    "    print(f\"  Finish Reason: {response.finish_reason}\")\n",
    "\n",
    "print(\"\\n‚ùå Test 0 failed! The agent should not have successfully responded.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error is a result of the default `SageMakerAIModel` response parser being unable to parse the model's response. We need to use a custom model parser to parse agent responses from this model through Strands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Create Custom Model Provider\n",
    "\n",
    "### How Custom Parsing Works\n",
    "\n",
    "We'll use the `LlamaModelProvider` class that:\n",
    "\n",
    "1. **Extends SageMakerAIModel**: Inherits all the endpoint invocation logic\n",
    "2. **Overrides parse_response()**: Implements custom parsing for sglang format\n",
    "3. **Adds error handling**: Provides meaningful error messages\n",
    "4. **Validates responses**: Checks structure before accessing fields\n",
    "\n",
    "The class is defined in `./code/llama_model_provider.py`\n",
    "\n",
    "### Customer Parser Agent Tests\n",
    "1. Simple Chat\n",
    "2. Multi-Turn Chat\n",
    "3. Complex Reasoning\n",
    "\n",
    "Let's build it! üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), './code')) \n",
    "from llama_model_provider import LlamaModelProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_provider = LlamaModelProvider(\n",
    "    endpoint_name=endpoint_name,  # The SageMaker endpoint to invoke\n",
    "    region_name=region,  # AWS region (from environment setup)\n",
    "    max_tokens=1000,  # Maximum response length (roughly 750 words)\n",
    "    temperature=0.7,  # Creativity level (0.0 = deterministic, 2.0 = very random)\n",
    "    top_p=0.9  # Diversity control (0.9 is a good default)\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"llama-assistant\",\n",
    "    model=llama_provider,\n",
    "    system_prompt=(\n",
    "        \"You are a helpful AI assistant powered by Llama 3.1, \"\n",
    "        \"deployed on Amazon SageMaker. You provide clear, accurate, \"\n",
    "        \"and friendly responses to user questions. When you don't know \"\n",
    "        \"something, you say so honestly rather than making things up.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "test_message = \"Hello! Can you tell me what you are and how you can help me?\"\n",
    "response = agent(test_message)\n",
    "\n",
    "# Display the response\n",
    "print(\"-\" * 60)\n",
    "print(f\"Agent: {response}\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Display metadata about the response\n",
    "print(\"\\nResponse Metadata:\")\n",
    "if hasattr(response, 'usage'):\n",
    "    print(f\"  Tokens Used: {response.usage.get('total_tokens', 'N/A')}\")\n",
    "    print(f\"  Prompt Tokens: {response.usage.get('prompt_tokens', 'N/A')}\")\n",
    "    print(f\"  Completion Tokens: {response.usage.get('completion_tokens', 'N/A')}\")\n",
    "if hasattr(response, 'finish_reason'):\n",
    "    print(f\"  Finish Reason: {response.finish_reason}\")\n",
    "\n",
    "print(\"\\n‚úÖ Test 1 passed! The agent successfully responded.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-turn conversation\n",
    "\n",
    "message1 = \"What are the three primary colors?\"\n",
    "print(f\"User: {message1}\\n\")\n",
    "print(\"‚è≥ Waiting for response...\\n\")\n",
    "\n",
    "response1 = agent(message1)\n",
    "print(\"-\" * 60)\n",
    "print(f\"Agent: {response1}\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Second message: Follow-up that requires context from first message\n",
    "# This tests if the agent remembers what we just talked about\n",
    "message2 = \"Can you mix them to create other colors?\"\n",
    "print(f\"\\nUser: {message2}\\n\")\n",
    "print(\"‚è≥ Waiting for response...\\n\")\n",
    "\n",
    "response2 = agent(message2)\n",
    "print(\"-\" * 60)\n",
    "print(f\"Agent: {response2}\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Third message: Another follow-up\n",
    "message3 = \"What color do you get if you mix the first two you mentioned?\"\n",
    "print(f\"\\nUser: {message3}\\n\")\n",
    "print(\"‚è≥ Waiting for response...\\n\")\n",
    "\n",
    "response3 = agent(message3)\n",
    "print(\"-\" * 60)\n",
    "print(f\"Agent: {response3}\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ Test 2 passed! The agent successfully maintained conversation context.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_prompt = \"\"\"\n",
    "I'm building a Python web application and need to decide between Flask and FastAPI.\n",
    "Can you compare these two frameworks and recommend which one I should use for a\n",
    "REST API that needs to handle high traffic and support async operations?\n",
    "\"\"\".strip()\n",
    "\n",
    "print(f\"User: {complex_prompt}\\n\")\n",
    "print(\"‚è≥ Waiting for response...\\n\")\n",
    "\n",
    "response = agent(complex_prompt)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Agent: {response}\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ Test 3 passed! The agent handled a complex reasoning task.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Cleanup\n",
    "\n",
    "### Cleanup Instructions\n",
    "\n",
    "When you're done with your endpoint, delete it to avoid ongoing charges. Follow these steps to clean up all resources created in this notebook.\n",
    "\n",
    "**What Needs to be Deleted:**\n",
    "\n",
    "1. **SageMaker Endpoint** - The running inference service (costs money!)\n",
    "2. **Endpoint Configuration** - The configuration (no cost, but good practice)\n",
    "3. **SageMaker Model** - The model definition (no cost, but good practice)\n",
    "4. **ECR Image** (Optional) - The container image (minimal storage cost)\n",
    "5. **CodeBuild Project** (Optional) - The build project (no cost when not building)\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "- Deleting the endpoint stops all charges immediately\n",
    "- Deletion is permanent - you'll need to redeploy to use the model again\n",
    "- You can keep the ECR image and CodeBuild project for future deployments\n",
    "- The generated project files remain on disk for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCEED_WITH_DELETION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete SageMaker resources\n",
    "print(\"=\" * 60)\n",
    "print(\"Resource Cleanup\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚ö†Ô∏è  WARNING: This will delete your SageMaker endpoint and stop all charges.\")\n",
    "print(\"You will need to redeploy if you want to use the model again.\\n\")\n",
    "\n",
    "if not PROCEED_WITH_DELETION:\n",
    "    print(\"‚ùå Deletion not confirmed.\")\n",
    "    print(\"\\nTo delete resources, set PROCEED_WITH_DELETION = True in the cell above.\")\n",
    "    print(\"\\nAlternatively, use the AWS CLI commands below for manual deletion.\\n\")\n",
    "else:\n",
    "    print(\"‚úÖ Deletion confirmed. Proceeding with cleanup...\\n\")\n",
    "    \n",
    "    # Create SageMaker client\n",
    "    sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "    \n",
    "    # Step 1: Delete the endpoint\n",
    "    print(\"Step 1: Deleting SageMaker Endpoint...\")\n",
    "    try:\n",
    "        sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"  ‚úì Endpoint '{endpoint_name}' deletion initiated\")\n",
    "        \n",
    "        # Wait for endpoint to be deleted\n",
    "        print(\"  ‚è≥ Waiting for endpoint to be deleted (this may take 2-3 minutes)...\")\n",
    "        waiter = sagemaker_client.get_waiter('endpoint_deleted')\n",
    "        waiter.wait(EndpointName=endpoint_name)\n",
    "        print(\"  ‚úì Endpoint deleted successfully\\n\")\n",
    "        \n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if 'Could not find endpoint' in str(e):\n",
    "            print(f\"  ‚ÑπÔ∏è  Endpoint '{endpoint_name}' not found (may already be deleted)\\n\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Error deleting endpoint: {e}\\n\")\n",
    "    \n",
    "    # Step 2: Delete the endpoint configuration\n",
    "    print(\"Step 2: Deleting Endpoint Configuration...\")\n",
    "    try:\n",
    "        # First, get the config name from the endpoint (if it still exists)\n",
    "        config_name = f\"{endpoint_name}-config\"\n",
    "        sagemaker_client.delete_endpoint_config(EndpointConfigName=config_name)\n",
    "        print(f\"  ‚úì Endpoint configuration '{config_name}' deleted\\n\")\n",
    "        \n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if 'Could not find endpoint configuration' in str(e):\n",
    "            print(f\"  ‚ÑπÔ∏è  Endpoint configuration not found (may already be deleted)\\n\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Error deleting endpoint configuration: {e}\\n\")\n",
    "    \n",
    "    # Step 3: Delete the model\n",
    "    print(\"Step 3: Deleting SageMaker Model...\")\n",
    "    try:\n",
    "        model_name = f\"{endpoint_name}-model\"\n",
    "        sagemaker_client.delete_model(ModelName=model_name)\n",
    "        print(f\"  ‚úì Model '{model_name}' deleted\\n\")\n",
    "        \n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if 'Could not find model' in str(e):\n",
    "            print(f\"  ‚ÑπÔ∏è  Model not found (may already be deleted)\\n\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Error deleting model: {e}\\n\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ Cleanup Complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nResources deleted:\")\n",
    "    print(\"  ‚úì SageMaker Endpoint (charges stopped)\")\n",
    "    print(\"  ‚úì Endpoint Configuration\")\n",
    "    print(\"  ‚úì SageMaker Model\")\n",
    "    print(\"\\nResources retained (optional cleanup):\")\n",
    "    print(\"  ‚Ä¢ ECR container image (minimal storage cost)\")\n",
    "    print(\"  ‚Ä¢ CodeBuild project (no cost when not building)\")\n",
    "    print(\"  ‚Ä¢ Generated project files (local, no cost)\")\n",
    "    print(\"\\nüí° You can redeploy anytime using the generated project files!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
