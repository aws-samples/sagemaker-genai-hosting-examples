{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddeb2c-7877-46e3-9a4e-b2efb0d1b7a4",
   "metadata": {},
   "source": [
    "# How to optimize the Meta Llama-3 70B Amazon JumpStart model for inference using Amazon SageMaker model optimization jobs\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to apply state-of-the-art optimization techniques to an Amazon JumpStart model (JumpStart model ID: `meta-textgeneration-llama-3-70b`) using Amazon SageMaker ahead-of-time (AOT) model optimization capabilities. Each example includes the deployment of the optimized model to an Amazon SageMaker endpoint. In all cases, the inference image will be the SageMaker-managed [LMI (Large Model Inference)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "You will successively:\n",
    "1. Deploy a pre-optimized variant of the Amazon JumpStart model with speculative decoding enabled (using SageMaker provided draft model). For popular models, the JumpStart team indeed selects and applies the best optimization configurations for you.\n",
    "2. Customize the speculative decoding with open-source draft model.\n",
    "3. Quantize the model weights using the AWQ algorithm.\n",
    "4. Compile the model for a deployment of AWS Inferentia 2 accelerated hardware.\n",
    "\n",
    "**Notices:**\n",
    "* Make sure that the `ml.p4d.24xlarge` and `ml.inf2.48xlarge` instance types required for this tutorial are available in your AWS Region.\n",
    "* Make sure that the value of your \"ml.p4d.24xlarge for endpoint usage\" and \"ml.inf2.48xlarge for endpoint usage\" Amazon SageMaker service quotas allow you to deploy at least one Amazon SageMaker endpoint using these instance types.\n",
    "\n",
    "This notebook leverages the [Model Builder Class](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-modelbuilder-creation.html) within the [`sagemaker` Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html) to abstract out container and model server management/tuning. Via the Model Builder Class you can easily interact with JumpStart Models, HuggingFace Hub Models, and also custom models via pointing towards an S3 path with your Model Data. For this sample we will focus on the JumpStart Optimization path.\n",
    "\n",
    "### License agreement\n",
    "* This model is under the Meta license, please refer to the original model card.\n",
    "* This notebook is a sample notebook and not intended for production use.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html#)\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.225.0 \n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a645403-0c3e-4062-9d16-ef0b1041fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker>=2.225.0 boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5631d3-1c16-4ad5-a42c-85a28cf9dd3e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65310881-31a9-453e-9f7b-c79876824cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import huggingface_hub\n",
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b6d0b-5c6c-4014-b836-df0268e11feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "\n",
    "artifacts_bucket_name = sagemaker_session.default_bucket()\n",
    "execution_role_arn = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "js_model_id = \"meta-textgeneration-llama-3-70b\"\n",
    "gpu_instance_type = \"ml.p4d.24xlarge\"\n",
    "neuron_instance_type = \"ml.inf2.48xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb26871-315d-425f-a5d2-b05627191700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = \"Hello, I'm a language model, and I'm here to help you with your English.\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": \"Hello, I'm a language model,\",\n",
    "    \"parameters\": {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "\n",
    "sample_output = [{\"generated_text\": response}]\n",
    "\n",
    "schema_builder = SchemaBuilder(sample_input, sample_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81838b96-ca02-4892-b861-8cb0634fb167",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Deploy a pre-optimized deployment configuration with speculative decoding (SageMaker provided draft model)\n",
    "The `meta-textgeneration-llama-3-70b` JumpStart model is available with multiple pre-optimized deployment configuration. Optimized model artifacts for each configuration have already been created by the JumpStart team and a readily available for deployment. In this section, you will deploy one of theses pre-optimized configuration to an Amazon SageMaker endpoint. \n",
    "\n",
    "### What is speculative decoding?\n",
    "Speculative decoding is an inference optimization technique introduced by [Y. Leviathan et al. (ICML 2023)](https://arxiv.org/abs/2211.17192) used to accelerate the decoding process of large and therefore slow LLMs for latency-critical applications. The key idea is to use a smaller, less powerful but faster model called the ***draft model*** to generate candidate tokens that get validated by the larger, more powerful but slower model called the ***target model***. At each iteration, the draft model generates $K>1$ candidate tokens. Then, using a single forward pass of the larger target model, none, part, or all candidate tokens get accepted. The more aligned the selected draft model is with the target model, the better guesses it makes, the higher candidate token acceptance rate and therefore the higher the speed ups. The larger the size gap between the target and the draft model, the largest the potential speedups.\n",
    "\n",
    "Let's start by creating a `ModelBuilder` instance for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490309f-da98-4489-8bab-d1ffde767370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294805b8-f056-4a24-9d10-df32039d1193",
   "metadata": {},
   "source": [
    "For each optimization configuration, the JumpStart team has computed key performance metrics such as time-to-first-token (TTFT) latency and throughput for multiple hardwares and concurrent invocation intensities. Let's visualize these metrics using the `display_benchmark_metrics` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eedd83-3104-4820-bb5c-6164b9d0477f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder.display_benchmark_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f84c42-d578-4136-9e86-34058e134707",
   "metadata": {},
   "source": [
    "Now, let's pick and deploy the `lmi-optimized` pre-optimized configuration to a `ml.p4d.24xlarge` instance. The `lmi-optimized` configuration enables speculative decoding. In this configuration, a SageMaker provided draft model is used. Therefore, you don't have to supply a draft model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bb346-4108-4721-86bc-c754d59c4bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder.set_deployment_config(config_name=\"lmi-optimized\", instance_type=gpu_instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35232c11-eb78-4521-ae60-243bdc7e1666",
   "metadata": {},
   "source": [
    "Currently set deployment configuration can be visualized using the `get_deployment_config` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e44a7-cce1-4417-b701-6895638fa42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder.get_deployment_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad37a7b-e2cc-45af-bf27-ea6c1aa258ba",
   "metadata": {},
   "source": [
    "Now, let's build the `Model` instance and use it to deploy the selected optimized configuration. This operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972aea37-4a16-4eee-b3ad-33b1077cbcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_model = model_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a2527-a189-4b26-b579-bae031f69248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = optimized_model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7fb08-3ff0-40c5-8a4e-9a026d7fba62",
   "metadata": {},
   "source": [
    "Once the deployment has finished successfully, you can send queries to the model by simply using the predictor's `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8249e-9e8a-48d1-9dd2-2a31d664d954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984819c-e3ec-47d9-92a8-d91fa4998b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4750b-de17-4764-a93e-bba011b84613",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Customize the speculative decoding with open-source draft model, then deploy the optimized model\n",
    "In this section and instead of relying on a pre-optimized model, you will use an Amazon SageMaker optimization toolkit to enable speculative decoding on the `meta-textgeneration-llama-3-70b` JumpStart model. In this example, the draft model is from HuggingFace model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6b5db-5c86-4b57-a977-503232f9b6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb872c31-e77e-4c15-8c79-ce3812b39f9e",
   "metadata": {},
   "source": [
    "The optimization operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b503949-3ff5-4516-9f46-125ed1de652c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_model = model_builder.optimize(\n",
    "    instance_type=gpu_instance_type,\n",
    "    speculative_decoding_config={\"ModelSource\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"},\n",
    "    env_vars={\"HF_TOKEN\": \"<replace-with-hf-token>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b559d-49e7-4b96-8760-db3439fb325c",
   "metadata": {},
   "source": [
    "Now let's deploy the quantized model to an Amazon SageMaker endpoint. This operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e9a77-e7bc-4690-8daf-d77685116dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = optimized_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4edf3-11d1-4861-99d5-44667d70d061",
   "metadata": {},
   "source": [
    "Once the deployment has finished successfully, you can send queries to the model by simply using the predictor's `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c198a59-c6a6-4ec6-ad49-f838798d7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61ec19-05f2-47fe-a0ab-e45bf3cdf457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ff522-c231-4f52-ac36-0a56e1e446d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Run optimization job to quantize the model using AWQ, then deploy the quantized model\n",
    "In this section, you will quantize the `meta-textgeneration-llama-3-70b` JumpStart model with the AWQ quantization algorithm by running an Amazon SageMaker optimization job. \n",
    "\n",
    "### What is quantization?\n",
    "In our particular context, quantization means casting the weights of a pre-trained LLM to a data type with a lower number of bits and therefore a smaller memory footprint. The benefits of LLM quantization include:\n",
    "* Reduced hardware requirements for model serving: A quantized model can be served using less expensive and more available GPUs or even made accessible on consumer devices or mobile platforms.\n",
    "* Increased space for the KV cache to enable larger batch sizes and/or sequence lengths.\n",
    "* Faster decoding latency. As the decoding process is memory bandwidth bound, less data movement from reduced weight sizes directly improves decoding latency, unless offset by dequantization overhead.\n",
    "* A higher compute-to-memory access ratio (through reduced data movement), known as arithmetic intensity. This allows for fuller utilization of available compute resources during decoding.  \n",
    "\n",
    "AWQ (Activation-aware Weight Quantization) is a post-training weight-only quantization algorithm introduced by [J. Lin et al. (MLSys 2024)](https://arxiv.org/abs/2306.00978) that allows to quantize LLMs to low-bit integer types like INT4 with virtually no loss in model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded24c7d-5d4c-4a9d-acfb-3accb4e544d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235b250-fda4-4c4a-b149-d596ed7d706e",
   "metadata": {},
   "source": [
    "Quantizing the model is as easy as supplying the following inputs:\n",
    "* The location of the unquantized model artifacts, here the Amazon SageMaker JumpStart model ID.\n",
    "* The quantization configuration.\n",
    "* The Amazon S3 URI where the output quantized artifacts needs to be stored.\n",
    "Everything else (compute provisioning and configuration for example) is managed by SageMaker. This operation takes around 120min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379f0d8-e8e4-42d0-99ec-994b9c53eba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_model = model_builder.optimize(\n",
    "    instance_type=gpu_instance_type,\n",
    "    accept_eula=True,\n",
    "    quantization_config={\n",
    "        \"OverrideEnvironment\": {\n",
    "            \"OPTION_QUANTIZE\": \"awq\",\n",
    "        },\n",
    "    },\n",
    "    output_path=f\"s3://{artifacts_bucket_name}/awq-quantization/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb119673-840e-473d-b44a-1e0b7fb61394",
   "metadata": {},
   "source": [
    "Now let's deploy the quantized model to an Amazon SageMaker endpoint. This operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c8e0d-be1b-498c-b6c2-19bd35fd45c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quantized_instance_type = \"ml.g5.12xlarge\"  # We can use a smaller instance type once quantized\n",
    "predictor = optimized_model.deploy(instance_type=quantized_instance_type, accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab86b58-a250-48df-a024-119aa4290b05",
   "metadata": {},
   "source": [
    "Once the deployment has finished successfully, you can send queries to the model by simply using the predictor's `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1cd1c5-0bba-4bd2-b489-6d1fdae8388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e73288-312e-48ee-ae87-aa18e2a28bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4550542-4e10-452f-bca7-a1f1a24a28f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Run optimization job to compile the model using the AWS Neuron Compiler then deploy the compiled model to an Inferentia 2 endpoint\n",
    "In this section, you will use an Amazon SageMaker optimization job as a managed model compiler to compile the `meta-textgeneration-llama-3-70b` JumpStart model for [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/) hardware. The optimization job allows you to decouple compilation from deployment. The model is compiled once while the compiled artifacts can be deployed many times. In other words, the compilation overhead is paid once instead of occuring upon each deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75309273-2780-44c7-acfe-4879029e00cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d28124-6dc0-4643-9472-86cc7ab75d05",
   "metadata": {},
   "source": [
    "Now let's compile the model for Inferentia 2. This operation takes around 40min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26e543-6951-40dd-801a-93984e3ffaf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_model = model_builder.optimize(\n",
    "    instance_type=neuron_instance_type,\n",
    "    accept_eula=True,\n",
    "    compilation_config={\n",
    "        \"OverrideEnvironment\": {\n",
    "            \"OPTION_TENSOR_PARALLEL_DEGREE\": \"24\",\n",
    "            \"OPTION_N_POSITIONS\": \"8192\",\n",
    "            \"OPTION_DTYPE\": \"fp16\",\n",
    "            \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "            \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"4\",\n",
    "            \"OPTION_NEURON_OPTIMIZE_LEVEL\": \"2\",\n",
    "        }\n",
    "    },\n",
    "    output_path=f\"s3://{artifacts_bucket_name}/neuron/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811e8b3-f145-4d08-85ee-3a8d797d5ae6",
   "metadata": {},
   "source": [
    "Now let's deploy the compiled model to an Amazon SageMaker endpoint powered by Inferentia 2 hardware. This operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c8835-081d-457f-aabd-53dc948df0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = optimized_model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221bcb2-b16c-4c2b-8e63-9f684ca1e9e0",
   "metadata": {},
   "source": [
    "Once the deployment has finished successfully, you can send queries to the model by simply using the predictor's `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c939d2-2b37-4377-92bf-d54e2b3bb467",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23da2a4-b113-44d4-9a59-cbfdeff7a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
