{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946d5a3d",
   "metadata": {},
   "source": [
    "#  Serve Multiple Fine-Tuned LoRA Adapters with DJL Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35756ca1",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how you can deploy multiple fine-tuned LoRA adapters with a single base model copy on SageMaker using the DJL Serving Large Model Inference DLC. LoRA (Low Rank Adapters) is a powerful technique for fine-tuning large language models. This technique significantly reduces the number of trainable parameters compared to traditional fine-tuning while achieving comparable or superior performance. You can learn more about the LoRA technique in this [paper](https://arxiv.org/abs/2106.09685).\n",
    "\n",
    "A major benefit of LoRA is that the fine-tuned adapters can easily be added to and removed from the base model, which makes switching adapters pretty cheap and viable at runtime. In this notebook we will show how you can deploy a SageMaker endpoint with a single base model and multiple LoRA adapters, and change adapters for different requests.\n",
    "\n",
    "Since LoRA adapters are much smaller than the size of a base model (can realistically be 100x-1000x smaller), we can deploy an endpoint with a single base model and multiple LoRA adapters using much less hardware than deploying an equivalent number of fully fine-tuned models.\n",
    "\n",
    "The example we will work through in this notebook is guided by the multi adapter example in HuggingFace's PEFT library: https://github.com/huggingface/peft/blob/main/examples/multi_adapter_examples/PEFT_Multi_LoRA_Inference.ipynb\n",
    "\n",
    "#### Can we teach a new language to the Large Model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e3b6d",
   "metadata": {},
   "source": [
    "## Install Packages and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a4ff2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub sagemaker boto3 awscli --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12eb06ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sagemaker.utils import name_from_base\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37b203",
   "metadata": {},
   "source": [
    "## Download Model Artifacts and Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1515c5",
   "metadata": {},
   "source": [
    "We will be deploying an endpoint with 2 LoRA adapters. These are the models we will be using:\n",
    "- Base Model: https://huggingface.co/huggyllama/llama-7b\n",
    "- LoRA Fine Tuned Adapter 1: https://huggingface.co/tloen/alpaca-lora-7b\n",
    "- LoRA Fine Tuned Adapter 2: https://huggingface.co/22h/cabrita-lora-v0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d57e9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf lora-multi-adapter\n",
    "!mkdir -p lora-multi-adapter/adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978224f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41114d64635a4d5fadab32d811d7c4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf135d46bef145eda5fa70f0a98aee62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e15bde4e2d40fb94a63976ce11a59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b668cd2deb34f5b88f304a4bd5d11ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/823 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b06225d24ec4b4fa7480a8776a5eb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/67.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/djl-demo/aws/sagemaker/large-model-inference/sample-llm/lora-multi-adapter/adapters/eng_alpaca'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(\"tloen/alpaca-lora-7b\", local_dir=\"lora-multi-adapter/adapters/eng_alpaca\", local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba747f00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f274a6dd42d24af89d7bd6a6ed8c9ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680745ea660b44b298497058ba63453d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952bbe4515f6402cb9ad651404791925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/370 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4b828a39cc49829877fd6306987d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852086b779b04c338726bdce1cf606dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/djl-demo/aws/sagemaker/large-model-inference/sample-llm/lora-multi-adapter/adapters/portuguese_alpaca'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(\"22h/cabrita-lora-v0-1\", local_dir=\"lora-multi-adapter/adapters/portuguese_alpaca\", local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89777de",
   "metadata": {},
   "source": [
    "## Creating Inference Handler and DJL Serving Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd1577",
   "metadata": {},
   "source": [
    "The following files cover the model server configuration (`serving.properties`) and custom inference handler (`model.py`). The custom inference handler is optional and if not specified, default handler from djl-serving will be used. This configuration can be used as an example to write your own inference handler for different models. \n",
    "\n",
    "The core structure to cover here is the model directory. We include both the base model and LoRA adapters in the model directory like this:\n",
    "\n",
    "```\n",
    "|- model_dir\n",
    "    |- adapters/\n",
    "        |--- <adapter_1>/\n",
    "        |--- <adapter_2>/\n",
    "        |--- ...\n",
    "        |--- <adapter_n>/\n",
    "    |- serving.properties\n",
    "    |- model.py (optional)\n",
    "\n",
    "```\n",
    "\n",
    "Each of the adapters in the `adapters` directory contains the LoRA adapter artifacts. Typically there are two files: `adapter_model.bin` and `adapter_config.json` which are the adapter weights and adapter configuration respectively. These are typically obtained from the Peft library via the `PeftModel.save_pretrained()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e988852",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lora-multi-adapter/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile lora-multi-adapter/serving.properties\n",
    "engine=Python\n",
    "option.model_id=huggyllama/llama-7b\n",
    "option.adapters=adapters\n",
    "option.dtype=fp16\n",
    "option.entryPoint=model.py\n",
    "option.tensor_parallel_degree=1\n",
    "load_on_devices=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4032f47e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lora-multi-adapter/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lora-multi-adapter/model.py\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "from djl_python.inputs import Input\n",
    "from djl_python.outputs import Output\n",
    "import logging\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "        Write a response that appropriately completes the request. ### Instruction: {instruction} ### Input: {input} \n",
    "        ### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the \n",
    "        request.### Instruction: {instruction} ### Response:\"\"\"\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "        instruction,\n",
    "        adapters,\n",
    "        input=None,\n",
    "        max_new_tokens=64,\n",
    "        **kwargs,\n",
    "):\n",
    "    prompts = []\n",
    "    for inp in instruction:\n",
    "        prompts.append(generate_prompt(inp, input))\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(torch.cuda.current_device())\n",
    "    attention_mask = inputs[\"attention_mask\"].to(torch.cuda.current_device())\n",
    "    generation_config = GenerationConfig(num_beams=1, do_sample=False)\n",
    "\n",
    "    logging.info(f\"using adapters: {adapters}\")\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            adapters=adapters,\n",
    "            generation_config=generation_config,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    output = tokenizer.batch_decode(generation_output, skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_model(model_id):\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = '[PAD]'\n",
    "    logging.info(f\"Loaded Base Model {model_id}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def register_adapter(inputs: Input):\n",
    "    \"\"\"\n",
    "    Registers lora adapter with the model.\n",
    "    \"\"\"\n",
    "    global model\n",
    "    adapter_name = inputs.get_property(\"name\")\n",
    "    adapter_model_id_or_path = inputs.get_property(\"src\")\n",
    "    logging.info(\n",
    "        f\"Registering adapter {adapter_name} from {adapter_model_id_or_path}\")\n",
    "    if isinstance(model, PeftModel):\n",
    "        model.load_adapter(adapter_model_id_or_path, adapter_name)\n",
    "    else:\n",
    "        model = PeftModel.from_pretrained(model,\n",
    "                                           adapter_model_id_or_path,\n",
    "                                           adapter_name)\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        properties = inputs.get_properties()\n",
    "        model_id = properties.get(\"model_id\")\n",
    "        model, tokenizer = load_model(model_id)\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "\n",
    "\n",
    "    json_inputs = inputs.get_as_json()\n",
    "    sentence = json_inputs.get(\"inputs\")\n",
    "    adapters = json_inputs.get(\"adapters\", [])\n",
    "    generation_kwargs = json_inputs.get(\"parameters\", {})\n",
    "    outputs = evaluate(sentence, adapters, **generation_kwargs)\n",
    "\n",
    "    return Output().add_as_json(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c45680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./model.py\n",
      "./serving.properties\n",
      "./adapters/\n",
      "./adapters/portuguese_alpaca/\n",
      "./adapters/portuguese_alpaca/README.md\n",
      "./adapters/portuguese_alpaca/adapter_model.bin\n",
      "./adapters/portuguese_alpaca/adapter_config.json\n",
      "./adapters/portuguese_alpaca/.gitattributes\n",
      "./adapters/eng_alpaca/\n",
      "./adapters/eng_alpaca/README.md\n",
      "./adapters/eng_alpaca/adapter_model.bin\n",
      "./adapters/eng_alpaca/adapter_config.json\n",
      "./adapters/eng_alpaca/.gitattributes\n"
     ]
    }
   ],
   "source": [
    "!rm -f model.tar.gz\n",
    "!rm -rf lora-multi-adapter/.ipynb_checkpoints\n",
    "!tar czvf model.tar.gz -C lora-multi-adapter ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6dca13",
   "metadata": {},
   "source": [
    "## Create SageMaker Model and Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cc6e59a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#role = \"arn:aws:iam::125045733377:role/AmazonSageMaker-ExecutionRole-djl\"  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = \"hf-large-model-djl/g5-lora-multi-adapter\"  # folder within bucket where code artifact will go\n",
    "\n",
    "role = sagemaker.get_execution_role() \n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab57364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact_accelerate = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f382a9b9-999f-4331-904e-2214a829f798",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-622343165275/hf-large-model-djl/g5-lora-multi-adapter/model.tar.gz'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_code_artifact_accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e67228",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not working image --- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121::\n",
      "finally the image --- > 622343165275.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121::\n",
      "g5-lora-multi-adapter-2024-03-20-17-55-22-611\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=region,\n",
    "        version=\"0.26.0\"\n",
    "    )\n",
    "\n",
    "model_name_acc = name_from_base(f\"g5-lora-multi-adapter\")\n",
    "\n",
    "print(f\"Not working image --- > {inference_image_uri}::\")\n",
    "inference_image_uri='622343165275.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121'\n",
    "print(f\"finally the image --- > {inference_image_uri}::\")\n",
    "model_name_acc = name_from_base(f\"g5-lora-multi-adapter\")\n",
    "print(model_name_acc)\n",
    "# LoRA Adapters feature is \n",
    "\n",
    "# LoRA Adapters feature is a preview feature and ENABLE_ADAPTERS_PREVIEW environmnet variable should be set to use it\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name_acc,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri,\n",
    "                      \"ModelDataUrl\": s3_code_artifact_accelerate,\n",
    "                      \"Environment\": {\"ENABLE_ADAPTERS_PREVIEW\": \"true\"},\n",
    "                     })\n",
    "model_arn = create_model_response[\"ModelArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adc57f77-7aed-4db5-88da-1ad0cab067ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:us-east-1:622343165275:model/g5-lora-multi-adapter-2024-03-20-17-55-22-611'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8699f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name_acc}-config\"\n",
    "endpoint_name = f\"{model_name_acc}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name_acc,\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 1800,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 1800,\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a1585b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: g5-lora-multi-adapter-2024-03-20-17-55-22-611-endpoint\n"
     ]
    }
   ],
   "source": [
    "print(f\"endpoint_name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a346666f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e6ad681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:622343165275:endpoint/g5-lora-multi-adapter-2024-03-20-17-55-22-611-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680b38a",
   "metadata": {},
   "source": [
    "## Make Inference Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84948200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 ms, sys: 0 ns, total: 18.5 ms\n",
      "Wall time: 5.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[\\n  \"Below is an instruction that describes a task. Write a response that appropriately completes the \\\\n        request.### Instruction: Tell me about Alpacas ### Response: that Alpacas are\\\\n        small, cute, and fluffy animals native to South America. They are related to\\\\n        camels and llamas, and are prized for their soft, luxurious fleece. Alpacas are\\\\n        raised for their fleece,\",\\n  \"Below is an instruction that describes a task. Write a response that appropriately completes the \\\\n        request.### Instruction: Invente uma desculpa criativa pra dizer que não preciso ir à festa. ### Response: Eu não posso ir à festa porque tenho que cuidar de minha avó. Eu tenho que cuidar dela porque ela está doente e não pode ir. Eu tenho que cuidar dela porque ela está doente e não pode ir. Eu tenho que cu\",\\n  \"Below is an instruction that describes a task. Write a response that appropriately completes the \\\\n        request.### Instruction: Tell me about AWS ### Response: that Amazon Web Services (AWS) is a cloud computing platform that provides a wide range of services, including computing power, storage, databases, analytics, and more. AWS is used by businesses of all sizes to build and deploy applications, websites, and other services.\"\\n]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"inputs\": [\"Tell me about Alpacas\", \"Invente uma desculpa criativa pra dizer que não preciso ir à festa.\", \"Tell me about AWS\"],\n",
    "                     \"adapters\": [\"eng_alpaca\", \"portuguese_alpaca\", \"eng_alpaca\"]}),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32e35f3c-c19b-4b73-8456-5e0e9725ccf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 ms, sys: 478 µs, total: 19.4 ms\n",
      "Wall time: 4.44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[\\n  \"Below is an instruction that describes a task. Write a response that appropriately completes the \\\\n        request.### Instruction: Tell me about Alpacas ### Response: that Alpacas are\\\\n        small, cute, and fluffy animals native to South America. They are related to\\\\n        camels and llamas, and are prized for their soft, luxurious fleece. Alpacas are\\\\n        raised for their fleece,\",\\n  \"Below is an instruction that describes a task. Write a response that appropriately completes the \\\\n        request.### Instruction: Invente uma desculpa criativa ### Response: Eu tenho um \\\\n        problema urgente que preciso resolver. Por favor, me ajude a resolvê-lo. ###\\\\n\\\\n    Resposta: Eu tenho um problema urgente que preciso resolver. Por favor, me ajude a resolvê-lo.\"\\n]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"inputs\": [\"Tell me about Alpacas\",  \"Invente uma desculpa criativa\"],\n",
    "                     \"adapters\": [\"eng_alpaca\", \"portuguese_alpaca\"]}),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d434c23",
   "metadata": {},
   "source": [
    "Inference Request targetting the base model without any adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef62f3e",
   "metadata": {},
   "source": [
    "## Clean up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85474bc1-c4e1-4a14-af41-fa96fb55792e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a077aabe-879a-4ff1-8c61-3faf37b3066d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-neuronx-sdk2.16.0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_uris.retrieve(\"djl-neuronx\", region=region, version=\"0.26.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9c6bd-1436-420f-8fce-5eb64da3709a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
