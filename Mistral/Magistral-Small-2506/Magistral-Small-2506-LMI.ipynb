{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddeb2c-7877-46e3-9a4e-b2efb0d1b7a4",
   "metadata": {},
   "source": [
    "# How to deploy Magistral-Small-2506 for inference on Amazon SageMakerAI\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to deploy the Magistral-Small-2506 model (HuggingFace model ID: [mistralai/Magistral-Small-2506](https://huggingface.co/mistralai/Magistral-Small-2506)) using Amazon SageMaker AI. The inference image will be the SageMaker-managed [LMI (Large Model Inference) v15](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "Building upon Mistral Small 3.1 (2503), with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.\n",
    "\n",
    "Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.\n",
    "\n",
    "Learn more about Magistral in Mistral AI's [blog post](https://mistral.ai/news/magistral/).\n",
    "\n",
    "### Key Features\n",
    "- **Reasoning**: Capable of long chains of reasoning traces before providing an answer.\n",
    "\n",
    "- **Multilingual**: Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.\n",
    "\n",
    "- **Apache 2.0 License**: Open license allowing usage and modification for both commercial and non-commercial purposes.\n",
    "\n",
    "- **Context Window**: A 128k context window, but performance might degrade past 40k. Hence we recommend setting the maximum model length to 40k.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.246.0\n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a645403-0c3e-4062-9d16-ef0b1041fbe3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5631d3-1c16-4ad5-a42c-85a28cf9dd3e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65310881-31a9-453e-9f7b-c79876824cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import logging\n",
    "import time\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83937110-ffc0-4c42-b67d-0021b829f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    boto_region = boto3.Session().region_name\n",
    "    sagemaker_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "    role = sagemaker.get_execution_role()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3035e-f732-4429-a7a5-89bf8f822750",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODEL_ID = \"mistralai/Magistral-Small-2506\"\n",
    "\n",
    "base_name = HF_MODEL_ID.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = HF_MODEL_ID.split(\"/\")[0]\n",
    "base_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf3361-420d-48ab-841f-65580a9c66b5",
   "metadata": {},
   "source": [
    "## Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "If you are deploying a model hosted on the HuggingFace Hub, you must specify the `option.model_id=<hf_hub_model_id>` configuration. When using a model directly from the hub, we recommend you also specify the model revision (commit hash or branch) via `option.revision=<commit hash/branch>`. \n",
    "\n",
    "Since model artifacts are downloaded at runtime from the Hub, using a specific revision ensures you are using a model compatible with package versions in the runtime environment. Open Source model artifacts on the hub are subject to change at any time. These changes may cause issues when instantiating the model (updated model artifacts may require a newer version of a dependency than what is bundled in the container). If a model provides custom model (modeling.py) and/or custom tokenizer (tokenizer.py) files, you need to specify option.trust_remote_code=true to load and use the model.\n",
    "\n",
    "In this example, we will demonstrate how to download your copy of the model from huggingface and upload it to an s3 location in your AWS account, then deploy the model with the downloaded model artifacts to an endpoint.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e550b27c-4f7b-4eb2-bb30-1728ce7f2525",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    "> **Store Models in Your Own S3 Bucket**\n",
    "For production use-cases, always download and store model files in your own S3 bucket to ensure validated artifacts. This provides verified provenance, improved access control, consistent availability, protection against upstream changes, and compliance with organizational security protocols.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c378b4-968e-466d-94d4-e02e59a8716e",
   "metadata": {},
   "source": [
    "First, download the model artifact data from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ba009-7ba1-4bf5-bec2-6b937c087ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sagemaker\n",
    "import jinja2\n",
    "\n",
    "magistral_small_2506 = \"mistralai/Magistral-Small-2506\"\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\".\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = magistral_small_2506\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.safetensors\", \"*.bin\", \"*.txt\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b30454-df95-4caa-bfbc-b10afc2ae99f",
   "metadata": {},
   "source": [
    "### Upload model files to S3\n",
    "SageMaker AI allows us to provide [uncompressed](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html) files. Thus, we directly upload the folder that contains model files to s3\n",
    "> **Note**: The default SageMaker bucket follows the naming pattern: `sagemaker-{region}-{account-id}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb99ce7-2b39-467a-b356-00250f7f2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_prefix = (\n",
    "    \"hf-large-models/magistral-small-2506\"  # folder within bucket where model artifact will go\n",
    ")\n",
    "\n",
    "model_artifact = sagemaker_session.upload_data(path=model_download_path, key_prefix=s3_model_prefix)\n",
    "print(f\"Model uploaded to --- > {model_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5d428-e250-47e8-b751-c48f38fd6b55",
   "metadata": {},
   "source": [
    "### Configure Model Serving Properties\n",
    "\n",
    "Now we'll create a `serving.properties` file that configures how the model will be served. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753dfbe-803b-478a-8dd7-97c8928eaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory that will contain the configuration files\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('config-magistral-small-2506')\n",
    "model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ae607-c0a6-46f7-9ca1-df71893f6ed2",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    ">**Separate Configuration from Model Artifacts**\n",
    "> The LMI container supports separating configuration files from model artifacts. While you can store serving.properties with your model files, placing configurations in a distinct S3 location allows for better management of all your configurations files.\n",
    ">\n",
    "> **Note**: When your model and configuration files are in different S3 locations, set `option.model_id=<s3_model_uri>` in your serving.properties file, where `s3_model_uri` is the S3 object prefix containing your model artifacts. SageMaker AI will automatically download the model files by looking at the S3URI in model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb092ff-a52e-442f-890b-c7c6a7e3d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = f\"\"\"engine=Python\n",
    "option.async_mode=true\n",
    "option.rolling_batch=disable\n",
    "option.entryPoint=djl_python.lmi_vllm.vllm_async_service\n",
    "option.model_loading_timeout=1500\n",
    "fail_fast=true\n",
    "option.max_rolling_batch_size=8\n",
    "option.trust_remote_code=false\n",
    "option.model_id={model_artifact}\n",
    "option.tool_call_parser=mistral\n",
    "option.enable_auto_tool_choice=true\n",
    "option.tokenizer_mode=mistral\n",
    "option.config_format=mistral\n",
    "option.load_format=mistral\n",
    "\"\"\"\n",
    "with open(\"config-magistral-small-2506/serving.properties\", \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0094ec7a-5409-4ba8-8b23-b99c4d4a3bae",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    "> **Store Models in Your Own S3 Bucket**\n",
    "For production use-cases, always download and store model files in your own S3 bucket to ensure validated artifacts. This provides verified provenance, improved access control, consistent availability, protection against upstream changes, and compliance with organizational security protocols.\n",
    ">\n",
    ">**Separate Configuration from Model Artifacts**\n",
    "> The LMI container supports separating configuration files from model artifacts. While you can store serving.properties with your model files, placing configurations in a distinct S3 location allows for better management of all your configurations files.\n",
    ">\n",
    "> When your model and configuration files are in different S3 locations, set `option.model_id=<s3_model_uri>` in your serving.properties file, where `s3_model_uri` is the S3 object prefix containing your model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff13de0-159d-4d77-95bd-362735c2ef08",
   "metadata": {},
   "source": [
    "#### Optional configuration files\n",
    "\n",
    "(Optional) You can also specify a `requirements.txt` to install additional libraries.\n",
    "We update vllm to version vllm==0.8.5 for magistral support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff5d49-e3be-4d9e-bb18-698aced5ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config-magistral-small-2506/requirements.txt\n",
    "vllm==0.8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dd00a-d351-4825-a8e4-6e7629e1c1fc",
   "metadata": {},
   "source": [
    "### Upload config files to S3\n",
    "Here we will upload our config files to a different path to keep model files and config separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34b933-26ad-4017-9cda-a4450d90f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "sagemaker_default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "config_files_uri = S3Uploader.upload(\n",
    "    local_path=\"config-magistral-small-2506\",\n",
    "    desired_s3_uri=f\"s3://{sagemaker_default_bucket}/lmi/{base_name}/config-files\"\n",
    ")\n",
    "\n",
    "print(f\"code_model_uri: {config_files_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78d08f-407c-4c57-aa61-172fc28729f0",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Magistral-Small-2506, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container**: A container optimized for large language model inference\n",
    "- **[G6e Instance](https://aws.amazon.com/ec2/instance-types/g6e/)**: AWS's GPU instance type powered by NVIDIA L40S Tensor Core GPUs \n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.g6e.12xlarge` instance\n",
    "> **Note**: The region in the container URI should match your AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a3759-4cce-4a69-9f77-68251aabbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_instance_type = \"ml.g6e.12xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c013786-ac4e-4213-b4a0-29c851077aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_VERSION = '0.33.0-lmi15.0.0-cu128'\n",
    "image_uri = \"763104351884.dkr.ecr.{}.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\".format(sagemaker_session.boto_session.region_name)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4104f5e-883a-4ab3-a82e-93b3b85b43f4",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Model Data** (`model_data`): points to our configuration files in S3\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n",
    "\n",
    "### Required Permissions\n",
    "The IAM role needs:\n",
    "- S3 read access for model artifacts\n",
    "- CloudWatch permissions for logging\n",
    "- ECR permissions to pull the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4ca9e-142d-41cb-82a8-15820c7232e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 URI for your uncompressed config files\n",
    "model_data = {\n",
    "    \"S3DataSource\": {\n",
    "        \"S3Uri\": f\"{config_files_uri}/\",\n",
    "        \"S3DataType\": \"S3Prefix\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef2bd0-5ae5-490c-bff0-292536c7cb93",
   "metadata": {},
   "source": [
    "> **Note**: Here S3 URI points to the configuration files S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527937f2-43e9-428a-b201-ce299894390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = name_from_base(base_name, short=True)\n",
    "\n",
    "# Create model\n",
    "magistral_small_model = Model(\n",
    "    name = model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,  # Path to uncompressed config code files\n",
    "    role=role,\n",
    "    env={\n",
    "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fe9cf-a47c-4406-acbd-fd335ac08253",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (G6e instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.g6e.12xlarge` for high-performance inference\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment can take up to 15 minutes\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb2f42-4790-4c12-850e-b7482c05be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(base_name, short=True)\n",
    "\n",
    "magistral_small_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=gpu_instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83950a-ff8d-4344-add3-6c88b48b8d36",
   "metadata": {},
   "source": [
    "### Use the code below to create a predictor from an existing endpoint and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66831e-dc02-487f-a253-5caa915a98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "endpoint_name = \"magistral-small-2506-250612-1355\"# replace with your enpoint name \n",
    "\n",
    "magistral_small_predictor = Predictor(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc088b5-0681-411b-9e1d-650736b34723",
   "metadata": {},
   "source": [
    "#### Use predictor to make inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a837d9c1-3996-40c6-b64e-b0e02852bbde",
   "metadata": {},
   "source": [
    "It is highly recommend including the default system prompt used during RL for the best results, you can edit and customise it if needed for your specific use case.\n",
    "\n",
    "```\n",
    "<s>[SYSTEM_PROMPT]system_prompt\n",
    "\n",
    "A user will ask you to solve a task. You should first draft your thinking process (inner monologue) until you have derived the final answer. Afterwards, write a self-contained summary of your thoughts (i.e. your summary should be succinct but contain all the critical steps you needed to reach the conclusion). You should use Markdown to format your response. Write both your thoughts and summary in the same language as the task posed by the user. NEVER use \\boxed{} in your response.\n",
    "\n",
    "Your thinking process must follow the template below:\n",
    "<think>\n",
    "Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate a correct answer.\n",
    "</think>\n",
    "\n",
    "Here, provide a concise summary that reflects your reasoning and presents a clear final answer to the user. Don't mention that this is a summary.\n",
    "\n",
    "Problem:\n",
    "\n",
    "[/SYSTEM_PROMPT][INST]user_message[/INST]<think>\n",
    "reasoning_traces\n",
    "</think>\n",
    "assistant_response</s>[INST]user_message[/INST]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a8c418-1ba4-4bbb-a641-7326d66c90bd",
   "metadata": {},
   "source": [
    "We can just download the prompt template from huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d77531-07b2-4278-9470-8bc1969c43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Retrieve the prompt template from huggingface_hub\n",
    "def load_system_prompt(repo_id: str, filename: str) -> str:\n",
    "    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    with open(file_path, \"r\") as file:\n",
    "        system_prompt = file.read()\n",
    "    return system_prompt\n",
    "\n",
    "SYSTEM_PROMPT = load_system_prompt(HF_MODEL_ID, \"SYSTEM_PROMPT.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6a906-d894-4916-b5b0-87ed91fe8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"messages\" : [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\":500,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "response = magistral_small_predictor.predict(payload)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Print usage statistics\n",
    "print(\"=== Token Usage ===\")\n",
    "usage = response['usage']\n",
    "print(f\"Prompt Tokens: {usage['prompt_tokens']}\")\n",
    "print(f\"Completion Tokens: {usage['completion_tokens']}\")\n",
    "print(f\"Total Tokens: {usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158db2a-96db-42c5-8c28-4942513a6950",
   "metadata": {},
   "source": [
    "#### Invoke endpoint with boto3\n",
    "Now you can invoke the endpoint with boto3 `invoke_endpoint` or `invoke_endpoint_with_response_stream` runtime api calls. If you have an existing endpoint, you don't need to recreate the `predictor` and can follow below example to invoke the endpoint with an endpoint name.\n",
    "\n",
    "Note that based on the [Magistral Small hugging face page description](https://huggingface.co/mistralai/Magistral-Small-2506), It is highly recommend including the default system prompt used during RL for the best results, you can edit and customise it if needed for your specific use case. You can switch to no thinking by omitting or customizing the default system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ca443-c231-4935-a8d0-b101b2624835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "prompt = {\n",
    "    'messages':[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "],\n",
    "    'temperature':0.7,\n",
    "    'top_p':0.8,\n",
    "    'top_k':20,\n",
    "    'max_tokens':512,\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b6285-1bc0-4f34-a9ec-b6540727d767",
   "metadata": {},
   "source": [
    "#### No thinking by omitting the system prompt. You can always customize the system prompt for your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ce721-ec1a-4143-8c23-059d909b4d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = {\n",
    "    'messages':[\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "],\n",
    "    'temperature':0.7,\n",
    "    'top_p':0.8,\n",
    "    'top_k':20,\n",
    "    'max_tokens':512,\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf043333-5f0c-4314-9234-e5ec8d940e9f",
   "metadata": {},
   "source": [
    "### Streaming content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0fd4c-9af4-42a7-81fc-af16e9f765b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    'messages':[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"},\n",
    "    ],\n",
    "    'temperature':0.9,\n",
    "    'max_tokens':800,\n",
    "    'stream': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3b708-cf53-402d-8dff-d3b465775fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "# Create SageMaker Runtime client\n",
    "smr_client = boto3.client(\"sagemaker-runtime\",region_name='eu-west-3')\n",
    "##Add your endpoint here \n",
    "endpoint_name = \"magistral-small-2506-250612-1355\"\n",
    "\n",
    "# Invoke the model\n",
    "response_stream = smr_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(body)\n",
    ")\n",
    "\n",
    "first_token_received = False\n",
    "ttft = None\n",
    "token_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Response:\", end=' ', flush=True)\n",
    "full_response = \"\"\n",
    "\n",
    "for event in response_stream['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        \n",
    "        try:\n",
    "            # Handle SSE format (data: prefix)\n",
    "            if chunk.startswith('data: '):\n",
    "                data = json.loads(chunk[6:])  # Skip \"data: \" prefix\n",
    "            else:\n",
    "                data = json.loads(chunk)\n",
    "            \n",
    "            # Extract token based on OpenAI format\n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                if 'delta' in data['choices'][0] and 'content' in data['choices'][0]['delta']:\n",
    "                    token_count += 1\n",
    "                    token_text = data['choices'][0]['delta']['content']\n",
    "                                    # Record time to first token\n",
    "                    if not first_token_received:\n",
    "                        ttft = time.time() - start_time\n",
    "                        first_token_received = True\n",
    "                    full_response += token_text\n",
    "                    print(token_text, end='', flush=True)\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "            \n",
    "# Print metrics after completion\n",
    "end_time = time.time()\n",
    "total_latency = end_time - start_time\n",
    "\n",
    "print(\"\\n\\nMetrics:\")\n",
    "print(f\"Time to First Token (TTFT): {ttft:.2f} seconds\" if ttft else \"TTFT: N/A\")\n",
    "print(f\"Total Tokens Generated: {token_count}\")\n",
    "print(f\"Total Latency: {total_latency:.2f} seconds\")\n",
    "if token_count > 0 and total_latency > 0:\n",
    "    print(f\"Tokens per second: {token_count/total_latency:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9783d3-a8e4-4c86-81f8-054e2175ce5a",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984819c-e3ec-47d9-92a8-d91fa4998b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "magistral_small_predictor.delete_model()\n",
    "magistral_small_predictordelete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f329ff-031b-4a53-9cbc-842114e501d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
