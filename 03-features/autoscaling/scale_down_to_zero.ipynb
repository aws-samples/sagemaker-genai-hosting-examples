{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660292bc-cea8-4bb9-965c-684ef3368679",
   "metadata": {},
   "source": [
    "# Deploy Multiple SOTA LLMs on a single endpoint with Scale-to-Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09668c9e-4794-4aa0-8a12-6fb134a66f8a",
   "metadata": {},
   "source": [
    "This demo notebook demonstrate how you can scale in your SageMaker endpoint to zero instances during idle periods, eliminating the previous requirement of maintaining at least one running instance.\n",
    "\n",
    "The new [Scaling to Zero feature](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/) expands the possibilities for managing SageMaker Inference endpoints. It allows customers to configure the endpoints so they can scale to zero instances during periods of inactivity, providing an additional tool for resource management. Using this feature customers can closely match their compute resource usage to their actual needs, potentially reducing costs during times of low demand. This enhancement builds upon SageMaker's existing auto-scaling capabilities, offering more granular control over resource allocation. Customers can now configure their scaling policies to include scaling to zero, allowing for more precise management of their AI inference infrastructure. \n",
    "\n",
    "The Scaling to Zero feature presents new opportunities for how businesses can approach their cloud-based machine learning operations. It provides additional options for managing resources across various scenarios, from development and testing environments to production deployments with variable traffic patterns. As with any new feature, customers are encouraged to carefully evaluate how it fits into their overall architecture and operational needs, considering factors such as response times and the specific requirements of their applications.\n",
    "\n",
    "#### Determining When to Scale Down to Zero\n",
    "\n",
    "SageMaker's scale-to-zero capability is ideal for three scenarios:\n",
    "\n",
    "1. **Predictable traffic patterns:** If your inference traffic is predictable and follows a consistent schedule, you can use this scaling functionality to automatically scale in to zero during periods of low or no usage. This eliminates the need to manually delete and recreate inference components/endpoints.\n",
    "\n",
    "2. **Sporadic workloads:** For applications that experience sporadic or variable inference traffic patterns, scaling in to zero instances can provide significant cost savings. However, it's important to note that scaling out from zero instances to serving traffic is not instantaneous. During the scale-out process, any requests sent to the endpoint will fail, and these \"NoCapacityInvocationFailures\" will be captured in CloudWatch.\n",
    "\n",
    "3. **Development and testing:** The scale-to-zero functionality is also beneficial when testing and evaluating new machine learning models. During model development and experimentation, you may create temporary inference endpoints to test different configurations. However, it's easy to forget to delete these endpoints when you're done. Scaling to zero ensures these test endpoints automatically scale in to zero instances when not in use, preventing unwanted charges. This allows you to freely experiment without closely monitoring infrastructure usage or remembering to manually delete endpoints. The automatic scaling to zero provides a cost-effective way to test out ideas and iterate on your machine learning solutions.\n",
    "   \n",
    "**Note:** Scale-to-zero is only supported when using inference components. for more information on Inference Components see ‚Äú[Reduce model deployment costs by 50% on average using the latest features of Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/reduce-model-deployment-costs-by-50-on-average-using-sagemakers-latest-features/)‚Äù blog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c926cc-d7b0-4eb7-8ac7-6d6d84c57c7d",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "- Fetch and import dependencies\n",
    "- Initialize SageMaker environment and required clients to access AWS services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4789fd-7ab2-469f-b335-bdeac2ae39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker==2.245.0 --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419128f-f128-4848-92b2-f8f44349aed4",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db730a3d-f7fe-4fd7-be32-3d5303001392",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_VERSION = \"0.34.0-lmi16.0.0-cu128\"\n",
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    "\n",
    "instance = {\"type\": \"ml.g5.12xlarge\", \"num_gpu\": 4}\n",
    "endpoint_config_name = endpoint_name = sagemaker.utils.name_from_base(\"lab3\", short = True)\n",
    "timeout = 600\n",
    "variant_name = \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da78e05-4db1-4e99-8247-c1824c517a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmi_env = {\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"16384\",\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444afc2-029e-4028-9176-0c6091f8aabe",
   "metadata": {},
   "source": [
    "### Create a SageMaker endpoint configuration and Endpoint\n",
    "\n",
    "There are a few parameters we want to setup for our endpoint. We begin by creating the endpoint configuration and setting MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use, see the [blog](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/). In addition we will use Managed Instance Scaling which allows SageMaker to scale the number of instances based on the requirements of the scaling of your inference components. Lastly, we set *RoutingStrategy* for the endpoint to optimally tune how to route requests to instances and inference components for the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b795f8-e8e0-4374-a78e-2cbc0f57ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    ProductionVariants = [\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance[\"type\"],\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": timeout,\n",
    "            \"RoutingConfig\": {'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'},\n",
    "            'ManagedInstanceScaling': {\n",
    "                'Status': 'ENABLED',\n",
    "                'MinInstanceCount': 0,\n",
    "                'MaxInstanceCount': 2\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint = sm_client.create_endpoint(EndpointName = endpoint_name,\n",
    "                                     EndpointConfigName = endpoint_config_name)\n",
    "_ = sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5776bac1-e28e-4779-b68f-c4463dbff9b8",
   "metadata": {},
   "source": [
    "### Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5131ac-4100-4b78-a4bf-7cb8d599ee2e",
   "metadata": {},
   "source": [
    "#### Qwen/Qwen3-4B\n",
    "\n",
    "We will use 1 GPU on the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37678f-e54f-4c00-aa26-985f435fbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_env = {\n",
    "    \"HF_MODEL_ID\": \"Qwen/Qwen3-4B\",\n",
    "    \"HF_TOKEN\": \"<YOUR_HF_TOKEN>\",\n",
    "}\n",
    "qwen_model_name = sagemaker.utils.name_from_base(\"qwen\", short=True)\n",
    "qwen_ic_name = f\"ic-{qwen_model_name}\"\n",
    "\n",
    "min_memory_required_in_mb = 4096\n",
    "number_of_accelerator_devices_required = 1\n",
    "\n",
    "model_response = sm_client.create_model(\n",
    "    ModelName = qwen_model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": inference_image,\n",
    "        \"Environment\": qwen_env | lmi_env,\n",
    "    },\n",
    ")\n",
    "\n",
    "ic_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName = qwen_ic_name,\n",
    "    EndpointName = endpoint_name,\n",
    "    VariantName = variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": qwen_model_name,\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": timeout,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": timeout,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"MinMemoryRequiredInMb\": min_memory_required_in_mb,\n",
    "            \"NumberOfAcceleratorDevicesRequired\": number_of_accelerator_devices_required,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\n",
    "        \"CopyCount\": 1,\n",
    "    },\n",
    ")\n",
    "_ = sess.wait_for_inference_component(qwen_ic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87a691-c9bf-4e16-a63d-1089ae05d213",
   "metadata": {},
   "source": [
    "#### Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd2714-d45a-4ff3-b59e-359399b12a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Name popular places to visit in London?\"}\n",
    "    ],\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "res = smr_client.invoke_endpoint(EndpointName = endpoint_name,\n",
    "                                 InferenceComponentName = qwen_ic_name,\n",
    "                                 Body = json.dumps(payload),\n",
    "                                 ContentType = \"application/json\")\n",
    "response = json.loads(res[\"Body\"].read().decode(\"utf8\"))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"‚úÖ Response time: {end_time-start_time:.2f}s\\n\")\n",
    "display(Markdown(response[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00b512-7474-4497-afba-d7ca62a4de78",
   "metadata": {},
   "source": [
    "## Configure per model Autoscaling and Enable Cost-Saving Autoscaling with Scale-to-Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac2748-56a6-456d-afe4-f7af03149efc",
   "metadata": {},
   "source": [
    "### Scaling policies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bde477-bb35-4ece-a03f-330ca2065883",
   "metadata": {},
   "source": [
    "Once our models are deployed and InService, we can then add the necessary scaling policies:\n",
    "\n",
    "* A [target tracking](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html) policy that can scale in the copy count for our inference component model copies to zero, and from 1 to n. \n",
    "* A [step scaling policy](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html) policy that will allow the endpoint to scale out from zero.\n",
    "\n",
    "These policies work together to provide cost-effective scaling - the endpoint can scale to zero when idle and automatically scale out as needed to handle incoming requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a8456-8815-4195-8de9-6ca93f4e3704",
   "metadata": {},
   "source": [
    "### Autoscaling Helper Function\n",
    "\n",
    "> **Note** ofr the target tracking policy, Application Auto Scaling creates two CloudWatch alarms per scaling target. The first triggers scale-out actions after 30 seconds (using 3 sub-minute data point), while the second triggers scale-in after 15 minutes (using 90 sub-minute data points). The time to trigger the scaling action is usually 1‚Äì2 minutes longer than those minutes because it takes time for the endpoint to publish metrics to CloudWatch, and it also takes time for AutoScaling to react. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8102a6-4156-40c8-91af-901e019a4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client = boto3.client(\"application-autoscaling\")\n",
    "cloudwatch_client = boto3.client(\"cloudwatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9beaef9-cb51-434c-9972-1d38ba4c9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_autoscaling(\n",
    "    inference_component_name,\n",
    "    min_capacity=0,\n",
    "    max_capacity=2,\n",
    "    target_requests_per_copy=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Configure complete autoscaling: scale-to-zero + scale-out\n",
    "\n",
    "    Args:\n",
    "        inference_component_name (str): Name of the inference component to configure\n",
    "        min_capacity (int): Minimum number of model copies (set to 0 for scale-to-zero)\n",
    "        max_capacity (int): Maximum number of model copies this component can scale to\n",
    "        target_requests_per_copy (int): Concurrent requests threshold per copy before scaling out\n",
    "    \"\"\"\n",
    "\n",
    "    resource_id = f\"inference-component/{inference_component_name}\"\n",
    "\n",
    "    # 1. Register autoscaling target\n",
    "    aas_client.register_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\",\n",
    "        MinCapacity=min_capacity,  # Configurable minimum (0 enables scale-to-zero)\n",
    "        MaxCapacity=max_capacity,  # Configurable maximum\n",
    "    )\n",
    "\n",
    "    # 2. Target tracking policy (scales min_capacity+1 ‚Üí max_capacity)\n",
    "    aas_client.put_scaling_policy(\n",
    "        PolicyName=f\"target-tracking-{inference_component_name}\",\n",
    "        PolicyType=\"TargetTrackingScaling\",\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\",\n",
    "        TargetTrackingScalingPolicyConfiguration={\n",
    "            \"PredefinedMetricSpecification\": {\n",
    "                \"PredefinedMetricType\": \"SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\",\n",
    "            },\n",
    "            \"TargetValue\": target_requests_per_copy,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 3. Step scaling policy (only needed if min_capacity = 0)\n",
    "    step_policy_response = aas_client.put_scaling_policy(\n",
    "        PolicyName=f\"step-scaling-{inference_component_name}\",\n",
    "        PolicyType=\"StepScaling\",\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\",\n",
    "        StepScalingPolicyConfiguration={\n",
    "            \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "            \"MetricAggregationType\": \"Maximum\",\n",
    "            \"Cooldown\": 60,\n",
    "            \"StepAdjustments\": [{\"MetricIntervalLowerBound\": 0, \"ScalingAdjustment\": 1}]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # CloudWatch alarm for scale-out from zero\n",
    "    cloudwatch_client.put_metric_alarm(\n",
    "        AlarmName=f\"scale-from-zero-{inference_component_name}\",\n",
    "        AlarmActions=[step_policy_response['PolicyARN']],\n",
    "        MetricName='NoCapacityInvocationFailures',\n",
    "        Namespace='AWS/SageMaker',\n",
    "        Statistic='Maximum',\n",
    "        Dimensions=[{'Name': 'InferenceComponentName', 'Value': inference_component_name}],\n",
    "        Period=30,\n",
    "        EvaluationPeriods=1,\n",
    "        DatapointsToAlarm=1,\n",
    "        Threshold=1,\n",
    "        ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Autoscaling configured for {inference_component_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c210d-84f5-4c66-a7b5-52d66a1f70cd",
   "metadata": {},
   "source": [
    "### Setup autoscaling for Qwen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d6eee-b134-49e5-9d1e-296cf2f69464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply autoscaling configuration\n",
    "setup_autoscaling(qwen_ic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c788cb3-ce02-475e-a97c-155573fde79a",
   "metadata": {},
   "source": [
    "## Testing scale to Zero behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeda9a3-8970-4dde-888b-cc313ce832b2",
   "metadata": {},
   "source": [
    "### IC copy count scales in to zero\n",
    "We'll pause for a few minutes without making any invocations to our model. Based on our target tracking policy, when our SageMaker endpoint doesn't receive requests for about 10 to 15 minutes, it will automatically scale down to zero the number of model copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7d95a-f238-4bfa-a0cc-6a8ce4067c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(900)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sm_client.describe_inference_component(InferenceComponentName=qwen_ic_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sm_client.describe_inference_component(InferenceComponentName=qwen_ic_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47a8a7-e19f-4dca-aaac-91a31576fffb",
   "metadata": {},
   "source": [
    "### Endpoint's instances scale in to zero\n",
    "\n",
    "After 10 additional minutes of inactivity, SageMaker automatically terminates all underlying instances of the endpoint, eliminating all associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447c5f4-7fa0-4fbb-9dbd-c470fe2bc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 10mins instances will scale down to 0\n",
    "time.sleep(600)\n",
    "# verify whether CurrentInstanceCount is zero\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b9dd1-270e-43cb-863b-5d7ea62f3746",
   "metadata": {},
   "source": [
    "#### Invoke Qwen model with a sample prompt\n",
    "\n",
    "If we try to invoke our endpoint while instances are scaled down to zero, we get a validation error: `An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcce608-9cc0-4b96-88f9-304ad0edfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "try:\n",
    "    res = smr_client.invoke_endpoint(EndpointName = endpoint_name,\n",
    "                                     InferenceComponentName = qwen_ic_name,\n",
    "                                     Body = json.dumps(payload),\n",
    "                                     ContentType = \"application/json\")\n",
    "    response = json.loads(res[\"Body\"].read().decode(\"utf8\"))\n",
    "    display(Markdown(response[\"choices\"][0][\"message\"][\"content\"]))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Reason: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c742af-0cbe-4b5d-ab4d-6fa7dd8f6eb1",
   "metadata": {},
   "source": [
    "### Scale out from zero kicks in\n",
    "However, after 1 minutes our step scaling policy should kick in. SageMaker will then start provisioning a new instance and deploy our inference component model copy to handle requests. This demonstrates the endpoint's ability to automatically scale out from zero when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76297f-0bc3-4a9d-8dec-6c9ddb7dfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sm_client.describe_inference_component(InferenceComponentName=qwen_ic_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2978a-798b-4a2b-9f7a-fc87833d7176",
   "metadata": {},
   "source": [
    "#### verify that our endpoint has succesfully scaled out from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbe8e8-9706-4492-ab70-3929754b9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "try:\n",
    "    res = smr_client.invoke_endpoint(EndpointName = endpoint_name,\n",
    "                                     InferenceComponentName = qwen_ic_name,\n",
    "                                     Body = json.dumps(payload),\n",
    "                                     ContentType = \"application/json\")\n",
    "    response = json.loads(res[\"Body\"].read().decode(\"utf8\"))\n",
    "    display(Markdown(response[\"choices\"][0][\"message\"][\"content\"]))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Reason: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197f8d0-c960-4e2a-9ec6-6d276bdbd3c5",
   "metadata": {},
   "source": [
    "## Testing scaling behavior 1->n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84134c-4c8b-43b8-a7b0-73ca327c782d",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf85641-d984-49a4-80f9-037122c1affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def make_prediction(endpoint_name, payload, request_id):\n",
    "    \"\"\"Make a single prediction and return results with timing\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        res = smr_client.invoke_endpoint(EndpointName = endpoint_name,\n",
    "                                         InferenceComponentName = qwen_ic_name,\n",
    "                                         Body = json.dumps(payload),\n",
    "                                         ContentType = \"application/json\")\n",
    "        response = json.loads(res[\"Body\"].read().decode(\"utf8\"))\n",
    "        duration = time.time() - start_time\n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': True,\n",
    "            'duration': duration,\n",
    "            'response': response['choices'][0]['message']['content'][:50] + \"...\" if 'choices' in response else response[0]['generated_text'][:50] + \"...\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        duration = time.time() - start_time\n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': False,\n",
    "            'duration': duration,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def get_current_copy_count(inference_component_name):\n",
    "    \"\"\"Get the current number of inference component copies\"\"\"\n",
    "    try:\n",
    "        response = sm_client.describe_inference_component(\n",
    "            InferenceComponentName=inference_component_name\n",
    "        )\n",
    "        return response['RuntimeConfig']['CurrentCopyCount']\n",
    "    except Exception as e:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e85a9-d038-40f0-b760-2aebad0fd6d2",
   "metadata": {},
   "source": [
    "### Autoscaling demonstration\n",
    "\n",
    "In the following code we will run 6 iterations with 6 concurrents request each.\n",
    "\n",
    "What we will observe:\n",
    "- Initial low component count\n",
    "- Concurrent load being applied\n",
    "- Component count increasing as autoscaling kicks in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167e9de-146f-4cde-9c97-272617990912",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 6\n",
    "requests_per_iteration = 6\n",
    "wait_between_iterations = 10\n",
    "\n",
    "print(\"Starting Inference load\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    print(f\"\\nIteration {iteration}/{num_iterations}\")\n",
    "\n",
    "    # Show current inference component count\n",
    "    current_copies = get_current_copy_count(qwen_ic_name)\n",
    "    print(f\"Current Inference Components: {current_copies}\")\n",
    "\n",
    "    # Make concurrent requests\n",
    "    print(f\"Sending {requests_per_iteration} concurrent requests...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=requests_per_iteration) as executor:\n",
    "        futures = [\n",
    "            executor.submit(make_prediction, endpoint_name, payload, f\"{iteration}-{i+1}\")\n",
    "            for i in range(requests_per_iteration)\n",
    "        ]\n",
    "\n",
    "        results = [future.result() for future in as_completed(futures)]\n",
    "\n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r['success'])\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    if successful > 0:\n",
    "        avg_response_time = sum(r['duration'] for r in results if r['success']) / successful\n",
    "        print(f\"‚úÖ Results: {successful}/{requests_per_iteration} successful\")\n",
    "        print(f\"‚è±Ô∏è Total: {total_time:.1f}s, Average response: {avg_response_time:.1f}s\")\n",
    "    else:\n",
    "        print(f\"‚ùå All requests failed\")\n",
    "\n",
    "    # Show any failures\n",
    "    failures = [r for r in results if not r['success']]\n",
    "    if failures:\n",
    "        print(f\"‚ö†Ô∏è  {len(failures)} requests failed\")\n",
    "\n",
    "    # Wait between iterations (except last one)\n",
    "    if iteration < num_iterations:\n",
    "        print(f\"‚è≥ Waiting {wait_between_iterations}s before next iteration...\")\n",
    "        time.sleep(wait_between_iterations)\n",
    "\n",
    "print(f\"\\n Inference load complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef5fc6-5207-413a-b8ff-032018821115",
   "metadata": {},
   "source": [
    "#### Check that our model has scaled to 2 copies\n",
    "\n",
    "> **Note** you can rerun the cell above and increase the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd01da0-2ae4-4355-be66-c6591bfea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    desc = sm_client.describe_inference_component(InferenceComponentName=qwen_ic_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "current_copies = get_current_copy_count(qwen_ic_name)\n",
    "print(f\"Current Inference Components: {current_copies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396fdc6-5f66-41d8-98ab-f403e8920fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "try:\n",
    "    res = smr_client.invoke_endpoint(EndpointName = endpoint_name,\n",
    "                                     InferenceComponentName = qwen_ic_name,\n",
    "                                     Body = json.dumps(payload),\n",
    "                                     ContentType = \"application/json\")\n",
    "    response = json.loads(res[\"Body\"].read().decode(\"utf8\"))\n",
    "    display(Markdown(response[\"choices\"][0][\"message\"][\"content\"]))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Reason: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb051a35-69ad-42c4-875d-a9c21c35e112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T21:46:50.557381Z",
     "iopub.status.busy": "2025-08-24T21:46:50.557056Z",
     "iopub.status.idle": "2025-08-24T21:46:50.560177Z",
     "shell.execute_reply": "2025-08-24T21:46:50.559678Z",
     "shell.execute_reply.started": "2025-08-24T21:46:50.557361Z"
    }
   },
   "source": [
    "### Note: \n",
    "**If you do not have a multi-GPU instance, you can skip to the cleanup section**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07573401-0367-49b0-9b62-6c352752a030",
   "metadata": {},
   "source": [
    "## (OPTIONAL MULTI-GPUs INSTANCE) Deploy gpt-oss-20b model on the same endpoint\n",
    "\n",
    "Now we will deploy gpt-oss-20b model on the same endpoint previously created with the Qwen model. \n",
    "> **Please note that you require a multi-GPU machine for this to work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d6c7a-a963-4af3-98de-ee725ebab3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptoss_env = {\n",
    "    \"HF_MODEL_ID\": \"openai/gpt-oss-20b\",\n",
    "    \"HF_TOKEN\": \"YOUR_HF_TOKEN\",\n",
    "}\n",
    "gptoss_model_name = sagemaker.utils.name_from_base(\"gpt-oss\", short=True)\n",
    "gptoss_ic_name = f\"ic-{gptoss_model_name}\"\n",
    "\n",
    "min_memory_required_in_mb = 4096\n",
    "number_of_accelerator_devices_required = 1\n",
    "\n",
    "model_response = sm_client.create_model(\n",
    "    ModelName = gptoss_model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": inference_image,\n",
    "        \"Environment\": gptoss_env | lmi_env,\n",
    "    },\n",
    ")\n",
    "\n",
    "ic_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName = gptoss_ic_name,\n",
    "    EndpointName = endpoint_name,\n",
    "    VariantName = variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": gptoss_model_name,\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": timeout,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": timeout,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"MinMemoryRequiredInMb\": min_memory_required_in_mb,\n",
    "            \"NumberOfAcceleratorDevicesRequired\": number_of_accelerator_devices_required,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\n",
    "        \"CopyCount\": 1,\n",
    "    },\n",
    ")\n",
    "_ = sess.wait_for_inference_component(gptoss_ic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8decb3e3-0347-4e3d-90dc-4350b1db6a00",
   "metadata": {},
   "source": [
    "#### Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f370a-7658-476d-8ed8-992177c08c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Name popular places to visit in London?\"}\n",
    "    ],\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "res = smr_client.invoke_endpoint(EndpointName = endpoint_name,\n",
    "                                 InferenceComponentName = gptoss_ic_name,\n",
    "                                 Body = json.dumps(payload),\n",
    "                                 ContentType = \"application/json\")\n",
    "response = json.loads(res[\"Body\"].read().decode(\"utf8\"))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"‚úÖ Response time: {end_time-start_time:.2f}s\\n\")\n",
    "display(Markdown(response[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5f580-289a-4989-86d3-08a8721040fb",
   "metadata": {},
   "source": [
    "### Setup autoscaling for GPT-OSS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efecdcc-e804-4acb-bf09-c980c0b100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply autoscaling configuration\n",
    "setup_autoscaling(gptoss_ic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de860dd7-eb9f-4f7f-ad48-72e410d1ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_copies = get_current_copy_count(gptoss_ic_name)\n",
    "print(f\"Current Inference Components: {current_copies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d08e7-a532-4225-bdee-5463535fa3c3",
   "metadata": {},
   "source": [
    "### Understanding Model Copies and scale-to-zero\n",
    "\n",
    "**Important** To scale to zero on an endpoint with multiple inference components, all components must be either set to 0 or deleted.\n",
    "\n",
    "**Model Copy**: One loaded instance of your model in GPU memory\n",
    "- Copies can share GPU instances (depending on model size and available accelerators)\n",
    "\n",
    "**Instance**: The underlying compute resource (e.g., ml.g5.2xlarge)\n",
    "- SageMaker automatically manages instances based on copy demands\n",
    "- Multiple small model copies can share one instance\n",
    "- Large model copies might need dedicated instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60d413-8f12-459d-8b81-a13f8233459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker describe-inference-component --inference-component-name $gptoss_ic_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abf809-ca6e-48f6-80ea-172acaf4db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker describe-endpoint --endpoint-name $endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ffab4-db77-467f-b472-a621c229adee",
   "metadata": {},
   "source": [
    "# Clean up the environment\n",
    "\n",
    "- Deregister scalable target\n",
    "- Delete cloudwatch alarms\n",
    "- Delete scaling policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa2a8b-6c3a-47dd-a3b0-e5d96db5f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_autoscaling(inference_component_names, aas_client, cloudwatch_client):\n",
    "    \"\"\"Clean up autoscaling resources for workshop\"\"\"\n",
    "\n",
    "    if isinstance(inference_component_names, str):\n",
    "        inference_component_names = [inference_component_names]\n",
    "\n",
    "    print(\"üßπ Cleaning up autoscaling resources...\")\n",
    "\n",
    "    for ic_name in inference_component_names:\n",
    "        resource_id = f\"inference-component/{ic_name}\"\n",
    "\n",
    "        # Clean up all autoscaling components\n",
    "        try:\n",
    "            # Get and delete policies\n",
    "            policies = aas_client.describe_scaling_policies(\n",
    "                ServiceNamespace=\"sagemaker\",\n",
    "                ResourceId=resource_id,\n",
    "                ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\"\n",
    "            )['ScalingPolicies']\n",
    "\n",
    "            for policy in policies:\n",
    "                aas_client.delete_scaling_policy(\n",
    "                    PolicyName=policy['PolicyName'],\n",
    "                    ServiceNamespace=\"sagemaker\",\n",
    "                    ResourceId=resource_id,\n",
    "                    ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\"\n",
    "                )\n",
    "\n",
    "            # Delete alarm\n",
    "            cloudwatch_client.delete_alarms(AlarmNames=[f\"scale-from-zero-{ic_name}\"])\n",
    "\n",
    "            # Deregister target\n",
    "            aas_client.deregister_scalable_target(\n",
    "                ServiceNamespace=\"sagemaker\",\n",
    "                ResourceId=resource_id,\n",
    "                ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\"\n",
    "            )\n",
    "\n",
    "            print(f\"‚úÖ Cleaned up autoscaling for {ic_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ÑπÔ∏è  Partial cleanup for {ic_name} (some resources may not exist)\")\n",
    "\n",
    "    print(\"üéâ Autoscaling cleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cdc7f-81fc-40cd-b45c-54bc3e71b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_autoscaling([qwen_ic_name, gptoss_ic_name], aas_client, cloudwatch_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8d9e6-953d-47a8-94fd-4f142be39132",
   "metadata": {},
   "source": [
    "- Delete inference component\n",
    "- Delete endpoint\n",
    "- Delete endpoint-config\n",
    "- Delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f2e16-f136-4b23-b4d3-187b31a0e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_inference_component(qwen_ic_name, wait=True)\n",
    "sess.delete_inference_component(gptoss_ic_name, wait=True)\n",
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(qwen_model_name)\n",
    "sess.delete_model(gptoss_model_name)\n",
    "\n",
    "print(\"‚úÖ Workshop cleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f71a1-cfd2-40e1-8513-d94eb6e52fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
